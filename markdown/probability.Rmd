# Probability to describe uncertainty {#probability}

```{r package-load-probability, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
library("igraph")
library("ggthemes")
library("kableExtra")
library("tidyverse")
library("plotly")
library("grid")
library("plyr")
# set some useful variables
source("./code/useful-variables.R")
```

We saw in the previous chapter that there is uncertainty in scientific evidence. By asking a few simple questions about the uncertainties in a particular context, we demonstrated a systematic approach to assessing the uncertainty which can easily be applied to forensic evidence. As part of this process, the expert assesses the magnitude of their personal uncertainty given the scientific expertise and experience that they have. An example we gave of this was when the uncertainty was quantified as follows: out of every 1000 similar case circumstances, the expert believes 200 would yield a fibre match if the suspect had truly been at the crime scene. This is useful because it has described exactly how uncertain the expert is about the evidence in what is likely to be the prosecution's version of events. In this example, the uncertainty has been converted into a probability, which has then been converted into a so-called natural frequency for the purpose of communication. In this chapter we discuss probability, describing what it is and using examples to demonstrate some of its useful properties as a framework for handling uncertainty.

## Quantifying uncertainty

Quantifying uncertainty gives us a systematic way of assimilating and comparing uncertainties. This means that different personal uncertainties for the same object can be assessed consistently and also that personal uncertainties for different objects can be compared. This is because the framework of mathematics forces quantities to obey a coherent and consistent set of logical rules. The subset of mathematics which handles uncertainty is known as probability. The main benefit of using probability is the framework of logic that it enforces, rather than the quantification of uncertainty (although this is useful).

A probability is a number between 0 and 1 that describes the magnitude of uncertainty for the occurrence of an event. The probability must obey certain rules which we will show in subsequent examples in this chapter. A probability of 0 means that the event is impossible whilst a probability of 1 means that an event is certain. Uncertainty is described by probabilities which fall between 0 and 1. Probabilities of 0.5 describe an event whose occurrence is exactly as likely as its non-occurrence. Events whose occurrence is less likely than not should have a probability less than 0.5 on the scale, whilst events whose occurrence is more likely than not should have a probability greater than 0.5 on the scale. How close these probabilities are to 0 and 1 should reflect the magnitude of uncertainty in their occurrence or non-occurrence. Since we made the argument that uncertainty is personal because it depends upon an individual's beliefs, it follows that probabilities are personal too. In forensic science, personal probabilities are generally interpreted as an individual's degree of belief in the occurrence of an event. Not everyone who is familiar with probability theory agrees with this interpretation, but this historical debate is not important for the purposes of this book.

Quantifying direct versus indirect uncertainty is conceptually different. For example, if an expert is examining fibre evidence and is uncertain about the background prevalence of a particular colour of wool textiles in the local area, then they could consult a local population textile survey to help them determine this. The population survey could give information about the proportion of woollen garments worn in the local area, as well as the proportion of those which are the colour of interest. The expert can use these proportions to quantify their direct uncertainty about the background prevalence of that colour of wool fibre. However, if the expert is uncertain about the reliability of the methodology employed by the fibre survey, then this is indirect uncertainty and quantifying this information is more challenging. That is why indirect uncertainties are often communicated using verbal expressions of evidence quality. Indirect uncertainties about forensic evidence are usually verbally qualified in court. We only consider quantifying direct uncertainties here.

Constructing probabilities to describe uncertainty is often done by assuming a probabilistic **model** for how the uncertainty is expected to behave in reality. Since the process that is being modelled is uncertain, the expectations might not be exactly what is observed in practice. This gives rise to the phrase "all models are wrong, but some are useful". The most useful models can accurately align an individual's magnitude of uncertainty to a quantitative probability, accepting that this can never be done perfectly.

## Example: coin toss {#single-coin-toss}

The classic example for demonstrating probability is tossing a coin. The outcome of a coin toss is uncertain in most cases. We can consider some of the questions from the previous section to describe this uncertainty.

**What uncertainties are there?** The uncertainty is whether the coin will land heads-up or tails-up as a result of the toss.

**What are the sources of this uncertainty?** There is a randomness to the flipping process. We also do not know if the coin is double-sided, e.g. has two heads instead of one head and one tail. You may consider other sources here too, such as trust in the person flipping the coin to be acting fairly.

The potential double-sidedness of the coin represents epistemic uncertainty. We can eliminate this uncertainty by checking both sides of the coin before it is tossed.

The randomness of the flipping process represents a combination of aleatory and epistemic uncertainty. The epistemic uncertainty comes from trust in the person flipping the coin and other factors which might unfairly influence the outcome of the coin flip. It is possible to reduce this uncertainty by learning about the person flipping the coin and making changes if needed, e.g. by letting someone trustworthy flip the coin instead.

Assume that it has been checked that the coin has one head and one tail, and that the person flipping the coin is acting fairly. There is now only aleatory uncertainty about the outcome. This is an irreducible uncertainty of the coin flip; no further information can be learned which will make a guess of the outcome better than randomly guessing heads or tails. Analysing this uncertainty using the model from Section \@ref(uncertainty-model) gives:

**What is the level of this uncertainty?** This is direct uncertainty about the coin toss.

**What is the magnitude of uncertainty?** First, we can bound the magnitude of this uncertainty using what we know about the coin and the logic of probability. A probability of 1 means that an event is certain. Since the coin has one head and one tail, the coin toss must either result in a head or tail. This means that the event 'head or tail' occurring must have a probability of 1. It also means that the event 'head and tail' has probability 0; it is impossible to get both in a single toss. The logic of probability has dictated that any personal probabilities must agree with the above conditions in order to be coherent.

The event 'head or tail', which has probability 1, is made up of two other events: the event 'head' and the event 'tail'. In other words, the distinct outcomes 'head' or 'tail' combine into the single outcome 'head or tail'. These two distinct outcomes are known as **mutually exclusive**, since they cannot occur together. When outcomes are mutually exclusive, then their probabilities can be added together - this is one of the laws of probability. In addition, since they are the only two possible outcomes then their probabilities must sum to 1. This property is known as **exhaustivity**, i.e. the events exhaust all possible outcomes. Outcomes which are not exhaustive do not need to sum to 1. Without having yet quantified any uncertainty, the logic of probability states that the sum of the probabilities for head and tail must equal 1.
<!-- [MAYBE ADD VENN DIAGRAM FOR VISUAL AID] -->

Knowing that the probabilities of these two outcomes add up to 1 is helpful when assigning a personal probability to them individually. This is because we can consider their probabilities in relation to each other. Is getting a 'head' more or less likely than getting a 'tail'? We assumed that the person tossing the coin was doing so fairly, and so a rational belief with this information is that head and tail are equally likely. If they are equally likely, then they must each have a probability equal to 0.5. This is because the total probability is equal to 1 (due to exhaustivity) and is divided into two equally sized pieces (due to mutual exclusivity and the coin toss being fair), which makes them of size 0.5 each. Notice how this quantified uncertainty is the result of the logic of probability. The logic was the most important step because it constrained any beliefs about the coin to be coherent, then the quantification was secondary. 

Now that we have beliefs for the outcome of the coin toss, we have a probabilistic model. Checking the outcomes of this model can be a good way to confirm that it does align with our uncertainty. One helpful way to do this is to frame our beliefs in terms of expected frequencies. For this example, this means assuming that some number of tosses will be performed and to project what the model expects to occur for the outcome of these tosses. If we hold the belief that each probability is 0.5, then we expect that the outcomes should be evenly distributed between heads and tails. 

```{block eval=FALSE, include=isDynamicOutput()}
### Interactive Example

In the example below, there are two tree diagrams. The probability tree displays the degree of belief for the coin toss in terms of the probabilities for the outcomes. The expected frequency tree displays the expected number of heads and tails corresponding to those probabilities for a fixed number of coin tosses. 

Use the sliders below to change the probability of heads for the coin toss and also the number of tosses with which to view the expected outcomes. Firstly, fix the probability of heads to 0.5, since this is what we did in the example above. Then change the number of tosses to see that the expected number of heads and tails are equal. Next, fix the probability of heads to another number and see how this affects the expected outcomes. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabCoinTree",NULL,TRUE), height = '750px')
}
```
  
```{block eval=TRUE, include=!isDynamicOutput()}
### Expected frequency tree

Below is an example of an expected frequency tree. The expected frequency tree displays the expected number of heads and tails corresponding to the probabilities of heads and tails for a fixed number of coin tosses. 

A probability of heads of 0.5 means that we expect 5000 or every 10,000 tosses to result in heads, and the other 5000 to result in tails.

```
```{r echo=FALSE, freq-tree-coin-toss, echo=FALSE, fig.cap="An expected frequency tree diagram of the coin toss example. Out of 10,000 cosses, we expect 5000 to be heads and 5000 to be tails. The probability of heads is 0.5 and is equal to the probability of tails. ", out.width = '80%', fig.align = 'center'}
if(!isDynamicOutput()){
e <- c(1, 2, 1, 3)
v <- c("10,000\ntosses", "5000\nheads", "5000\ntails")
freqTree <- graph(edges=e, n=3, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 3))
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
}
```

<!-- To view all of the interactive examples in this book, please visit [Interactive Stats Book](`r getInteractiveLink()`) (`r getInteractiveLink()`). -->

The expected frequency tree in the example above shows what the probability model expects to occur for a specified probability of a head. It does not necessarily show what will be observed in practice because there is an unavoidable uncertainty about the outcomes (aleatory uncertainty). Tossing the coin, observing the outcomes, and then using that to refine the probability model falls into the realm of statistics.

As a final comment, notice that if we did not ignore some of the epistemic uncertainties about the coin toss (e.g. if we did not trust the coin flipper to be fair), then the probability  model would need to be adjusted to align with this belief, e.g. by changing the probability of heads from 0.5. We could then observe actual outcomes of the tosses and compare those to the expected outcomes under the new probability model in order to refine our beliefs using statistics.

## Personal probabilities

We mentioned in the previous section that probability was personal because individuals have different knowledge and beliefs. In the previous example someone might believe that the person (or computer) tossing the coin was doing so unfairly, e.g. that the flip was in favour of a head or a tail. This person's probability for a head should then be different from 0.5 to align with this belief. 

Some differences in beliefs might be more or less reasonable since they might be based upon more or less reliable information. Probability assignments based upon more reliable and generally agreed information will be more **objective**. For example, the scientific evidence underlying the processing and analysis of full DNA profiles for single donors is well established and so experts are generally confident and in agreement about assigning match probabilities in this circumstance.

Probability assignments which are based upon less reliable or less generally agreed information will be more **subjective**. For example, the transfer and persistence of DNA on surfaces is an active area of research for scientists and so probabilities involving these circumstances may be subjective. Subjective probabilities are not necessarily bad because they can still be the best assessment of the available knowledge. This might happen for example when there is very limited published scientific literature about a particular scenario. Even though there is limited published information, an expert may have significant previous case experience in that scenario and so may be able to assign subjective probabilities. In other words, subjective probability assignments are not necessarily (and should not be) arbitrary.

For repeatable events like the coin toss we can check and update our probabilities using empirical data, e.g. repeated flips of the coin. This can be seen as eliminating epistemic uncertainty about the biasedness of the flipper or gaining a better understanding of the aleatory uncertainty about the distribution of the outcomes. With enough repetition, personal probabilities which were initially different between individuals can converge to the same value. This would happen for example when all epistemic uncertainty is eliminated and there are enough data to agree on the nature of the aleatory uncertainty. Repeating the process about which there is uncertainty makes personal probabilities more easily assessable, as beliefs which are coherent with or contradictory to available information become clearer.

In criminal cases, the opportunity to repeat events and gather scientific data from the same case circumstances can be limited. This is because some events are one-off and difficult (if not impossible) to completely replicate in scientific experiments. Repeating some case circumstances might also be undesirable when they are harmful. For example, it would be harmful to exactly replicate the case circumstances of stabbings in order to examine the resulting blood patterns on nearby surfaces. In these situations, it is not feasible to assign probabilities by empirical repetition alone. Probabilities can still be informed by other available empirical data however, e.g. by observations from similar circumstances, and can also still be informed by expert knowledge. This knowledge should be disclosed and be available for audit by the court. In the blood pattern example above, scientists perform experiments using mannequins and animal blood in order to understand potential blood patterns resulting from attacks with weapons.

## Conditioning on information

Information which is used to construct a probability is called **conditioning** information. This is because we are constructing a probability which is conditional on that information, otherwise known as a **conditional probability**. The process of using information this way to construct such a conditional probability is itself called **conditioning**.

This means that probability is conditional on our current state of individual knowledge. However, to avoid being overly tedious we usually omit to say this each time we talk about a specific probability, e.g. we do not say the probability of heads followed by listing all of an individual's assumptions and conditioning information each time it is mentioned. Instead, it is conventional to list initial assumptions when the probability is first mentioned. This information is known as **background information** since after it is listed it is assumed to be part of the background of any belief that is expressed. For example, for the coin toss we stated that the flipper was tossing the coin fairly. This assumption was a key part of the background information that led to reasonably assigning a 0.5 probability to a head. This probability might not have been rational for individuals who did not assume this same background information. Background information is not always made explicit but it is always a component of probability assignments. 

Most explicit applications of conditional probability are not focussed on background information, but on other extra contextual information which is logically separate from (and usually more important than) the background information. The distinction between these two types of information is context-dependent, as background information in some situations might be critical conditioning information in other situations. For example, when interpreting fibre evidence recovered from a crime scene, the expert may wish to condition on the person of interest having been present at the crime scene. This would be background information in a case in which the person of interest admits to having been present at the crime scene. Otherwise it would be disputed by the person of interest and thereby become a critical piece of conditioning information. In any case, important contextual information is usually made explicit when probabilities are expressed whilst the background information is usually contained within the assumptions underlying the probabilities. Any conditioning information is referred to as **relevant** to a specific event if conditioning on it changes an individual's probability of that event occurring. 

A useful application of conditional probability is to condition on possible outcomes of future events to see how this affects probabilities of interest. For example, banks who lend money will need to consider the probability that loanees can repay money (with interest). Possible future events which might affect that probability include the status of the lonees' employment. If their employment is unstable and their probability of repayment would greatly decline if they were to lose their job, then the bank might be less likely to lend to them compared to if their probability of repayment was unaffected by their employment status (e.g. if they own physical assets that the bank could seize if they fail to repay). Considering possible future events and risks helps to make uncertain decisions in the present. 

We can also condition on possible events in the past. This is useful when exploring uncertainty related to the causes of important outcomes in the past. There were causes for the outcomes, but we do not known them and so this is epistemic uncertainty about the past. In these cases, we can consider the probability of the observed outcome conditioned on each of the candidate causal events. These conditional probabilities can then be compared to determine which of the causal events is most supported by the observed outcome. This is how conditional probabilities are used for interpreting forensic evidence: we look at the events which the defence and prosecution assert and compare how likely the observed evidence would have been when it is conditioned on each assertion. For example, when interpreting fibre evidence recovered from the clothing of a person of interest, the expert must consider how probable this evidence would have been in light of what both the prosecution and defence claim to have happened. We will revisit this idea in later chapters. For now, we will explore conditional probability by adding a second coin to the coin toss example. 

## Example: double coin toss {#double-coin-toss}

Suppose now that we toss 2 coins, labelled coin 1 and coin 2. Coin 1 is tossed first and the outcome is recorded. Then coin 2 is tossed and its outcome is recorded. The outcome of the toss of coin 1 can be conditioned on to see how it affects the probabilities of the outcomes of coin 2. This means that the uncertainty of interest is the toss of coin 2 conditioned on the outcome of the toss of coin 1. 

Suppose that the background information is that both coins are tossed in such a way that guarantees a 0.5 probability of a head for each coin. This means that the uncertainties and probabilities for the toss of coin 1 are unchanged from the single toss example in section \@ref(single-coin-toss). The possible outcomes for coin 1 are 'head' with probability 0.5 and 'tail' with probability 0.5. Assuming that coin 1 has been tossed and is a head, what is the probability that coin 2 will be a head? The background information states that this probability should be 0.5 because coin 2 (like coin 1) is tossed in such a way that guarantees this. This means that the probability of a tail for coin 2 conditioned on a head for coin 1 is also 0.5 (since those two probabilities must sum to equal 1). The same reasoning applies for conditioning on coin 1 being a tail too; for coin 2 there is an equal probability of 0.5 for a head or a tail conditioned on coin 1 being a tail. 

Figure \@ref(fig:freq-tree-double-coin) shows this information in an expected frequency tree for the double coin toss.

```{r freq-tree-double-coin, echo=FALSE,fig.cap="An expected frequency tree diagram of the double coin toss example. Out of every 10,000 double coin tosses, we expect 2500 to be double heads. The probability of getting two heads is 0.25.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(2, 2, 1, 2, 1, 1, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

The first branch of the outcomes is the same as in section \@ref(single-coin-toss). After these outcomes of coin 1, there is a probability of 0.5 for coin 2 being a head or a tail. In expected frequency terms, for every 5000 tosses in which coin 1 is heads we expect 2500 heads and 2500 tails from coin 2. Similarly, for every 5000 tosses in which coin 2 is tails we expect 2500 heads and 2500 tails from coin 2. This also means that out of the total 10,000 double coin tosses, we expect there to be 2500 double heads. This reflects the belief that there is a $\frac{2500}{10000}=\frac{1}{4}=0.25$ probability of obtaining a double heads.

This probability of 0.25 for the double heads can also be obtained using another rule of probability, known as the **multiplication rule**. This mathematical rule states that the joint probability of two events, say events A and B, can be obtained by multiplying the probability of event A by the probability of event B conditioned on event A. The order of events can also be switched so that one multiplies the probability of event B by the conditional probability of event A conditioned on event B. In the double coin toss example, that means the probability of a head for both coin 1 and coin 2 could be obtained by multiplying the probability of a head from coin 1 with the conditional probability of a head from coin 2 conditioned on a head from coin 1. These probabilities were 0.5 and 0.5 respectively, and so multiplying these together gave the joint probability as 0.25. 

The probabilities of coin 2 in this example are unaffected by conditioning on the outcomes of coin 1. This is technically known as **independence** between the coin tosses, and is an example of conditioning information that is not relevant. Independence between the coins was guaranteed by construction in this example because the background information stated an unconditional probability of heads of 0.5 for both coins. In practical applications of probability, the independence between two processes should be made explicit and should be justified. 

```{block eval=FALSE, include=isDynamicOutput()}
### Interactive Example

The example below displays the probability and expected frequency trees for the double coin toss.  

Use the sliders below to change the probability of a head for both coins and also the number of double tosses for the expected outcomes. Firstly, fix the probability of a head to be 0.5, since this is what we did in the example above. Then change the number of tosses to see that the expected number of double heads and double tails are still equal. Next, fix the probability of heads to another number and see how this affects the expected outcomes. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDoubleCoinTree",NULL,TRUE), height = '800px')
}
```

We can also consider two events for the same example which are not independent. Suppose there is a game that is decided by the result of the double coin toss. Player 1 wins if the coin tosses result in double heads and player 2 wins if the coin tosses result in double tails. If neither player wins then a draw is called and the coins are tossed again. Since double heads is just as likely as double tails, the probability is 0.25 for each, then both players are equally likely to win prior to the first coin being tossed. We can see this from the highlighted Figure \@ref(fig:freq-tree-double-coin-bet) below.

```{r freq-tree-double-coin-bet, echo=FALSE,fig.cap="An expected frequency tree diagram of the double coin toss game. Out of every 10,000 tosses, 2500 are double heads and 2500 are double tails. This means that wthe players have equal probability of winning of 0.25 each. The remaining 5000 tosses result in a draw and so the probability of a draw is 0.5.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000\ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(1, 1, 1, 2, 1, 1, 2)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

Out of 10,000 double tosses, player 1 is expected to win 2500, player 2 is expected to win 2500, and 5000 double tosses are expected to result in a draw. However, whilst the outcome of coin 1 does not affect the outcome of coin 2, it does affect the winning probabilities of this game. For instance after coin 1 has been tossed and shows heads, there is now a 0.5 probability that player 1 wins, a 0.5 probability that there is a draw, and it is impossible that player 2 wins in this round. Conditioning on coin 1 being heads has increased the probability of player 1 winning from 0.25 to 0.5, and it has decreased the probability of player 2 winning from 0.25 to 0. The opposite occurs when the outcome of coin 1 is a tail. On the other hand, the probability of a draw remains unchanged at 0.5 before coin 1 is tossed, and 0.5 after it is tossed regardless of the result. In summary, the winning outcomes are not independent of the result of coin 1, but the draw is independent of it.

## Odds

Odds are another way of expressing probabilities. Odds are given as a ratio of probabilities so that it is clear how much more or less likely one event is compared to another. This means that probability can be thought of as an absolute measure of uncertainty, whilst odds are a relative measure of uncertainty. Any two probabilities can be compared together to create odds. One pair which is commonly used is the probability of an event occurring versus the probability of the same event not occurring.

For example, the fair coin toss was just as likely to result in a head as it was to result in a tail. This results in odds written as 1:1, which is spoken as '1-to-1' or more commonly 'evens'. Intuitively, odds of 1:1 means that we can separate the possible outcomes into a total of $1+1=2$ parts which are equally probable. One part represents the probability of a head and the other part represents the probability of a tail.

We can convert from odds to probability for exhaustive events as follows. For the single coin toss we had odds of 1:1 for heads. The total probability, which must be 1, is made up of 2 equally sized parts. One of these parts represents a head, and so the probability of a head (and similarly for a tail) is $\frac{1}{2}=0.5$. Suppose instead we have odds of 1:9 for an event occurring versus not occurring. This means that the total probability, which must be 1, is made up of 10 equally sized parts. One of these ten parts represents the event occurring, and so its probability is $\frac{1}{10}=0.1$. The other nine parts represent the event not occurring, and so the probability of non-occurrence is $\frac{9}{10}=0.9$, Odds which are used to represent probabilities before any conditioning occurs, are known as **prior odds**. The conversion from odds to probabilities is more challenging when the events are not exhaustive, but we do not consider that here.

Converting from probabilities to odds is much simpler since we only measure how much bigger one probability is than the other. For example, the probability of player 1 winning the game in the double coin toss example by getting double heads was 0.25. That meant that the probability of player 1 not winning before any coins had been tossed was 0.75. Since the probability of player 1 not winning (0.75) was three times larger than the probability of player 1 winning (0.25), the prior odds of player 1 winning were 1:3.

Prior odds can be updated using conditioning information. These updated odds are known as **posterior odds**. For example, as part of the double coin toss game we conditioned on the result of coin 1 being a head. Before the conditioning, the probability of player 1 winning was 0.25; the prior odds of player 1 winning were 1:3. Conditioning on coin 1 being a head resulted in the probability of player 1 winning rising to 0.5. This means that the posterior odds (i.e. **after** observing coin 1 being a head) of player 1 winning versus not winning were evens (1:1) for that round. 

Another reason that odds are useful is because of the simplicity that they give to a very important mathematical result, which we discuss in the next section. 

## Bayes' rule {#bayes}

**Bayes' rule** is a mathematical rule which links the **prior odds** to the **posterior odds**. Suppose we are considering the odds of an event $A$ compared to an event $B$, and we observe some new information from an event $E$. It is a natural idea to update the prior odds for $A$ to $B$ with the new information gained from $E$ in order to obtain posterior odds - the odds obtained by conditioning on $E$. Bayes' rule states that this is done as follows:
$$\text{posterior odds of }A\text{ to } B=\frac{\text{probability of }E\text{ conditioned on }A}{\text{probability of }E\text{ conditioned on }B}\times \text{prior odds of }A\text{ to }B.$$

This rule states that the posterior odds of $A$ to $B$ (conditioned on $E$) are a product of the prior odds of $A$ to $B$ multiplied by a ratio of probabilities for $E$ conditioned on $A$ and $B$, respectively. This ratio is known as the **Bayes factor**, or **likelihood ratio** in this instance. From the formula above, the likelihood ratio behaves as the updating factor for the prior odds due to the event $E$. In this sense, the likelihood ratio describes the relative support of events $A$ and $B$ for the event $E$: how much more (or less) probable was event $E$ when conditioned on $A$ compared to when it was conditioned on $B$? Understanding the likelihood ratio is the main goal of this book and it is revisited in Chapter \@ref(likelihood-ratio). Bayes' rule describes how prior odds must be updated in light of new conditioning information.

Bayes' rule gives us another mathematical expression that probability assignments must obey. This means that posterior odds or probabilities must be assigned coherently in that they must equal the prior odds multiplied by the likelihood ratio. This is useful for example when odds are based on a significant amount of epistemic uncertainty, since they can be hard to accurately quantify. Bayes' rule ensures that any quantified belief is still coherent. It does not remove subjectivity from the probability or odds assignment, but it does remove subjectivity from how that probability or odds assignment should be updated in light of new information.

Bayes' rule allows us to switch the conditioning information as we move from the likelihood ratio to the posterior odds. Probabilities for $E$ which are conditioned on $A$ and $B$, the likelihood ratio, are switched to probabilities for $A$ and $B$ conditioned on $E$, the posterior odds. This switching of conditioning information is known as **transposing the conditional**. Bayes' rule gives us the correct way to transpose the conditional, i.e. by using the prior odds. Incorrectly transposing the conditional is a tempting mistake which is easy to make in practice. The most famous example of this in the legal domain is known as the **prosecutor's fallacy**, which we will revisit in Chapter \@ref(propositions).

Despite being logical, Bayes' rule can lead to highly counter-intuitive results. 

## Example: guessing coin 1

The double coin toss example in Section \@ref(double-coin-toss) introduced a game in which player 1 won if two heads were tossed and player 2 won if two tails were tossed. The game ended in a draw if there was any combination of head and tail from the two tosses. We conditioned on the two possible outcomes from coin 1 in order to understand the probabilities of each player winning after the result of coin 1 was known. After each conditioning, one of the players' probabilities of winning reduced to 0 and the other's increased to 0.5. In this example we alter this game slightly.

Suppose that player 1 now tosses the coins and hides the outcomes from player 2. After tossing the coins, player 1 only tells player 2 whether player 2 has won the original game or not, i.e. whether double tails were tossed or not. If player 2 does not win, then they are given another chance to win by guessing the result of coin 1's toss. If their guess is correct, then player 2 wins this new game. As part of the background information, assume that player 1 always tells the truth and is tossing the coins in such a way that the odds are even for heads and tails for each coin. What should player 2's guess be?

This is a classic situation of epistemic uncertainty. Before the coin tosses, there is aleatory uncertainty - an unavoidable randomness to the future outcomes of these coin tosses. After the coin tosses, there is only epistemic uncertainty - player 1 knows the outcomes but player 2 does not. 

After the coin tosses, there are two possible situations:

1. Player 2 is told that they have won, in which case there is no uncertainty for them any longer because they know that the toss of coin 1 must have resulted in tails.

2. Player 2 is told that they did not win, and now they have epistemic uncertainty about the results of both coin tosses. Player 2 is now offered the opportunity to guess the outcome of coin 1 to win. This is direct uncertainty about the toss of coin 1 and can be quantified with or without the use of Bayes' rule. 

Player 2 can use probabilities to make the best guess for the second situation above. We will show both ways that this can be done. The possible outcomes of the double coin toss are presented as an expected frequency tree in Figure \@ref(fig:freq-tree-double-coin-bet-2) below.

```{r freq-tree-double-coin-bet-2, echo=FALSE,fig.cap="An expected frequency tree diagram of the coin guessing game. Player 2 expects not to win in 7500 out of the original 10,000 tosses (highlighted in bold). Out of those 7500 non-winning double tosses, 2500 came from coin 1 being tails, and 5000 came from coin 1 being heads. Since twice as many possible outcomes originate from coin 1 being heads the posterior odds of coin 1 being a head are 2:1.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000\ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(1, 1, 1, 2, 2, 2, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```


**Reasoning without using Bayes' rule**

Player 2 knows that one of the following three outcomes must have occurred: 

1. *either* coin 1 was a head and coin 2 was a head, 
2. *or* coin 1 was a head and coin 2 was a tail, 
3. *or* coin 1 was a tail and coin 2 was a head. 

For every 10,000 double tosses, the three outcomes above are expected in 7500 cases. In 5000 of these 7500, coin 1 is a head. In the other 2500, coin 1 is a tail. There are twice as many outcomes in which coin 1 is a head (5000 compared to 2500) and so the odds are 2:1 in favour of heads. These represent posterior odds, with the conditioning information being the fact that double tails has not occurred (or otherwise player 2 would have won the original game).

This means that player 2 should always guess a head when given the choice in this game, since the odds will be in their favour. This result might seem counter-intuitive at first glance. The key thing to understand is that the information of player 2 not winning via double tails should update their belief about the possible outcomes of the tosses. We will now apply Bayes' rule to this problem to see this more clearly.

**Reasoning using Bayes' rule**

We can verify that posterior odds of 2:1 in favour of coin being heads are correct by using Bayes' rule. Event A from Bayes' rule in this example is coin 1 being a head and event B is coin 1 being a tail. Event E is the knowledge that the result was not a double tails, since this is the information that player 1 reveals. This results in the following application of Bayes' rule:
$$\text{posterior odds of coin 1 head to coin 1 tail}=\text{LR} \times \text{prior odds of coin 1 head to coin 1 tail},$$
where the likelihood ratio (LR) equals the following ratio of conditional probabilities
$$\text{LR}=\frac{\text{probability of no double tails conditioned on coin 1 head}}{\text{probability of no double tails conditioned on coin 1 tail}}.$$
To apply Bayes' rule, we need to calculate the probabilities underlying the LR and the prior odds of coin 1 head to coin 1 tail.

The likelihood ratio in this example considers the following question: how much more (or less) probable is it to toss something other than double tails if coin 1 is a head compared to if coin 1 is a tail? To calculate this, we need to calculate the two probabilities in the equation above.

**LR - numerator**: The first is the probability of no double tails conditioned on coin 1 being a head. If coin 1 is a head, then it is certain that a double tails will not be tossed. That means that this probability is 1.

**LR - denominator**: The second probability we need is the probability of no double tails conditioned on coin 1 being a tail. Out of every 5000 tosses in which coin 1 is tails, we expect 2500 to lead to coin 2 being heads and 2500 to lead to coin 2 being tails. The outcomes are evenly split and so the probability is 0.5. 

**LR**: Using these two probabilities results in a likelihood ratio of $\frac{1}{0.5}=2$. In other words, an outcome other than double tails is twice as likely when coin 1 is a head compared to when coin 1 is a tail.

**Prior odds**: The second component of Bayes' rule that we needed was the prior odds of coin 1 being a head. These odds are evens as they were given by the background information for the tosses of the coins, i.e. the coins are tossed so as to guarantee even odds of heads and tails. The prior odds of coin 1 showing a head are 1:1.

Bayes' rule states that the posterior odds must be equal to the prior odds multiplied by the likelihood ratio. With a likelihood ratio of 2 and prior odds of 1:1, we obtain posterior odds of 2:1 in favour of a head; coin 1 is twice as likely to be a head when we know the outcome is not double tails. This result is the same as when we reasoned without using Bayes' rule, and so we have verified that it satisfies Bayes' rule. 

In this example it was possible to calculate the posterior odds without using Bayes' rule. This meant that the odds could be verified. In many real situations, the posterior odds are hard to quantify without using Bayes' rule. Bayes' rule offers an powerful and elegant solution to this problem since the prior odds and LR are often easier to quantify.

## Reliable probabilities {#reliable-probabilities}

The notion of probability used in forensic science is subjective and personal. Anyone is capable of assigning a probability to their personal uncertainties. But even if everyone does this to the best of their ability, some people have more information about some events than others. This is clear from the idea of experts versus non-experts: the expert's probability assignment for a situation within their expertise will be more **reliable** than that of the non-expert. Reliability here refers to the fact that the expert's knowledge is closer to all the available knowledge about a given topic, and so their best assessment is better than that of an uninformed non-expert. This is one of the reasons expert witnesses are used. Reliability of probability assignments is a key part of interpreting and using expert evidence.

We mentioned in a previous section that background information through assumptions was important since it acts as conditioning information that informs the probability. In practice what happens is that assumptions are made and stated upfront, so that they are inherently conditioned on when any probability is stated. We did this in the coin toss example when we assumed that the coin tosses were done in such a way so as to favour neither heads nor tails. In practice this might not be able to be guaranteed, and so instead the assumption might be justified by a reasonable 'default' state of knowledge, i.e. by assuming that the coin tosses are fair unless there is reason to believe otherwise. This is often done in practice when assigning probabilities to real events. Assumptions like this might be considered widely reasonable in the scientific community and therefore accepted by most experts.

In all cases, it is important that assumptions are made transparent so that they are open to reasonable audit. This makes probability assignments **assessable**. They can then in principle be assessed for their reliability. We saw that independence was an important assumption that can be made. One reason for this is because it makes complex probability calculations easier. However, it can result in unreliable probabilities when it is incorrectly assumed.

Another thing that can be checked is the general **calibration** of personal probability assignments. This is a general measure of how accurately an individual's quantified uncertainty reflects an empirical uncertainty. This can be found by asking experts to assign probabilities to events whose uncertainty is (or can be) precisely measured. For example, weather forecasters are tasked with providing probabilities for rain in a particular region of interest throughout each day. Their calibration can be measured by comparing their historical probability assignments to whether rain actually occurred or not over a long period of time. Forecasters who assign high probabilities of rain when it actually does rain and low probabilities when it actually does not rain are well calibrated. Better calibration leads to greater reliability. Experts in forensic science can demonstrate this by performing competency tests. This involves conducting simulated examinations of evidence when the underlying truth is known to the assessors, but unknown to the experts under assessment.

Some probability assignments can be empirically validated using repetitions of the same event. For example, a coin toss can be repeated many times and the proportion of observed heads can be compared to the expected proportion of heads given by a probability model. A large difference between the model and the empirical observations indicate that the probability assignment is not reliable. However, many events cannot be replicated under the same circumstances of interest for the probability assignment. When this cannot be done, e.g. for one-off events, then information from similar events or expert judgement can be used to inform that probability assignment. Performing repetitions is part of a wider idea of using empirical **data** to construct more reliable probabilities. If there are known data relating to an event of interest, then an individual's best assessment of the probability of that event should incorporate that data. The degree to which the data anchors the probability assignment will depend upon the quality of the data and the relevance of the data to the probability in question. Quality of the data can be influenced by factors such as the conditions under which it was gathered and the amount of data that has been collected. The relevance of the data can be subjectively determined by the expert based upon their domain expertise. 

## False positives {#false-positives}

False positives and false negatives are terms to describe mistakes in uncertain categorical assignments. Typically these are binary assignments where something or someone is either labelled as a **positive** case or a **negative** case, and the truth about them actually being a positive or negative case is unknown.

If the truth is that they are a negative case, but they were mistakenly labelled as a positive case, then the assignment is a **false positive**. If the truth is that they are a positive case, but they were mistakenly labelled as a negative case, then the assignment is a **false negative**. If the labels were correct, then the assignment was a **true positive** or **true negative**, respectively. This information is presented in Table \@ref(tab:intro-fp-table).

```{r intro-data, include=FALSE}
introDf <- tibble("Truth" = c("Positive", "Negative"),
                  "Labelled positive"=c("True positive",
                                        "False positive"),
                  "Labelled negative"=c("False negative",
                                        "True negative"))
```


```{r intro-fp-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
introDf %>%
  mutate("Labelled positive" = cell_spec(
    `Labelled positive`, color = colPal[c(6, 7)], bold = T
  )) %>%
  mutate("Labelled negative" = cell_spec(
    `Labelled negative`, color = colPal[c(7, 6)], bold = T
  )) %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Labelling statistics based on the assigned label and the underlying truth.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")

```

For example, when testing someone for a specific disease, we are uncertain about whether or not they have the disease before applying the test. The test results categorise them as either positive or negative for the disease, but it is never absolutely guaranteed to be correct. Even the most reliable tests make mistakes, even if that's only very rarely. The test result should decrease our uncertainty about whether the tested person has the disease or not, but it can't totally eliminate it. The best tests will greatly decrease our uncertainty, and poor ones won't change it much.

If many assignments of positive/negative have been made under controlled conditions, e.g. when the underlying truth of positive or negative is known, then one can determine the **rate** of true/false positives/negatives. This rate corresponds to the probability of each entry in Table \@ref(tab:intro-fp-table) occurring.

The probability of a false positive occurring is called the **false positive rate** and the probability of a false negative occurring is called the **false negative rate**.

The probabilities of true assignments have different names. The probability of a true positive is called the **sensitivity** and the probability of a true negative is called the **specificity**. 

The **base rate** of a characteristic is the probability that when we randomly select an object from the population of interest, then that selected object has the specified characteristic. This is commonly called the **prevalence** when the characteristic that we are interested in is a disease. 

## Example: diagnostic tests {#exm-test}

The following example is adapted from @aitken2010. 

```{r test-data, include=FALSE}
testDf <- tibble("Disease" = c("Present",
                               "Absent",
                               "Total"),
                 "Test positive"=c(99, 495, 594),
                 "Test negative"=c(1, 9405, 9406),
                 "Total"=c(100, 9900, 10000))
```

The risk of a disease is 1% in a relevant population of 10,000 people. This means that the disease affects 100 people out of the total 10,000, and it does not affect the other 9,900.

A diagnostic test has been created for this disease. The test has a sensitivity of 99%; out of the 100 people who have the disease, 99 of them have a positive test result. The final 1 person tests negative despite having the disease. This person receives a false negative result.

The test has a specificity of 95%; out of the 9,900 people who do not have the disease, 9,405 have a negative test. The other 495 people test positive despite not having the disease. These people receive false positive results. This information is displayed more clearly in Table \@ref(tab:test-table).

```{r test-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
testDf %>%
  mutate(Disease = cell_spec(
    Disease, bold = T
  )) %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'The number of people who are affected by the disease and their diagnostic test results.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")

```

This test has high sensitivity (99%) and high specificity (95%), which makes it sound reliable. However, remember what these terms mean: the probability of testing positive given that you do have the disease (sensitivity), and the probability of testing negative given that you don't have the disease (specificity). This probability is conditioned upon knowing whether the person has the disease or not.

In practice, people do not know whether they have the disease or not, and that is why they get tested. This means that this is not useful conditioning information in practice. The information that people do have is whether their specific test result was positive or negative, and so this is the information that the probability should be condition on. What's the probability of actually having the disease given the result of the test? 

Look back to the columns of Table \@ref(tab:test-table). Consider the negative results first. A total of 9,406 people from our population of 10,000 tested negative. Out of these 9,406 who tested negative, 9,405 did not have the disease. There was only a single individual who tested negative despite having the disease. This means that a negative test result is a great (but not perfect) indicator for not having the disease. 

Now consider the positive results. A total of 594 people from our population of 10,000 tested positive. Out of these 594 who tested positive, only 99 (~17%) actually have the disease. The large majority of people who tested positive, 495 (~83%) of the 594, do not really have the disease. This can be seen more clearly in Figure \@ref(fig:freq-tree-test).

```{r freq-tree-test, echo=FALSE,fig.cap="An expected frequency tree diagram of the disease testing example. Out of the 594 people who test positive (shown in bold font), 99 (~17%) have the disease. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \npeople", "100\ndisease", "9,900\nno disease", "99\npositive", "1\nnegative", "495\npositive", "9,405\nnegative")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(rep(1, 3), 2, 1, 2, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

If a randomly selected individual from this population tests positive, then it is highly likely that they do not have the disease. A positive result for this test is a terrible indicator of whether someone has the disease. This is a counter-intuitive result that can be explained using the logic of Bayes' rule. 

This phenomenon is caused by the very low **base rate** (prior probability) of the disease. This is the same as the risk of having the disease for people within the population, which was 1%. A randomly selected individual has a very low probability of having the disease prior to being tested. The test has very high sensitivity and so it is able to detect almost all of the true positives. The issue was that it tested many people who didn't have the disease, and so even a low error rate led to many false positives. Due to the **base rate** being so low, the number of true positives (99) was much smaller than the number of false positives (495). This meant that the positive results largely consisted of false positives.

```{block eval=FALSE, include=isDynamicOutput()}
### Interactive disease testing example

The example above made the point that the base rate was an important factor in determining the proportion of true positive tests. This effect is demonstrated in the interactive tool below. By changing the base rate from low to high and vice versa, you may compare the number of true positive tests to the number of false positive ones as a function of the base rate of the disease. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDiseaseTest",TRUE), height = '1100px')
}
```

## Example: doping {#exm-doping}

The following example has been adapted from @statsprimer2020.

Table \@ref(tab:test-table) and Figure \@ref(fig:freq-tree-test) present the technical diagnostic testing information in a format which is easier to understand and base decisions on. However, it is often not presented this way in practice and so must first be `translated' from the raw numerical information. 

>A test designed to detect athletes who are doping is claimed to be '95% accurate'. If an athlete is doping then the test returns positive 95% of the time, and if the athlete is not doping then the test returns negative 95% of the time. It is suspected that around 1 in every 50 athletes dope. An athlete tests positive for doping using this test during a random drugs screening. How likely is it that they are really doping?

The answer is around 28%, pause for a moment and see whether that answer comes to you from reading the above text. After reflecting on the text, continue through the example below, where we present this same information in a more familiar format. 

We can convert some of the written information into our technical definitions. The second sentence states that the sensitivity and specificity are both 95%, although those words are not explicitly used. The base rate for doping is given as approximately 2%.

We haven't been given a relevant population size to use natural frequencies to describe these rates, but we can imagine one in order to aid our understanding. Since we are going to use a hypothetical population of athletes, we will have to talk in terms of what we would expect from such a population and so can use expected frequencies.

Assume, for clarity, that we have a relevant population of 10,000 athletes. Using the base rate, we expect 200 (2%) of these to be doping and 9,800 (98%) not to be doping. The sensitivity tells us that out of the expected 200 athletes who are doping, the test is expected to return positive for 190 (95%) of them and negative for 10 (5%) of them. We expect 10 false negatives. 

Out of the expected 9,800 athletes who are not doping, the specificity tells us to expect 9,310 (95%) to test negative. We expect 490 (5%) of these non-doping athletes to test positive; we expect 490 false positives.

We expect a total of 680 positive tests and 190 (~28%) of those positive tests to be from an athlete who is doping. The answer to our original question is that given a positive test result, we expect the athlete to be doping roughly 28% of the time. This information is presented in the expected frequency tree in Figure \@ref(fig:freq-tree-doping). 

```{r freq-tree-doping, echo=FALSE,fig.cap="An expected frequency tree diagram of the doping example. Out of the 680 athletes who test positive (shown in bold font), 190 (~28%) are doping. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \nathletes", "200\ndoping", "9,800\nnot doping", "190\npositive", "10\nnegative", "490\npositive", "9,310\nnegative")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

colPal <- colorblind_pal()(8)
# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(rep(1, 3), 2, 1, 2, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```


```{block eval=FALSE, include=isDynamicOutput()}
### Interactive Doping Example

Below is an interactive tool for the doping example. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDopingTest_probabilities","DopingTest",TRUE), height = '1100px')
}
```

```{block eval=FALSE, include=isDynamicOutput()}
## Summary: probability

Use the activity below to create a summary of the key points from this chapter.
```
``` {r probability-summary-questions, echo=FALSE}
if(isDynamicOutput()){
  knitr::include_url(paste0(QUESTIONS_HOST,"Probability-Summary.html"), height=800)
}
```

## More information

In this chapter on Probability we focussed on probability models and the expectations that a probability model forms for real events, e.g. a number of coin tosses. However, these expectations are often not exactly what is observed in practice, e.g. with real tosses of the coin. Analysing empirical events such as this moves from the field of probability to the field of statistics, and is currently outside the content of this book. If you would like to explore statistics for the coin toss examples presented in this chapter, then you may do so in the [interactive application](https://lrcfs.dundee.ac.uk/apps/interactive-lr/) which accompanies this book under the 'Coin Toss - Single Toss Samples' and 'Coin Toss - Double Toss Samples' tabs.

## Research study

If you are taking part in our research study, please return to the survey now to answer the questions about this chapter.
