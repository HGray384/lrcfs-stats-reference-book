# Probability to describe uncertainty {#probability}

```{r package-load-probability, include=FALSE}
# check and load the libraries
library("igraph")
library("ggthemes")
library("kableExtra")
library("tidyverse")
library("plotly")
library("grid")
library("plyr")

source("./code/useful-variables.R")
source("./code/helper-functions.R")
```

We saw in the previous chapter that there is uncertainty in scientific evidence. By asking a few simple questions about the uncertainties in a particular context, we demonstrated a systematic approach to assessing the uncertainty which can easily be applied to forensic evidence. As part of this process, the expert assesses the magnitude of their personal uncertainty given the scientific expertise and experience that they have, e.g. out of every 1000 similar case circumstances, the expert believes 200 would yield a fibre match if the suspect had truly been at the crime scene. This magnitude is based upon a probability assignment by the expert. In this chapter we discuss probability, describing what it is and using examples to demonstrate some of its useful properties as a framework for handling uncertainty.

## Quantifying uncertainty

Quantifying uncertainty is a systematic way of assimilating and comparing uncertainties. This means that different personal uncertainties for the same object can be assessed consistently and also that personal uncertainties for different objects can be compared. This is because the framework of mathematics forces quantities to obey a coherent and consistent set of logical rules. The subset of mathematics which handles uncertainty is known as probability. The main benefit of using probability is the framework of logic that it enforces, rather than the quantification of uncertainty (although this is useful).

A probability is a number between 0 and 1 that describes the magnitude of uncertainty for the occurrence of an event. The probability must obey certain rules which we will show in subsequent examples in this chapter. A probability of 0 means that the event is impossible whilst a probability of 1 means that an event is certain. Uncertainty is described by probabilities which fall between 0 and 1. Probabilities of 0.5 describe an event whose occurrence is exactly as likely as its non-occurrence. Events whose occurrence is less likely than not should have a probability less than 0.5 on the scale, whilst events whose occurrence is more likely than not should have a probability greater than 0.5 on the scale. How close these probabilities are to 0 and 1 should reflect an individual's magnitude of uncertainty. Since uncertainty is personal then it follows that probabilities are personal too. In forensic science, personal probabilities are generally interpreted as an individual's degree of belief in the occurrence of an event. Not everyone agrees with this interpretation, but this historical debate is outside the scope of this book.

Constructing probabilities to describe uncertainty is often done by assuming a probabilistic **model** for how the uncertainty is expected to behave in reality. Since the process that is being modelled is uncertain, the expectations might not be exactly what is observed in practice. This gives rise to the phrase "all models are wrong, but some are useful". The most useful models can accurately align an individual's magnitude of uncertainty to a quantitative probability, accepting that this can never be done perfectly.

We focus only on quantifying direct uncertainty, since this is what is done in practice when interpreting evidence. Verbal qualifiers are given for indirect uncertainties instead and are not covered here.

## Example: coin toss {#single-coin-toss}

The outcome of a coin toss is uncertain in most cases. We can consider some of the questions from the previous section to describe this uncertainty.

**What uncertainties are there?** 

The uncertainty is whether the coin will land heads-up or tails-up as a result of the toss.

**What are the sources of this uncertainty?** 

* the coin toss process
  + aleatory uncertainty: there is randomness to the coin toss process
  + epistemic uncertainty: is the person tossing the coin able to do so without favouring one side of the coin over the other? This can be reduced by learning about the person tossing the coin, e.g. from previous tosses.
* potential double sided coin
  + epistemic uncertainty: can be eliminated by checking both sides of the coin

Assume that the coin is checked and has one head and one tail, and that the person tossing the coin is doing so fairly. There is now only aleatory uncertainty about the outcome. This is an irreducible uncertainty about the coin toss. 

**What is the magnitude of uncertainty?** 

The coin toss has only two possible outcomes, head or tail. The probability that one of these outcomes must occur is 1, since it is certain. This means that the events are **exhaustive**, i.e. the events exhaust all possible outcomes.

A head and a tail cannot occur together and so they are also known as **mutually exclusive**. When outcomes are mutually exclusive, then their probabilities can be added together - this is one of the laws of probability. Because both outcomes of the coin toss are also exhaustive then this means that whatever the probability of the individual outcomes, they must sum to 1.
<!-- [MAYBE ADD VENN DIAGRAM FOR VISUAL AID] -->

If a head and a tail are equally likely, then due to the rules above they must each have a probability equal to 0.5. This quantified uncertainty is the result of the logic of probability. These values form a probability model for the outcomes of the coin toss. This probability model can be checked to confirm that it aligns with specified beliefs. For example, since we hold the belief that each probability is 0.5, then we expect that the outcomes of multiple coin tosses should be evenly distributed between heads and tails. 

Notice that if we did not ignore some of the epistemic uncertainties about the coin toss (e.g. if we did not trust the coin flipper to be fair), then the probability model would need to be adjusted to align with this belief, e.g. by changing the probability of heads from 0.5.

```{block eval=FALSE, include=isDynamicOutput()}
## Interactive coin toss

In the example below, there are two tree diagrams. The probability tree displays the degree of belief for the coin toss in terms of the probabilities for the outcomes. The expected frequency tree displays the expected number of heads and tails corresponding to those probabilities for a fixed number of coin tosses. 

Use the sliders below to change the probability of heads for the coin toss and also the number of tosses with which to view the expected outcomes. Firstly, fix the probability of heads to 0.5, since this is what we did in the example above. Then change the number of tosses to see that the expected number of heads and tails are equal. Next, fix the probability of heads to another number and see how this affects the expected outcomes.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabCoinTree",NULL,TRUE), height = '750px')
}
```
```{block eval=FALSE, include=isDynamicOutput()}
The expected frequency tree in the example above shows what the probability model expects to occur for a specified probability of a head. It does not necessarily show what will be observed in practice because there is an unavoidable uncertainty about the outcomes (aleatory uncertainty). Tossing the coin, observing the outcomes, and then using that to refine the probability model falls into the realm of statistics, which is not covered in this book. Interactive examples exploring this topic can be found in our [online application](https://lrcfs.dundee.ac.uk/apps/interactive-lr/).
```
```{block eval=TRUE, include=!isDynamicOutput()}
### Expected frequency tree

Below is an example of an expected frequency tree. The expected frequency tree displays the expected number of heads and tails corresponding to the probabilities of heads and tails for a fixed number of coin tosses. 

A probability of heads of 0.5 means that we expect 5000 or every 10,000 tosses to result in heads, and the other 5000 to result in tails.

```
```{r echo=FALSE, freq-tree-coin-toss, echo=FALSE, fig.cap="An expected frequency tree diagram of the coin toss example. Out of 10,000 cosses, we expect 5000 to be heads and 5000 to be tails. The probability of heads is 0.5 and is equal to the probability of tails. ", out.width = '80%', fig.align = 'center'}
if(!isDynamicOutput()){
  e <- c(1, 2, 1, 3)
  v <- c("10,000\ntosses", "5000\nheads", "5000\ntails")
  createBinaryTree(e,v)
}
```

<!-- To view all of the interactive examples in this book, please visit [Interactive Stats Book](`r getInteractiveLink()`) (`r getInteractiveLink()`). -->

## Personal probabilities

We mentioned in the previous section that probability was personal because individuals have different knowledge and beliefs. Some differences in beliefs might be more or less reasonable since they can be based upon more or less reliable information. 

Probability assignments based upon more reliable and generally agreed information will be more **objective**. For example, the scientific evidence underlying the processing and analysis of full DNA profiles for single donors is well established and so experts are generally confident and in agreement about assigning probabilities in this circumstance.

Probability assignments which are based upon less reliable or less generally agreed information will be more **subjective**. For example, the transfer and persistence of DNA on surfaces is an active area of research for scientists and so probabilities involving these circumstances may be subjective. Subjective probabilities are not necessarily bad because they can still be the best assessment of the available knowledge. This might happen for example when there is very limited published scientific literature about a particular scenario. Even though there is limited published information, an expert may have relevant previous case experience in that scenario and so may be able to assign subjective probabilities. In other words, subjective probability assignments are not necessarily (and should not be) arbitrary.

For repeatable events like the coin toss we can check and update our probabilities using empirical data, e.g. repeated tosses of the coin. This can be seen as gaining a better understanding of the aleatory uncertainty about the distribution of the outcomes and eliminating epistemic uncertainty about whether the coin tossing process was being done fairly. With enough repetition, personal probabilities which were initially different between individuals can converge to the same value as more objective information becomes known. In criminal cases, the opportunity to repeat events and gather scientific data from the same case circumstances can be limited. This is because some events are one-off and difficult (if not impossible) to completely replicate in scientific experiments. In these situations, probabilities can still be informed by other available empirical data however, e.g. by observations from similar circumstances, and can also still be informed by expert knowledge. This knowledge should be disclosed and be available for audit by the court.

## Conditioning on information

Information which is used to construct a probability is called **conditioning** information. This is because we are constructing a probability which is conditional on that information, otherwise known as a **conditional probability**. The process of using information this way to construct such a conditional probability is itself called **conditioning**.

This means that probability is conditional on our current state of individual knowledge. Conditioning information can be broken down into two types:

* information that is emphasised in the conditioning of the probability. This information is explicit and critical to the application of the probability. For example, the hypotheses of the prosecution and defence.
* all other information. This includes assumptions and other contextual information, otherwise known as **background information**. An example of this is the relevant case circumstances that are agreed by both the prosecution and defence. This information can be tedious to state each time a probability is mentioned and so it is often stated once and then only mentioned again if it changes. 

The distinction between these two types of information is context-dependent since it is just a matter of emphasising different contextual information. Information that is important to emphasise in one context may be background information in another. For example, when interpreting fibre evidence recovered from a crime scene, the expert may wish to condition on the person of interest having been present at the crime scene. This would be background information in a case in which the person of interest admits to having been present at the crime scene. Otherwise it would be disputed by the person of interest and thereby become a critical piece of conditioning information. 

## Conditioning on the past and future

A useful application of conditional probability is to condition on possible outcomes of future events to see how this affects probabilities of interest. For example, banks who lend money will need to consider the probability that loanees will be able to repay that money (with interest). A future event which would affect that probability could be the lonees' employment status. If their employment is unstable and their probability of repayment would greatly decline if they were to lose their job, then the bank might be less likely to lend to them compared to if their probability of repayment was unaffected by their employment status (e.g. if they own physical assets that the bank could seize if they fail to repay). Considering possible future events and risks helps to make uncertain decisions in the present. 

We can also condition on possible events in the past. This is useful for example when the causes of past events are unknown. This is epistemic uncertainty. In this case, we can consider the probability of the observed outcome conditioned on multiple candidate causal events. These conditional probabilities can then be compared to determine the support of each of the causal events for the observed outcome. This is how conditional probabilities are used for interpreting forensic evidence: we look at the events which the defence and prosecution assert and compare how likely the observed evidence would have been when it is conditioned on each hypothesis. For example, when interpreting fibre evidence recovered from the clothing of a person of interest, the expert must consider how probable this evidence would have been in light of what both the prosecution and defence claim to have happened.

## Example: double coin toss {#double-coin-toss}

Suppose now that we toss 2 coins, labelled coin 1 and coin 2. Coin 1 is tossed first and the outcome is recorded. Then coin 2 is tossed and its outcome is recorded. Both coins are tossed in such a way that guarantees a 0.5 probability of a head. The outcome of the toss of coin 1 can be used as conditioning information to see how it affects the probabilities of the outcomes of coin 2. This means that the uncertainty of interest is the toss of coin 2 conditioned on the outcome of the toss of coin 1. Question: what is the probability of getting a head with coin 2 after coin 1 has shown a head?

Conditioning information:

* coin 1 showing head
* background information - each coin has a 0.5 probability of a head.

With the above conditioning information the question becomes: is the outcome of coin 2 affected by the outcome of coin 1? If not, then the answer to the original question is 0.5 since the background information guarantees a 0.5 probability of a head for coin 2. The idea that the outcome of coin 2 is unaffected by the outcome of coin 1 is known as **independence**. This can sometimes be missed as an underlying unchecked assumption. It is helpful to add this to the background information so that the probability model is fully specified. All relevant assumptions can then be assessed by someone else.

Updated conditioning information:

* coin 1 showing head
* background information - each coin has a 0.5 probability of a head, tosses from coin 1 and 2 are independent of one another.

Figure \@ref(fig:freq-tree-double-coin) shows an expected frequency tree for this information for 10,000 tosses of the coins.

```{r freq-tree-double-coin, echo=FALSE,fig.cap="An expected frequency tree diagram of the double coin toss example. Out of every 10,000 double coin tosses, we expect 2500 to be double heads. The probability of getting two heads is 0.25.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
createBinaryTree(e,v)
```

After the 5000 tosses in which coin 1 is heads we expect 2500 heads from coin 2. This reflects the belief that coin 2 has a 0.5 probability of a head and is independent of coin 1. 

The expected frequency tree can also help to answer other questions, such as: what is the probability of both coins showing a head? Out of the 10,000 tosses, 2,500 are expected to result in both coins showing heads. This reflects the belief that there is a $\frac{2500}{10000}=\frac{1}{4}=0.25$ probability of obtaining a double heads.

```{block eval=FALSE, include=isDynamicOutput()}
## Interactive double coin toss

The example below displays the probability and expected frequency trees for the double coin toss. If the plots are not displaying correctly, then please refresh your page. You can view this example and others in our [online application](https://lrcfs.dundee.ac.uk/apps/interactive-lr/). Use the sliders below to change the probability of a head for both coins and also the number of double tosses for the expected outcomes. Here is an activity to try:

The **multiplication rule** for independent events states that the probability of two independent events both occurring can be obtained by multiplying the probability of each independent event together. Verify that this probability model for the double coin toss satisfies the multiplication rule by:

* Finding the probability for double heads on the probability tree and checking that it equals the probability of a head multiplied by itself. (Use an online calculator if needed!)
* Working out the probability of tails from the probability of heads. (Hint: they are exhaustive and mutually exclusive.)
* Finding the probability for double tails on the probability tree and checking that it equals the probability of a tail from the previous step multiplied by itself.
* Finding the probability of a coin 1 head with a coin 2 tail on the probability tree and checking that it equals the probability of a head multiplied by the probability of a tail. (This is the same for a coin 1 tail with a coin 2 head.)
* Changing the probability of a head to another number and repeating the previous steps once more.
  
```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDoubleCoinTree",NULL,TRUE), height = '800px')
}
```

## Example: coin toss game

We can also consider two events for the same example which are not independent. Suppose there is a game that is decided by the result of the double coin toss. Player 1 wins if the coin tosses result in double heads and player 2 wins if the coin tosses result in double tails. If neither player wins then a draw is called and the coins are tossed again. Since double heads is just as likely as double tails, the probability is 0.25 for each, then both players are equally likely to win prior to the first coin being tossed. We can see this from the highlighted Figure \@ref(fig:freq-tree-double-coin-bet) below.

```{r freq-tree-double-coin-bet, echo=FALSE,fig.cap="An expected frequency tree diagram of the double coin toss game. Out of every 10,000 tosses, 2500 are double heads and 2500 are double tails. This means that each player has equal an probability of winning of 0.25. The remaining 5000 tosses result in a draw and so the probability of a draw is 0.5.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000\ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads\nplayer 1 wins", "2500\ncoin 2 tails\ndraw", "2500\ncoin 2 heads\ndraw", "2500\ncoin 2 tails\nplayer 2 wins")
fw <- c(1, 1, 1, 2, 1, 1, 2)
createBinaryTree(e,v,fw)
```

Out of 10,000 double tosses, player 1 is expected to win 2500, player 2 is expected to win 2500, and 5000 double tosses are expected to result in a draw. However, whilst the outcome of coin 1 does not affect the outcome of coin 2, it does affect the winning probabilities for this game. For example, we can condition on coin 1 being a head as before.

Conditioning information:

* coin 1 showing head
* background information - each coin has a 0.5 probability of a head, tosses from coin 1 and 2 are independent of one another.

With the above conditioning information (i.e. only considering the left-hand branch of the tree), there is now a 0.5 probability that player 1 wins (2,500 out of 5,000), a 0.5 probability that there is a draw (2,500 out of 5,000), and it is impossible that player 2 wins in this round (0 out of 5,000). Conditioning on coin 1 being heads has increased the probability of player 1 winning from 0.25 to 0.5, and it has decreased the probability of player 2 winning from 0.25 to 0. The opposite occurs when the outcome of coin 1 is a tail. The probability of a draw remains unchanged at 0.5 before coin 1 is tossed, and 0.5 after it is tossed regardless of the result. In summary, the winning outcomes are not independent of the result of coin 1, but the draw is independent of it.

## Odds

Odds are another way of expressing probabilities. Odds are given as a ratio of probabilities so that it is clear how much more or less likely one event is compared to another. In this sense, probability is an absolute measure of uncertainty, whilst odds are a relative measure of uncertainty. Any two probabilities can be compared together to create odds. One pair which is commonly used is the probability of an event occurring versus the probability of the same event not occurring.

For example, the coin toss was just as likely to result in a head as it was to result in a tail. This is expressed as odds written as 1:1, which is spoken as '1-to-1' or more commonly 'evens'. Odds of 1:1 means that we can separate the possible outcomes into a total of $1+1=2$ parts which are equally probable. One part represents the probability of a head and the other part represents the probability of a tail.

We can convert from odds to probability for exhaustive events as follows. For the single coin toss we had odds of 1:1 for heads. The total probability, which must be 1, is made up of 2 equally sized parts. One of these parts represents a head, and so the probability of a head (and similarly for a tail) is $\frac{1}{2}=0.5$. Suppose instead we have odds of 1:9 for an event occurring versus not occurring. This means that the total probability, which must be 1, is made up of 10 equally sized parts. One of these ten parts represents the event occurring, and so its probability is $\frac{1}{10}=0.1$. The other nine parts represent the event not occurring, and so the probability of non-occurrence is $\frac{9}{10}=0.9$, Odds which are used to represent probabilities before any conditioning occurs (or are only conditioned on background information), are known as **prior odds**. The conversion from odds to probabilities is more challenging when the events are not exhaustive, but we do not consider that here.

Converting from probabilities to odds is much simpler since we only measure how much bigger one probability is than the other. For example, the probability of player 1 winning the double coin toss game  by getting double heads was 0.25. That meant that the probability of player 1 not winning before any coins had been tossed was 0.75. Since the probability of player 1 not winning (0.75) was three times larger than the probability of player 1 winning (0.25), the prior odds of player 1 winning were 1:3.

Prior odds can be updated using conditioning information. These updated odds are known as **posterior odds**. For example, as part of the double coin toss game we conditioned on the result of coin 1 being a head. Before the conditioning, the probability of player 1 winning was 0.25; the prior odds of player 1 winning were 1:3. Conditioning on coin 1 being a head resulted in the probability of player 1 winning rising to 0.5. This means that the posterior odds (i.e. **after** observing coin 1 being a head) of player 1 winning versus not winning were evens (1:1) for that round. 

Another reason that odds are useful is because of the simplicity that they give to a very important mathematical result, which we discuss in the next section. 

## Bayes' rule {#bayes}

Suppose we have a probability assigned for an event $A$ and we observe an event $E$ that is relevant to $A$. How should we update the probability assigned to $A$ in light of the relevant information from $E$? An example is a personal probability that it will rain next Saturday. Then on Friday, it rains throughout the day and into the evening. Your personal probability might increase now that you have observed rain just prior to your original probability assignment. But by how much should it increase exactly? This is a useful question that is answered using Bayes' rule.

<!-- 1. use the multiplication rule of probability to derive Bayes' rule for the probability of an event $A$ in light of information from an event $E$, -->
<!-- 2. state Bayes' rule for the probability of another event $B$ in light of information from the same event $E$, -->
<!-- 3. divide the conditional probability from step 1 by the conditional probability from step 2 in order to obtain posterior odds of $A$ to $B$ in light of $E$. -->

<!-- We go through these steps below in order to gain an intuition behind Bayes' rule. -->

<!-- **1. use the multiplication rule of probability to derive Bayes' rule for the probability of an event $A$ in light of information from an event $E$** -->

<!-- For events $A$ and $E$, the multiplication rule from Section \@ref(double-coin-toss) states that the joint probability of $A$ and $E$ co-occurring is -->
<!-- $$\text{probability of }A \text{ and }E = \text{probability of }A \text{ conditioned on }E \times \text{probability of }E.$$ -->
<!-- The joint probability can be expressed differently by conditioning on $A$ instead of $E$: -->
<!-- $$\text{probability of }A \text{ and }E = \text{probability of }E \text{ conditioned on }A \times \text{probability of }A.$$ -->
<!-- These two expressions are equivalent since they describe the same joint probability. This means that we can write the following: -->
<!-- \begin{align*} -->
<!--   &\text{probability of }A \text{ conditioned on }E \times \text{probability of }E \\ -->
<!--   &=\text{probability of }E \text{ conditioned on }A \times \text{probability of }A. -->
<!-- \end{align*} -->
<!-- Since we are interested in an expression for the probability of $A$ conditioned on $E$, we can isolate that term by dividing both sides of the equation above by the probability of $E$ (assuming it is not equal to zero). This gives Bayes' rule: -->
<!-- $$ \text{probability of }A\text{ conditioned on }E = \frac{\text{probability of }E\text{ conditioned on }A \times \text{probability of }A}{\text{probability of }E}.$$ -->

<!-- **2. state Bayes' rule for the probability of another event $B$ in light of information from the same event $E$** -->

<!-- Similarly for another event $B$, that is also related to $E$, Bayes' rule states: -->
<!-- $$ \text{probability of }B\text{ conditioned on }E = \frac{\text{probability of }E\text{ conditioned on }B \times \text{probability of }B}{\text{probability of }E}.$$ -->

<!-- **3. divide the conditional probability from step 1 by the conditional probability from step 2 in order to obtain posterior odds of $A$ to $B$ in light of $E$** -->

<!-- Dividing the terms in the equation for $A$ by the equation for $B$ gives a formula for the ratio of the probabilities of $A$ and $B$: -->
<!-- $$ \frac{\text{probability of }A\text{ conditioned on }E}{\text{probability of }B\text{ conditioned on }E}=\frac{\text{probability of }E\text{ conditioned on }A}{\text{probability of }E\text{ conditioned on }B}\times \frac{\text{probability of }A}{\text{probability of }B},$$ -->
<!-- where the probability of $E$ from both equations cancels out and so is ignored. The terms on the left hand side of the equation are the posterior odds of $A$ to $B$ conditioned on $E$. The ratio on the far right hand side is the prior odds of $A$ to $B$. When expressed like this, Bayes' rule provides a link between the prior and posterior odds:   -->
<!-- $$\text{posterior odds of }A\text{ to } B=\frac{\text{probability of }E\text{ conditioned on }A}{\text{probability of }E\text{ conditioned on }B}\times \text{prior odds of }A\text{ to }B.$$ -->

**Bayes' rule** is a mathematical rule for updating probability assignments in light of new information. For an event of interest $A$ and  new information from an event $E$, the probability version of Bayes' rule is: 
$$ \text{probability of }A\text{ conditioned on }E = \frac{\text{probability of }E\text{ conditioned on }A \times \text{probability of }A}{\text{probability of }E}.$$
It provides a means of obtaining a conditional probability for the event of interest based upon the new information (the left-hand side of the equation) by using probabilities that may already be assigned (the right-hand side of the equation).

For the odds of two events $A$ and $B$ in light of $E$, Bayes' rule is can also be expressed as:
$$ \frac{\text{probability of }A\text{ conditioned on }E}{\text{probability of }B\text{ conditioned on }E}=\frac{\text{probability of }E\text{ conditioned on }A}{\text{probability of }E\text{ conditioned on }B}\times \frac{\text{probability of }A}{\text{probability of }B},$$
The terms on the left hand side of this equation are the posterior odds of $A$ to $B$ conditioned on $E$. The ratio on the far right hand side is the prior odds of $A$ to $B$. When expressed like this, Bayes' rule provides a link between the prior and posterior odds:  
$$\text{posterior odds of }A\text{ to } B=\frac{\text{probability of }E\text{ conditioned on }A}{\text{probability of }E\text{ conditioned on }B}\times \text{prior odds of }A\text{ to }B.$$

This rule states that the posterior odds of $A$ to $B$ (conditioned on $E$) are a product of the prior odds of $A$ to $B$ multiplied by a ratio of probabilities for $E$ conditioned on $A$ and $B$, respectively. This ratio is known as the **Bayes factor**, or **likelihood ratio** (LR) in this instance. The LR acts as the updating factor for the prior odds due to the event $E$. It describes how much more (or less) probable event $E$ is when conditioned on $A$ compared to when it is conditioned on $B$. We focus on the LR in Chapter \@ref(likelihood-ratio).

Bayes' rule gives us mathematical expressions that probability assignments must obey. This is another means of assuring logical values for conditional probability assignments. It does not remove subjectivity from the probability or odds assignment, but it does remove subjectivity from how that probability or odds assignment should be updated in light of new information.

Bayes' rule switches the conditioning information as we move from the LR to the posterior odds. Probabilities for $E$ which are conditioned on $A$ and $B$ in the LR are switched to probabilities for $A$ and $B$ conditioned on $E$ in the posterior odds. This is known as **transposing the conditional**. Bayes' rule gives us the correct way to transpose the conditional using the logic of probability. The legal domain has a tricky and tempting trap for incorrectly transposing the conditional, known as the **prosecutor's fallacy**. We show this in Chapter \@ref(propositions).

## Example: guessing coin 1

The double coin toss example in Section \@ref(double-coin-toss) introduced a game in which player 1 won if two heads were tossed and player 2 won if two tails were tossed. The game ended in a draw if there was any combination of head and tail from the two tosses. We conditioned on the two possible outcomes from coin 1 in order to understand the probabilities of each player winning after the result of coin 1 was known. After each conditioning, one of the players' probabilities of winning reduced to 0 and the other's increased to 0.5. In this example we alter this game slightly.

Suppose that player 1 now tosses the coins and hides the outcomes from player 2. After tossing the coins, player 1 only tells player 2 whether player 2 has won the original game or not, i.e. whether double tails were tossed or not. If player 2 does not win, then they are given another chance to win by guessing the result of coin 1's toss. If their guess is correct, then player 2 wins this new game. As part of the background information, assume that player 1 always tells the truth and is tossing the coins in such a way that the odds are even for heads and tails for each coin. What should player 2's guess be?

This is a classic situation of epistemic uncertainty. Before the coin tosses, there is aleatory uncertainty - an unavoidable randomness to the future outcomes of these coin tosses. After the coin tosses, there is only epistemic uncertainty - player 1 knows the outcomes but player 2 does not. 

After the coin tosses, there are two possible situations:

1. Player 2 is told that they have won, in which case there is no uncertainty for them any longer because they know that the toss of coin 1 must have resulted in tails.

2. Player 2 is told that they did not win, and now they have epistemic uncertainty about the results of both coin tosses. Player 2 is now offered the opportunity to guess the outcome of coin 1 to win. This is direct uncertainty about the toss of coin 1 and can be quantified with or without the use of Bayes' rule. 

Player 2 can use probabilities to make the best guess for the second situation above. We will show both ways that this can be done. The possible outcomes of the double coin toss are presented as an expected frequency tree in Figure \@ref(fig:freq-tree-double-coin-bet-2) below.

```{r freq-tree-double-coin-bet-2, echo=FALSE,fig.cap="An expected frequency tree diagram of the coin guessing game. Player 2 expects not to win in 7500 out of the original 10,000 tosses (highlighted in bold). Out of those 7500 non-winning double tosses, 2500 came from coin 1 being tails, and 5000 came from coin 1 being heads. Since twice as many possible outcomes originate from coin 1 being heads the posterior odds of coin 1 being a head are 2:1.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000\ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
fw <- c(1, 1, 1, 2, 2, 2, 1)
createBinaryTree(e,v,fw)
```


**Reasoning without using Bayes' rule**

Player 2 knows that one of the following three outcomes must have occurred: 

1. *either* coin 1 was a head and coin 2 was a head, 
2. *or* coin 1 was a head and coin 2 was a tail, 
3. *or* coin 1 was a tail and coin 2 was a head. 

For every 10,000 double tosses, the three outcomes above are expected in 7500 cases. In 5000 of these 7500, coin 1 is a head. In the other 2500, coin 1 is a tail. There are twice as many outcomes in which coin 1 is a head (5000 compared to 2500) and so the odds are 2:1 in favour of heads. These represent posterior odds, with the conditioning information being the fact that double tails has not occurred (or otherwise player 2 would have won the original game).

This means that player 2 should always guess a head when given the choice in this game, since the odds will be in their favour. This result might seem counter-intuitive at first glance. The key thing to understand is that the information of player 2 not winning via double tails should update their belief about the possible outcomes of the tosses. We will now apply Bayes' rule to this problem to see this more clearly.

**Reasoning using Bayes' rule**

We can verify that posterior odds of 2:1 in favour of coin being heads are correct by using Bayes' rule. Event A from Bayes' rule in this example is coin 1 being a head and event B is coin 1 being a tail. Event E is the knowledge that the result was not a double tails, since this is the information that player 1 reveals. This results in the following application of Bayes' rule:
$$\text{posterior odds of coin 1 head to coin 1 tail}=\text{LR} \times \text{prior odds of coin 1 head to coin 1 tail},$$
where the likelihood ratio (LR) equals the following ratio of conditional probabilities
$$\text{LR}=\frac{\text{probability of no double tails conditioned on coin 1 head}}{\text{probability of no double tails conditioned on coin 1 tail}}.$$
To apply Bayes' rule, we need to calculate the probabilities underlying the LR and the prior odds of coin 1 head to coin 1 tail.

The likelihood ratio in this example considers the following question: how much more (or less) probable is it to toss something other than double tails if coin 1 is a head compared to if coin 1 is a tail? To calculate this, we need to calculate the two probabilities in the equation above.

**LR - numerator**: The first is the probability of no double tails conditioned on coin 1 being a head. If coin 1 is a head, then it is certain that a double tails will not be tossed. That means that this probability is 1.

**LR - denominator**: The second probability we need is the probability of no double tails conditioned on coin 1 being a tail. Out of every 5000 tosses in which coin 1 is tails, we expect 2500 to lead to coin 2 being heads and 2500 to lead to coin 2 being tails. The outcomes are evenly split and so the probability is 0.5. 

**LR**: Using these two probabilities results in a likelihood ratio of $\frac{1}{0.5}=2$. In other words, an outcome other than double tails is twice as likely when coin 1 is a head compared to when coin 1 is a tail.

**Prior odds**: The second component of Bayes' rule that we needed was the prior odds of coin 1 being a head. These odds are evens as they were given by the background information for the tosses of the coins, i.e. the coins are tossed so as to guarantee even odds of heads and tails. The prior odds of coin 1 showing a head are 1:1.

Bayes' rule states that the posterior odds must be equal to the prior odds multiplied by the likelihood ratio. With a likelihood ratio of 2 and prior odds of 1:1, we obtain posterior odds of 2:1 in favour of a head; coin 1 is twice as likely to be a head when we know the outcome is not double tails. This result is the same as when we reasoned without using Bayes' rule, and so we have verified that it satisfies Bayes' rule. 

In this example it was possible to calculate the posterior odds without using Bayes' rule. This meant that the odds could be verified. In many real situations, the posterior odds are hard to quantify without using Bayes' rule. Bayes' rule offers an powerful and elegant solution to this problem since the prior odds and LR are often easier to quantify.

## Reliable probabilities {#reliable-probabilities}

The notion of probability used in forensic science is subjective and personal. Anyone is capable of assigning a probability to their personal uncertainties. But even if everyone does this to the best of their ability, some people have more information about some events than others. This is clear from the idea of experts versus non-experts: the expert's probability assignment for a situation within their expertise will be more **reliable** than that of the non-expert. Reliability here refers to the fact that the expert's knowledge is closer to all the available knowledge about a given topic, and so their best assessment is better than that of an uninformed non-expert. This is one of the reasons expert witnesses are used. Reliability of probability assignments is a key part of interpreting and using expert evidence.

We mentioned in a previous section that background information through assumptions was important since it acts as conditioning information that informs the probability. In practice what happens is that assumptions are made and stated upfront, so that they are inherently conditioned on when any probability is stated. We did this in the coin toss example when we assumed that the coin tosses were done in such a way so as to favour neither heads nor tails. In practice this might not be able to be guaranteed, and so instead the assumption might be justified by a reasonable 'default' state of knowledge, i.e. by assuming that the coin tosses are fair unless there is reason to believe otherwise. This is often done in practice when assigning probabilities to real events. Assumptions like this might be considered widely reasonable in the scientific community and therefore accepted by most experts.

In all cases, it is important that assumptions are made transparent so that they are open to reasonable audit. This makes probability assignments **assessable**. They can then in principle be assessed for their reliability. We saw that independence was an important assumption that can be made. One reason for this is because it makes complex probability calculations easier. However, it can result in unreliable probabilities when it is incorrectly assumed.

Another thing that can be checked is the general **calibration** of personal probability assignments. This is a general measure of how accurately an individual's quantified uncertainty reflects an empirical uncertainty. This can be found by asking experts to assign probabilities to events whose uncertainty is (or can be) precisely measured. For example, weather forecasters are tasked with providing probabilities for rain in a particular region of interest throughout each day. Their calibration can be measured by comparing their historical probability assignments to whether rain actually occurred or not over a long period of time. Forecasters who assign high probabilities of rain when it actually does rain and low probabilities when it actually does not rain are well calibrated. Better calibration leads to greater reliability. Experts in forensic science can demonstrate this by performing competency tests. This involves conducting simulated examinations of evidence when the underlying truth is known to the assessors, but unknown to the experts under assessment.

Some probability assignments can be empirically validated using repetitions of the same event. For example, a coin toss can be repeated many times and the proportion of observed heads can be compared to the expected proportion of heads given by a probability model. A large difference between the model and the empirical observations indicate that the probability assignment is not reliable. However, many events cannot be replicated under the same circumstances of interest for the probability assignment. When this cannot be done, e.g. for one-off events, then information from similar events or expert judgement can be used to inform that probability assignment. Performing repetitions is part of a wider idea of using empirical **data** to construct more reliable probabilities. If there are known data relating to an event of interest, then an individual's best assessment of the probability of that event should incorporate that data. The degree to which the data anchors the probability assignment will depend upon the quality of the data and the relevance of the data to the probability in question. Quality of the data can be influenced by factors such as the conditions under which it was gathered and the amount of data that has been collected. The relevance of the data can be subjectively determined by the expert based upon their domain expertise. 

## False positives {#false-positives}

False positives and false negatives are terms to describe mistakes in uncertain categorical assignments. Typically these are binary assignments where something or someone is either labelled as a **positive** case or a **negative** case, and the truth about them actually being a positive or negative case is unknown.

If the truth is that they are a negative case, but they were mistakenly labelled as a positive case, then the assignment is a **false positive**. If the truth is that they are a positive case, but they were mistakenly labelled as a negative case, then the assignment is a **false negative**. If the labels were correct, then the assignment was a **true positive** or **true negative**, respectively. This information is presented in Table \@ref(tab:intro-fp-table).

```{r intro-data, include=FALSE}
introDf <- tibble("Truth" = c("Positive", "Negative"),
                  "Labelled positive"=c("True positive",
                                        "False positive"),
                  "Labelled negative"=c("False negative",
                                        "True negative"))
```


```{r intro-fp-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
introDf %>%
  mutate("Labelled positive" = cell_spec(
    `Labelled positive`, color = colPal[c(6, 7)], bold = T
  )) %>%
  mutate("Labelled negative" = cell_spec(
    `Labelled negative`, color = colPal[c(7, 6)], bold = T
  )) %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Labelling statistics based on the assigned label and the underlying truth.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")

```

For example, when testing someone for a specific disease, we are uncertain about whether or not they have the disease before applying the test. The test results categorise them as either positive or negative for the disease, but it is never absolutely guaranteed to be correct. Even the most reliable tests make mistakes, even if that's only very rarely. The test result should decrease our uncertainty about whether the tested person has the disease or not, but it can't totally eliminate it. The best tests will greatly decrease our uncertainty, and poor ones won't change it much.

If many assignments of positive/negative have been made under controlled conditions, e.g. when the underlying truth of positive or negative is known, then one can determine the **rate** of true/false positives/negatives. This rate corresponds to the probability of each entry in Table \@ref(tab:intro-fp-table) occurring.

The probability of a false positive occurring is called the **false positive rate** and the probability of a false negative occurring is called the **false negative rate**.

The probabilities of true assignments have different names. The probability of a true positive is called the **sensitivity** and the probability of a true negative is called the **specificity**. 

The **base rate** of a characteristic is the probability that when we randomly select an object from the population of interest, then that selected object has the specified characteristic. This is commonly called the **prevalence** when the characteristic that we are interested in is a disease. 

## Example: diagnostic tests {#exm-test}

The following example is adapted from @aitken2010. 

```{r test-data, include=FALSE}
testDf <- tibble("Disease" = c("Present",
                               "Absent",
                               "Total"),
                 "Test positive"=c(99, 495, 594),
                 "Test negative"=c(1, 9405, 9406),
                 "Total"=c(100, 9900, 10000))
```

The risk of a disease is 1% in a relevant population of 10,000 people. This means that the disease affects 100 people out of the total 10,000, and it does not affect the other 9,900.

A diagnostic test has been created for this disease. The test has a sensitivity of 99%; out of the 100 people who have the disease, 99 of them have a positive test result. The final 1 person tests negative despite having the disease. This person receives a false negative result.

The test has a specificity of 95%; out of the 9,900 people who do not have the disease, 9,405 have a negative test. The other 495 people test positive despite not having the disease. These people receive false positive results. This information is displayed more clearly in Table \@ref(tab:test-table).

```{r test-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
testDf %>%
  mutate(Disease = cell_spec(
    Disease, bold = T
  )) %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'The number of people who are affected by the disease and their diagnostic test results.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")

```

This test has high sensitivity (99%) and high specificity (95%), which makes it sound reliable. However, remember what these terms mean: the probability of testing positive given that you do have the disease (sensitivity), and the probability of testing negative given that you don't have the disease (specificity). This probability is conditioned upon knowing whether the person has the disease or not.

In practice, people do not know whether they have the disease or not, and that is why they get tested. This means that this is not useful conditioning information in practice. The information that people do have is whether their specific test result was positive or negative, and so this is the information that the probability should be condition on. What's the probability of actually having the disease given the result of the test? 

Look back to the columns of Table \@ref(tab:test-table). Consider the negative results first. A total of 9,406 people from our population of 10,000 tested negative. Out of these 9,406 who tested negative, 9,405 did not have the disease. There was only a single individual who tested negative despite having the disease. This means that a negative test result is a great (but not perfect) indicator for not having the disease. 

Now consider the positive results. A total of 594 people from our population of 10,000 tested positive. Out of these 594 who tested positive, only 99 (~17%) actually have the disease. The large majority of people who tested positive, 495 (~83%) of the 594, do not really have the disease. This can be seen more clearly in Figure \@ref(fig:freq-tree-test).

```{r freq-tree-test, echo=FALSE,fig.cap="An expected frequency tree diagram of the disease testing example. Out of the 594 people who test positive (shown in bold font), 99 (~17%) have the disease. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \npeople", "100\ndisease", "9,900\nno disease", "99\npositive", "1\nnegative", "495\npositive", "9,405\nnegative")
fw <- c(1, 1, 1, 2, 1, 2, 1)
createBinaryTree(e,v,fw)
```

If a randomly selected individual from this population tests positive, then it is highly likely that they do not have the disease. A positive result for this test is a terrible indicator of whether someone has the disease. This is a counter-intuitive result that can be explained using the logic of Bayes' rule. 

This phenomenon is caused by the very low **base rate** (prior probability) of the disease. This is the same as the risk of having the disease for people within the population, which was 1%. A randomly selected individual has a very low probability of having the disease prior to being tested. The test has very high sensitivity and so it is able to detect almost all of the true positives. The issue was that it tested many people who didn't have the disease, and so even a low error rate led to many false positives. Due to the **base rate** being so low, the number of true positives (99) was much smaller than the number of false positives (495). This meant that the positive results largely consisted of false positives.

```{block eval=FALSE, include=isDynamicOutput()}
### Interactive disease testing example

The example above made the point that the base rate was an important factor in determining the proportion of true positive tests. This effect is demonstrated in the interactive tool below. By changing the base rate from low to high and vice versa, you may compare the number of true positive tests to the number of false positive ones as a function of the base rate of the disease. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDiseaseTest", NULL, TRUE), height = '900px')
}
```

## Example: doping {#exm-doping}

The following example has been adapted from @statsprimer2020.

Table \@ref(tab:test-table) and Figure \@ref(fig:freq-tree-test) present the technical diagnostic testing information in a format which is easier to understand and base decisions on. However, it is often not presented this way in practice and so must first be `translated' from the raw numerical information. 

>A test designed to detect athletes who are doping is claimed to be '95% accurate'. If an athlete is doping then the test returns positive 95% of the time, and if the athlete is not doping then the test returns negative 95% of the time. It is suspected that around 1 in every 50 athletes dope. An athlete tests positive for doping using this test during a random drugs screening. How likely is it that they are really doping?

The answer is around 28%, pause for a moment and see whether that answer comes to you from reading the above text. After reflecting on the text, continue through the example below, where we present this same information in a more familiar format. 

We can convert some of the written information into our technical definitions. The second sentence states that the sensitivity and specificity are both 95%, although those words are not explicitly used. The base rate for doping is given as approximately 2%.

We haven't been given a relevant population size to use natural frequencies to describe these rates, but we can imagine one in order to aid our understanding. Since we are going to use a hypothetical population of athletes, we will have to talk in terms of what we would expect from such a population and so can use expected frequencies.

Assume, for clarity, that we have a relevant population of 10,000 athletes. Using the base rate, we expect 200 (2%) of these to be doping and 9,800 (98%) not to be doping. The sensitivity tells us that out of the expected 200 athletes who are doping, the test is expected to return positive for 190 (95%) of them and negative for 10 (5%) of them. We expect 10 false negatives. 

Out of the expected 9,800 athletes who are not doping, the specificity tells us to expect 9,310 (95%) to test negative. We expect 490 (5%) of these non-doping athletes to test positive; we expect 490 false positives.

We expect a total of 680 positive tests and 190 (~28%) of those positive tests to be from an athlete who is doping. The answer to our original question is that given a positive test result, we expect the athlete to be doping roughly 28% of the time. This information is presented in the expected frequency tree in Figure \@ref(fig:freq-tree-doping). 

```{r freq-tree-doping, echo=FALSE,fig.cap="An expected frequency tree diagram of the doping example. Out of the 680 athletes who test positive (shown in bold font), 190 (~28%) are doping. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \nathletes", "200\ndoping", "9,800\nnot doping", "190\npositive", "10\nnegative", "490\npositive", "9,310\nnegative")
fw <- c(1, 1, 1, 2, 1, 2, 1)
createBinaryTree(e,v,fw)
```


```{block eval=FALSE, include=isDynamicOutput()}
### Interactive Doping Example

Below is an interactive tool for the doping example. If the plots are not displaying correctly, then please refresh your page.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDopingTest_probabilities","DopingTest",TRUE), height = '1100px')
}
```

```{block eval=FALSE, include=isDynamicOutput()}
## Summary: probability

Use the activity below to create a summary of the key points from this chapter.
```
``` {r probability-summary-questions, echo=FALSE}
if(isDynamicOutput()){
  knitr::include_url(paste0(QUESTIONS_HOST,"Probability-Summary.html"), height=800)
}
```

## More information

In this chapter on Probability we focussed on probability models and the expectations that a probability model forms for real events, e.g. a number of coin tosses. However, these expectations are often not exactly what is observed in practice, e.g. with real tosses of the coin. Analysing empirical events such as this moves from the field of probability to the field of statistics, and is currently outside the content of this book. If you would like to explore statistics for the coin toss examples presented in this chapter, then you may do so in the [interactive application](https://lrcfs.dundee.ac.uk/apps/interactive-lr/) which accompanies this book under the 'Coin Toss - Single Toss Samples' and 'Coin Toss - Double Toss Samples' tabs.

<!-- ## Research study -->

<!-- If you are taking part in our research study, please return to the survey now to answer the questions about this chapter. -->
