# Probability to describe uncertainty {#probability}

```{r package-load-probability, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
needPackages("igraph",
             "ggthemes",
             "kableExtra",
             "tidyverse",
             "plotly",
             "grid",
             "plyr")
# set some useful variables
source("./code/useful-variables.R")
```

We saw in the previous chapter that there was uncertainty in scientific evidence. Using a conceptual framework, we showed how it was possible to  break down the uncertainties surrounding evidence in particular case circumstances. As part of this process the expert quantifies the uncertainty, e.g. out of every 1000 similar case circumstances they believe 200 would yield a fibre match if the suspect had truly been at the crime scene. This quantification stage converts the uncertainty into what is known as a probability (and then this is converted again for communication). In this chapter we discuss probability, describing what it is and giving reasons for why we use it when interpreting forensic scientific evidence.

## Quantifying uncertainty

A probability is just a number between 0 and 1 that describes the magnitude of certainty for the occurrence of an event. A probability of 0 means that the event is impossible whilst a probability of 1 means that an event is certain. Most interesting probabilities fall between 0 and 1 based on how much certainty there is about the occurrence of the event. Events with low certainty should have a probability closer to 0 on the scale, highly certain events should have a probability closer to 1 on the scale. Probabilities of 0.5 describe an event whose occurrence is exactly as likely as its non-occurrence. 

## Example: coin toss

The classic example for demonstrating probability is tossing a two-sided coin. Before the coin has been tossed, the outcome is uncertain. We can consider some of the questions from the previous section to describe this uncertainty.

What uncertainties are there? Whether the coin will land heads-up or tails-up as a result of the toss.

What are the sources of this uncertainty? There is a randomness to the flipping process. We also do not know if the coin is double-sided, e.g. has two heads instead of one heads and one tails. You may consider other sources here too, such as trust in the person flipping the coin to be acting fairly.

The double-sidedness of the coin represents epistemic uncertainty. We can eliminate this uncertainty by checking both sides of the coin before it is tossed.

The randomness of the flipping process represents a combination of aleatoric and epistemic uncertainty. The epistemic uncertainty comes from trust in the person flipping the coin and other factors which might unfairly influence the outcome of the coin flip. It is possible to mostly eliminate this uncertainty by learning about the properties of this coin and the person flipping it and overcoming them, e.g. by letting someone trustworthy flip the coin.

Once these factors are removed there is only aleatoric uncertainty about the outcome. This is an irreducible uncertainty of the coin flip; no further information can be learned which will make your guess better. This means that one cannot develop a better long-term guessing strategy than by guessing at random. Let's try to quantify this uncertainty.

We know that a probability of 1 means an event is certain. We also accept that the coin toss must either result in heads or tails. This means that the event 'heads or tails' has probability 1. It also means that the event 'heads and tails' has probability 0, it is impossible to get both in a single toss. 

The event 'heads or tails', which has probability 1, is made up of two other events: the event 'heads' and the event 'tails'. In other words, if we get either 'heads' or 'tails' then we get 'heads or tails'. In probabilities this means that the probability of 'heads or tails' (which is 1) is equal to the probability of 'heads' added to the probability of 'tails'. 

We then have two events whose probabilities add up to 1. Now we can consider them in relation to each other. Is getting a 'heads' more likely than getting a 'tails'? Is getting a 'heads' less likely than getting a 'tails'? Most people would answer no to these questions, and so the conclusion for those people is that 'heads' and 'tails' are equally likely. If they are equally likely, then they must each have a probability equal to 0.5.

There are ways we can check this. What would we expect if we toss the coin many times? If we hold the belief that each probability is 0.5, then the outcome should be evenly distributed between heads and tails when we flip the coin a very large number of times. So let's check the outcome of a trustworthy computer flipping a coin.

[interactive app which performs simulated coin flips]

As the number of tosses gets larger and larger, the proportion of heads and tails gets closer to 0.5, e.g. out of 10,000 flips, we expect 5000 to be heads and 5000 to be tails. This can be represented in what is known as an expected frequency tree, shown below.

```{r freq-tree-coin-toss, echo=FALSE,fig.cap="Out of 10,000 cosses, we expect 5000 to be heads and 5000 to be tails. The probability of heads is 0.5 and is equal to the probability of tails. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3)
v <- c("10,000\ntosses", "5000\nheads", "5000\ntails")
freqTree <- graph(edges=e, n=3, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 3))
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

If we did not ignore some of the epistemic uncertainties about the coin toss (e.g. if we did not trust the coin flipper), then this probability might not accurately represent our belief.

## Personal probabilities

We mentioned in the previous chapter that uncertainty was personal. Your uncertainties will be different than mine because we have different beliefs and information. If this is true then quantifying uncertainty is also personal. In other words, probability is personal too. 

In the previous example you might not agree that the person (or computer) tossing the coin was doing so fairly, e.g. the flip was in favour of heads being more likely than tails. This would adjust your probability in favour of heads. If this belief was marginal, then you might adjust it only a small amount, say to 5500 heads out of each 10,000 tosses, or a probability of heads of 0.55. You would then expect 4500 out of 10,000 tosses to be tails and so your probability of tails is adjusted to 0.45. If this belief was stronger, then you might adjust your probability of heads closer to 1, and your probability of tails closer to 0 accordingly. 

Some differences in opinion might be more or less reasonable since they might be based upon more or less reliable information. Those based upon more reliable and generally agreeable information will be more **objective**. Those based upon less reliable or agreeable information will more be more **subjective**.

In the coin tossing example, I take a neutral position on the flip - I said that the flip was unbiased so that it made heads no more likely than tails as an outcome. My rationale for this is based upon knowledge of theoretical coin flips designed to have heads and tails as equally probable. I also express ignorance about the person (or computer) flipping the coin so I take the neutral stance that they are flipping the coin fairly. These are reasonable expressions of my uncertainty based upon my knowledge and might be considered to be quite objective (even though they might not be right).

You might have believed that heads was more (or less) probable than tails. If you did this because you distrust the flipper of the coin without evidence then this would be subjective and subject to reasonable challenge. If you did this because you know the coin flipper very well then this probability is more objective. Further still if you can demonstrate that on previous occasions heads has been more (or less) likely when this person has tossed the coin, then this probability would be even more objective. Note that both mine and your probability can be different but still have reasonable rationales. In the case that you sufficiently demonstrate to me the previous malfeasance of the coin flipper then you have dispelled some of my epistemic uncertainty by giving me relevant information. I should then update my probability for heads to be closer (or equal to) yours.

Fortunately for repeatable events like the coin toss we can check and update our probabilities using empirical data, e.g. repeated flips of the coin in the example. This can be seen as eliminating epistemic uncertainty or gaining a better understanding of the aleatory uncertainty. Using the computer as the flipping device, data was provided from repeated flips which supported the toss leaving an equal probability for heads and tails. The personal probabilities in this case were easily assessable.

In criminal cases, the opportunity to repeat events and gather scientific data from the case circumstances is often not possible (nor even desirable when results are harmful outcomes). The events are uncertain and one-off, and so probabilities cannot be assessed empirical repetition. They can still be informed by other empirical data however, e.g. be validated repetitions in similar (but not the same) circumstances. 

Another thing that can be checked is the general **calibration** of personal probability assignments. This means asking experts to assign probabilities to events whose probability is known (but unknown to the expert). For example, an expert who assigns a probability of 0.4 to an event whose probability is 0.45 is calibrated better than an expert who assigns a probability of 0.8 to the event.

In practice that can be done by performing competency tests when the underlying truth is known. For example, comparing latent finger prints or footwear marks to numerous potential reference finger prints and footwear. The true sources of the prints or marks are known to those conducting the test but unknown to the expert being tested. The resulting proportion of correct source identifications and correct source exclusions is then a metric of expert calibration for questions about impression evidence.

## Example: some real events

Figure 

```{r, prop-ruler, echo=FALSE, fig.align = 'center'}
ruler.func<-function(gg){
seq.list<-list()
for(i in 1:length(gg)){  
  ystart<-seq(0.1,gg[i],0.1)
  yend<-ystart
  xstart<-rep(i-0.25,length(ystart))
  xend<-xstart+0.1
  nam.val<-c(LETTERS[i],rep(NA,length(ystart)-1))
  numb.val<-c(gg[i],rep(NA,length(ystart)-1))
  seq.list[[i]]<-data.frame(nam.val,numb.val,xstart,xend,ystart,yend)
}
df<-as.data.frame(do.call(rbind, seq.list))
p <- ggplot(df, aes(nam.val))
p <- p + geom_bar(aes(y=numb.val,fill=nam.val),stat="identity",width=0.5,color="black",lwd=1.1)+
    scale_x_discrete(limits=LETTERS[1:length(gg)])+
    geom_segment(aes(x=xstart,y=ystart,xend=xend,yend=yend))+
    geom_hline(yintercept=c(0.25, 0.5, 0.75),color="white",lwd=1.1)+
    ggtitle("Probability of real events")+
    ylim(c(0,max(gg)+0.5))+
  annotate("text",x=seq(1,length(gg),1),y=gg+0.1,label=gg,fontface="bold",size=rel(6))+
  theme_bw()+
  theme(axis.title=element_blank(),
        axis.text.y=element_blank(),
        axis.text.x=element_text(face="bold",size=rel(1.5)),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        panel.grid=element_blank(),
        legend.position = "bottom",
        legend.margin = margin()) +
  scale_fill_discrete(name="Event",
                      labels=c("an American male dying of cancer in their lifetime",
                               "any 2 people having the same birthday in a room of 23 random people",
                               "the Scottish city of Dundee being overcast on 26th January",
                               "a female born in 2020 living to age 70 or longer"))+
  guides(fill=guide_legend(nrow=4,byrow=TRUE))+
  coord_flip()
print(p)
}
ruler.func(c(0.21,0.51,0.68, 0.9))
```

Cancer: https://www.cancer.org/cancer/cancer-basics/lifetime-probability-of-developing-or-dying-from-cancer.html

Life expectancy: https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthandlifeexpectancies/articles/lifeexpectancycalculator/2019-06-07

Weather: https://weatherspark.com/m/40087/6/Average-Weather-in-June-in-Dundee-United-Kingdom

Birthday: https://en.wikipedia.org/wiki/Birthday_problem

## Conditioning on information

So far we have looked at probabilities and the uncertainties from which they derive. We showed that probability was personal because it is based upon our individual beliefs and current information. For example, in the coin toss example I had no knowledge about the person tossing the coin and so I assumed that they were tossing fairly, leading me to the probability of 0.5 for heads. However, you believed that the person who was tossing the coin was doing so in favour of heads (or tails) based on information I did not have and so had a different probability.

Formally, the information which we use to construct the probability is called **conditioning** information. This is because we are constructing a probability which is conditional on that information, otherwise known as a **conditional probability**. The process of using information this way to construct such a conditional probability is itself called **conditioning**.

This does imply that all probability is conditional on our current state of individual knowledge. However, to avoid being overly tedious we usually omit to say this each time we talk about a specific probability, e.g. we do not say the probability of heads followed by listing all assumptions and conditioning information each time we mention it. Instead, it's conventional to list initial assumptions when the probability is first mentioned and then to explicitly condition on other information. For example, for the coin toss we might state upfront to assume that the coin is balanced and the toss is fair. Having both assumed this, we might be more likely (but not guaranteed) to agree that there is a 0.5 probability of heads. Alternatively, if you showed me the information which incriminated the person tossing the coin, then I would condition on that and it would alter my probability of heads.

Conditioning information doesn't just have to be about our current state of knowledge. A very useful application of conditional probability is to condition on possible information/events in the future. For example, considering possible disastrous events in the future (e.g. health emergencies) and the probability that that would ruin personal finances is a key factor in deciding whether to get insurance or not (and in how insurance companies set prices for their products). Considering these possible future events can help us make decisions in the present. 

In the same way, one can condition on events which could have occurred in the past in order to investigate how that would have affected the probability of an event which was ultimately observed. If we can find that the observed event was much more likely when conditioning on past event A than when conditioning on past event B, then we can say that this provides more evidence in favour of event A having occurred than it does event B. This is how we use conditional probabilities for interpreting forensic evidence: we look at the events which the defence and prosecution assert and compare how likely the observed evidence would have been when it is conditioned on each in turn. We will revisit this idea in later chapters. For now, let's look at an example of conditional probability by adding another coin toss. 

## Example: double coin toss

Suppose now that we toss 2 coins, labelled coin 1 and coin 2. We toss coin 1 first and then coin 2. Let's first lay out some working assumptions to avoid tedious detail later on. Assume that they are tossed in such a way that guarantees a 0.5 probability of heads on each coin for each toss. What's the probability that both coins land heads up? Figure \@ref(fig:freq-tree-double-coin) shows the expected outcomes.

```{r freq-tree-double-coin, echo=FALSE,fig.cap="Out of every 10,000 double coin tosses, we expect 2500 to be double heads. The probability of getting two heads is 0.25.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(2, 2, 1, 2, 1, 1, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

The first branch of the outcomes is exactly the same as we saw in \@ref(fig:freq-tree-coin-toss) as it's just one coin being tossed at first. Out of every 10,000 tosses of coin 1, we expect 5000 to be heads and 5000 to be tails. After tossing coin 1, we toss coin 2. So for each of the potential outcomes of coin 1, we can consider the potential outcomes of coin 2. Out of every 5000 tosses of coin 1 which result in heads, we expect 2500 to lead to tosses of coin 2 which also result in heads. In summary, out of all 10,000 tosses of both coins, we expect 2500 of them to result in double heads; the probability of getting a heads on both coins in a double coin toss is 0.25.

As part of this inference, we considered a conditional probability: out of every 5000 tosses of coin 1 which result in heads, we expect 2500 to lead to lead to tosses of coin 2 which also result in heads. In other words, the probability of getting a heads from coin 2 **conditioned on** coin 1 being heads is 0.5. By condiitoning on this information, we restricted our focus to only those coin 1 tosses which were heads, which was 5000 instead of the 10,000 total tosses. Only then did we consider the outcomes for coin 2. 

Note how this probability is the same as getting a heads from tossing coin 2 **without** conditioning on coin 1 being a heads anyway. We can see this by looking at the 'tails' branch of coin 1 tosses. Out of 5000 tosses for which coin 1 is tails, we expect 2500 of those to result in coin 2 being heads. The probability of getting a heads from coin 2 **conditioned on** on coin 1 being tails is also 0.5. The result from tossing coin 1 **does not** affect the probability of getting a heads for coin 2. This is known as **independence** between the coin tosses. When events are independent, conditioning one on the result of the other has no affect on the probability of the other occurring. Let's take a look at events which are not independent.

Suppose we now play a game with this double coin toss. I win if the coin toss results in double heads and you win if the coin toss results in double tails. If neither of us wins then we call it a draw and toss the coins again. Since double heads is just as likely as double tails then we are both equally likely to win. We can see this from the highlighted Figure \@ref(fig:freq-tree-double-coin-bet) below.

```{r freq-tree-double-coin-bet, echo=FALSE,fig.cap="Out of every 10,000 tosses, 2500 are double heads and 2500 are double tails. This means that we have equal probability of winning of 0.25 each. The remaining 5000 tosses result in a draw and so the probability of a draw is 0.5.", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000\ntosses", "5000\ncoin 1 heads", "5000\ncoin 1 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails", "2500\ncoin 2 heads", "2500\ncoin 2 tails")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(1, 1, 1, 2, 1, 1, 2)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

Out of 10,000 double flips, I will win 2500, you will win 2500, and 5000 result in a draw. We each share a winning probability of 0.25, and there is a 0.5 probability that we draw. Let's look at some conditional probabilities. Out of every 5000 heads from coin 1, I win on 2500 occasions and we draw on 2500 occasions. You win on 0 occasions. The probability of me winning conditional on coin 1 being heads is 0.5 and the probability of you winning conditional on coin 1 being heads is 0. My probability of winning rises from 0.25 to 0.5 conditional on coin 1 being heads, and yours decreases from 0.25 to 0. This means that the events of me and you winning are not independent of the result of coin 1 (i.e. the are dependent). The probability of a draw remains the same after coin 1 is tossed at 0.5. This means that the event of a draw is independent of the outcome of coin 1. 

## Odds

Odds are just another way of expressing probabilities. Odds are given as a ratio of probabilities so that it is clear how much more or less likely one event is compared to another. You can compare any two probabilities together to create odds. One pair which is commonly used is the probability of an event occurring versus the probability of it not occurring.

For example, the fair coin flip was just as likely to result in heads as it was to result in tails. This results in odds written as 1:1, which is spoken as '1-to-1' or more commonly 'evens'. Intuitively, this means that we can separate the possible outcomes into a total of (1+1=)2 parts which are equally probable. One part represents the probability of heads and the other part represents the probability of tails.

We can convert from odds to probability as follows. For the single coin toss we had odds of 1:1 for heads. The total probability, which must be 1, is made up of 2 equally sized parts. One of these parts represents heads, and so the probability of heads (and similarly for tails) is $\frac{1}{2}=0.5$. Suppose we have odds of 1:9 for an event occurring versus not occurring. This means that the total probability, which must be 1, is made up of 10 equally sized parts. One of these ten parts represents the event occurring, and so its probability is $\frac{1}{10}=0.1$. 

Odds like this, which are used to represent probabilities before any conditioning occurs, are known as **prior odds**.

Odds which are used to represent conditional probabilities are known as **posterior odds**. For example, in our double coin toss game we conditioned on the result of the coin 1 being heads. Before the conditioning, my probability of winning was 0.25. This means my prior odds of winning were 1:3. The conditioning resulted in the probability of me winning rising to 0.5, which is posterior odds (i.e. the odds of me winning **after** observing coin 1 being heads) of evens (1:1). 

Another reason that odds are useful is because of the simplicity that they give to a very important mathematical result, which we discuss in the next section. 

## Bayes' rule

Bayes' theorem is a mathematical rule which links the **prior odds** to the **posterior odds**. Suppose we have two events A and B, Bayes' rule states

It allows us to switch the conditioning information in two probabilities, known as **transposing the conditional**.

## Example: guessing coin 1

In the double coin toss example we played a game in which I won if two heads were tossed and you won if two tails were tossed. We drew if there were a combination of heads and tails. When thinking about the probabilities of winning, we conditioned on coin 1 showing heads and also on coin 1 showing tails. In each conditioning, one of our probabilities reduce to 0 and the other's increased to 0.5.

Suppose we change the game so that I toss the coins and don't show you which sides were facing up. I only tell you whether you won or not. If you didn't win, I give you another change to win by asking you to guess the result of coin 1's toss. If you guess correctly, you win this new game. What should you guess? The answer is found by conditioning on you not winning the first part of the game, instead of conditioning on the outcome of coin 1 like we did in the first game.

## Assigning probabilities

## More information

## Exercises