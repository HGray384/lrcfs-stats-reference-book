# Likelihood ratio to evaluate uncertainty {#likelihood-ratio}

```{r package-load-likelihood-ratio, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
library("igraph")
library("ggthemes")
library("gdtools")
library("kableExtra")
library("tidyverse")
library("plotly")
library("DiagrammeR")
# set some useful variables
source("./code/useful-variables.R")
```

Using the theory of probability and propositions, we have seen how uncertainty can be described and framed around the key facts of a criminal case. The final idea is to evaluate the uncertainty surrounding evidence in a way which satisfies probability theory, so that it remains logically consistent, and also so that it assists the court in determining the truth of key propositions. This is achieved using the likelihood ratio, which has been mentioned in previous chapters. 

In this chapter, we focus on the likelihood ratio in detail. We revisit how it relates to probabilities, propositions, and evidence, how it is used to quantify probative value, what factors can affect its robustness, and how it is categorised and communicated to the court.

## Relative support for competing propositions

Suppose that there are two competing propositions, $H_p$ for the prosecution and $H_d$ for the defence. There is observed evidence $E$, and the expert needs to determine whether $E$ was more likely assuming $H_p$ or $H_d$, since this will assist the court in assessing the truth of $H_p$ or $H_d$. If $E$ is more likely assuming $H_p$ were true compared to if $H_d$ were true, then the evidence $E$ provides more support for $H_p$ than $H_d$, and vice versa if $E$ is more likely assuming $H_d$ to be true compared to $H_p$. The likelihood ratio (LR) quantifies the magnitude of this support.

The LR is the relative size of the probability of observing $E$ conditioned on $H_p$, compared to the probability of observing $E$ conditioned on $H_d$. Mathematically, this is equivalent to dividing the probability of observing $E$ conditioned on $H_p$, by the probability of observing $E$ conditioned on $H_d$. As a formula it is written as
$$\text{LR}=\frac{\text{probability of }E\text{ assuming }H_p \text{ is true}}{\text{probability of }E\text{ assuming }H_d \text{ is true}}.$$

By assuming each of the competing propositions to be true, we can see how much more likely $E$ was to occur in the prosecution's version of events compared to the defence's.

Since each term in the LR is a probability, their value must lie between 0 and 1. This means that the LR itself must be between 0 and $\infty$. Values of the LR which are greater than 1 indicate relative support in favour of $H_p$ compared to $H_d$, since it means that the probability of $E$ assuming that $H_p$ is true is greater than the probability of $E$ assuming $H_d$ is true. Values of the LR which are less than 1 indicate relative support in favour of $H_d$ compared to $H_p$, since it means that the probability of $E$ assuming that $H_d$ is true is greater than the probability of $E$ assuming $H_p$ is true. Values of the LR which are equal to 1 indicate that evidence $E$ provides equal support for $H_p$ and $H_d$. This is shown in Table \@ref(tab:lr-meaning-table).

```{r lr-meaning-df, include=FALSE}
lrDf <- tibble("LR" = c("less than 1",
                        "equal to 1",
                        "greater than 1"),
                  "Meaning"=c("Evidence $E$ provides more support for $H_d$ compared to $H_p$",
                              "Evidence $E$ provides equal support for $H_p$ and $H_d$",
                              "Evidence $E$ provides more support for $H_p$ compared to $H_d$"))
```

```{r lr-meaning-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
lrDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Meaning of values of the LR for evidence $E$ considering $H_p$ and $H_d$.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```

## Quantifying probative value

The magnitude of the LR conveys the strength of the support that $E$ provides for $H_p$ compared to $H_d$. LRs of 1,000,000 and 10 both provide support in favour of proposition $H_p$ compared to $H_d$, but the LR of 1,000,000 provides much stronger support than the LR of 10 does. Similarly, LRs of 0.0000001 and 0.1 provide support in favour of proposition $H_d$ compared to $H_p$, but the LR of 0.0000001 provides much stronger support than the LR of 0.1 does. As the value of the LR gets further away from 1, the stronger the support is in favour of proposition $H_p$ or $H_d$ (depending on whether the LR is greater or less than 1) when compared to its competitor.

Since the LR provides a numerical value for the relative support that a piece of evidence provides for $H_p$ against $H_d$, it can be seen as quantifying the **probative value** of a piece of evidence for those propositions. This interpretation of the LR is one of the reasons why it is advocated as a tool to quantify expert evidence. It is the role of the expert witness to present the probative value of scientific evidence within their domain of expertise to the court. The LR provides a logical means to achieve this. The fact finder can then use the probative value of evidence given by the LR to reason about the truth of $H_p$ and $H_d$.

LRs are often categorised verbally instead of quoting the numerical value, using categories such as ‘strong support for $H_p$ compared to $H_d$’. Table \@ref(tab:verbal-scale-table) shows an example set of categories, but others, some with fewer categories, are also in common use.

```{r verbal-scale-df, include=FALSE}
strength <- c("weak",
              "moderate",
              "moderately strong",
              "strong",
              "very strong",
              "extremely strong")
strengthSup <- paste(strength, "support")
props <- c("for $H_p$ compared to $H_d$",
           "for $H_d$ compared to $H_p$")
verbExpr <- c(paste(rev(strengthSup), props[2]),
              "equal support for $H_p$ and $H_d$",
              paste(strengthSup, props[1]))
lrRange <- c("less than 0.000001",
             "at least 0.000001 but less than 0.0001",
             "at least 0.0001 but less than 0.001",
             "at least 0.001 but less than 0.01",
             "at least 0.01 but less than 0.1",
             "at least 0.1 but less than 1",
             "1",
             "at least 1 but less than 10",
             "at least 10 but less than 100",
             "at least 100 but less than 1000",
             "at least 1000 but less than 10,000",
             "at least 10,000 but less than 1,000,000",
             "at least 1,000,000")
vsDf <- dplyr::tibble("LR"=lrRange, "verbal category"=verbExpr)
```


```{r verbal-scale-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
vsDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Verbal categories to convey the strength of numerical LRs.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")


```


```{block eval=FALSE, include=isDynamicOutput()}
### LR calculator {#lr-calc}

The LR is determined by the values of its underlying conditional probabilities. Type probability values into the calculator below to see how they affect the resulting LR.
```
```{r echo=FALSE, include=isDynamicOutput()}
  knitr::include_app(getInteractiveLink("tabLRCalc",NULL,TRUE), height = '460px')
```


```{block eval=FALSE, include=isDynamicOutput()}
### LR visualisation

Figure \@ref(fig:lr-function) shows how the value of the LR changes based on the values of these probabilities. Hover your mouse over the graph to see the values of the conditional probabilities and the resulting LR. 
```
```{r lr-function, echo=FALSE, warning=FALSE, include=isDynamicOutput(), fig.cap="A heatmap of the LR values as its underlying probabilities vary. Values are coloured by their support for either proposition: blue (above the diagonal) is support in favour of $H_p$ compared to $H_d$, and orange (below the diagonal) is support for $H_d$ compared to $H_p$. White (along the diagonal) means equal support for both.", out.width = '100%', fig.align = 'center'}
gridVals <- 101
prob1 <- prob2 <- seq(0, 1, length.out = gridVals)[-1]
vars <- expand.grid(prob1, prob2)
lrDf <- tibble(prob1 = vars$Var1, prob2 = vars$Var2)%>%
  mutate(lr = prob1/prob2)

graph = ggplot(lrDf, aes(x=prob2, y=prob1, fill=lr)) +
  geom_tile(aes(text = paste('probability of E assuming H<sub>p</sub> is true: ', prob1,
                             '<br>probability of E assuming H<sub>d</sub> is true:', prob2,
                             '<br>LR: ',lr))) +
  xlab("probability of E assuming H<sub>d</sub> is true") +
  ylab("probability of E assuming H<sub>p</sub> is true") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_gradientn(colors = c(colPal[7],
                                  colPal[2],
                                  "white",
                                  colPal[3],
                                  colPal[6]),
                       breaks = c(0.01, 0.1,
                                  1,10, 100),
                       labels=c(0.01, 0.1,
                                  1,10, 100),
                       name="LR", trans = "log"
                       # values = c(0, 0.4, 
                       #            0.5, 
                       #            0.6, 1)
  )

ggplotly(graph, tooltip = "text")
  
```


## Example: DNA match

Suppose that a high quality full DNA profile is recovered from a blood stain at a crime scene. This is known as the **questioned profile**. The profile is analysed and is found to contain only one person's DNA, making it a **single donor profile**. A suspect is detained and their DNA profile is taken. This is known as the **reference profile**. The reference profile is found to **match** the questioned profile. Consider the following competing source-level propositions:

- $H_p$: the suspect is the source (of the questioned profile),
- $H_d$: someone other than the suspect is the source (of the questioned profile).

In this situation the DNA match is the evidence for which we would like probabilities conditioned on the above propositions. The LR is given by

$$\text{LR}=\frac{\text{probability of a match assuming the suspect is the source}}{\text{probability of a match assuming someone other than the suspect is the source}}.$$
To obtain the LR, we need to obtain values for the above conditional probabilities. Consider the numerator first. 

The probability of obtaining a match assuming that the suspect is the source is usually set to 1; it is considered to be certain that a match would be obtained if the suspect were truly the source. This is reasonable although it is not strictly true. There is always the risk of a false positive or other laboratory or technical errors occurring, but in practice it is typically assumed that the risks of these errors are negligible, especially for high-quality single donor full profiles. If the questioned profile is not high quality, or another factor affecting the integrity of the match is present for a given case, then the expert might assign a value less than 1 to the probability of a match assuming the suspect is the source.

The probability of obtaining a match assuming that someone other than the suspect is the source is more complex to assign a value to. It requires the probability of a match if we compare the questioned profile with that from a person other than the suspect plucked at random from the population to which the suspect belongs. This is known as the **random match probability** (RMP). The RMP reflects how common the recovered profile is in the relevant population for the case. The more common the profile, the higher the RMP and the lower the LR when the formula is applied; The LR is inversely related to the RMP. The logic behind this is the following: the more common a characteristic is in a population, the worse that characteristic is at discriminating between source level propositions. More common DNA profiles are worse at discriminating between source-level propositions and so they result in smaller LRs when compared to uncommon profiles. Figure \@ref(fig:freq-tree-rmp) displays the expected frequency tree for a RMP of 1 in 40 million in a population of 40 million and 1 people (which includes the guilty individual).

```{r freq-tree-rmp, echo=FALSE,fig.cap="An expected frequency tree for the RMP. Out of 40 million innocent people, 1 DNA profile matches. The RMP is 1 in 40 million. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("40,000,001 \npeople", "1\nguilty", "40,000,000\ninnocent", "1\nmatch", "0\nno match", "1\nmatch", "39,999,999\nno match")
fw <- c(1, 1, 2, 1, 1, 2, 1)
createBinaryTree(e,v,fw)
```

The figure highlights that this RMP reflects the belief that out of 40 million innocent individuals from a relevant population, it is expected that 1 will match the questioned profile. 

In this example, the LR is 40 million. This means that the DNA match is 40 million times more likely if the suspect were the source of the bloodstain compared to if they were not. From the verbal scale in Table \@ref(tab:verbal-scale-table), this LR translates into extremely strong support for $H_p$ compared to $H_d$. Note that this LR is for competing source level propositions. If the propositions were moved to the activity level, then the LR could be considerably reduced depending on the case circumstances. It is important to keep in mind the specific competing propositions to which any given LR refers.

The RMP is calculated using frequency databases of profiles from specific ethnic groups of people, since ethnic group is a large factor in determining genetic variation. The genetics behind DNA evidence is highly discriminating between individuals, so the RMP is usually very small. This results in LRs which can be very large for competing source level propositions for single donor full profiles. 

<!-- In the interactive example below, you can change the value for the RMP and see what effect that has on the LR. -->

<!-- [Interactive example with RMP; display graph of LR as a function of the RMP, the user can highlight points on the graph or input values of the RMP to see what the corresponding LR is.] -->

The RMP is also affected by other factors, such as the degree of relatedness in a population. This is known as the **population sub-structure** or **co-ancestry**. It is a numerical factor which adjusts the RMP to take account of the co-ancestry present in the relevant population, which can also differ depending on ethnic background.

We did not consider direct blood relatives in our propositions. $H_d$ considered *anyone* other than the suspect. But if there was reason to suspect close relatives of the suspect, such as their siblings or parents, then the RMP as calculated from population frequency databases alone would not reflect the required probability. The genetic similarity between these individuals would mean the probability of a match is higher than the RMP. For this reason, and when there is no reason to suspect close relatives of the suspect, the defence proposition is sometimes constructed as

- $H_d$: someone unrelated to the suspect is the source (of the questioned profile).

There are other corrective factors that are applied to calculate the conditional probabilities which feed into the LR in the context of DNA evidence, but they are not not mentioned here. To learn more about DNA evidence and resulting LR calculations, please see @dnaprimer2017 and @puch2012. 

## Updating odds

The LR has a very clear interpretation from Bayes' rule, which was introduced in Chapter \@ref(probability). The rule states that
$$\text{posterior odds} = \text{LR} \times \text{prior odds}.$$
The LR is the numerical factor by which we multiply prior odds in order to obtain posterior odds. The LR tells us *how much* to update odds in light of new information. 

We saw in Table \@ref(tab:lr-meaning-table) that values of the LR equal to 1 meant that the evidence provided equal support for both propositions when compared to each other. Logically, if evidence provides equal support for two competing propositions, then it should not update any odds prior to collecting the evidence. If we look at Bayes' theorem above and plug in an LR of 1, then this is exactly what happens. The prior odds for the propositions are equal to the posterior odds for them; observing the evidence did not change the odds. 

We also saw that when the LR is greater than 1, then the evidence provides more support for $H_p$ than proposition $H_d$. The intuition is that the evidence should increase the odds to be more in favour of $H_p$ (compared to $H_d$) than it previously was. This is confirmed with Bayes' rule above; when the LR is greater than 1, then the prior odds in favour of $H_p$ *increases* when becoming the posterior odds. Observing the evidence was found to be more probable assuming $H_p$ than $H_d$, and so the odds were increased in favour of $H_p$ by a factor equal to the LR.

When the LR is less than 1, the evidence provides more support for $H_d$ than $H_p$. This tells us we should update the prior odds to be more in favour of proposition $H_d$ (compared to $H_p$) than it previously was. According to Bayes' rule, this is exactly what happens; an LR less than 1 means that the prior odds in favour of $H_p$ are *reduced* when becoming the posterior odds. Reducing the odds of $H_p$ means *increasing* the odds in favour of $H_d$. Observing the evidence was found to be more probable assuming $H_d$ than $H_p$, and so the odds of $H_p$ were reduced (increased in favour of $H_d$) by a factor equal to the LR.

## Example: doping (revisited)

Recall the example of detecting doping athletes from Section \@ref(exm-doping).

The test returns positive for 95 out of every 100 doping athletes (95% sensitivity). The test returns negative for 95 out of every 100 non-doping athletes (95% specificity). It was speculated that 2 out of every 100 athletes are doping (2% base rate). We used these numbers to work out the probability of an athlete doping given that they tested positive as around 28%.

A related question is this: How much more likely are we to see a positive test result when the athlete is doping compared to when the athlete is not doping? This question is important as it tells us how informative a positive test result is. This is the type of question that an LR is used to address. 

First break down the question down into competing propositions and the available evidence. 

Competing propositions:

- $H_p$: the athlete is doping,
- $H_d$: the athlete is not doping.

Evidence: 

- $E$: a positive test result.

The likelihood ratio is given by
$$\text{LR} = \frac{\text{probability of a positive test result assuming the athlete is doping}}{\text{probability of a positive test result assuming the athlete is not doping}}.$$

The conditional probabilities which contribute to this LR can be extracted from the expected frequency tree in which we assumed a population of 10,000 athletes, replicated in Figure \@ref(fig:freq-tree-lr-doping). 
```{r freq-tree-lr-doping, echo=FALSE,fig.cap="An expected frequency tree for the doping example. The terms which contribute to the LR of a positive test are those who test positive and this is shown in bold font. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \nathletes", "200\ndoping", "9,800\nnot doping", "190\npositive", "10\nnegative", "490\npositive", "9,310\nnegative")
fw <- c(1, 2, 2, 2, 1, 2, 1)
createBinaryTree(e,v,fw)
```

For the numerator of the LR, "assuming that the athlete is doping" means that we are only looking at the branches of the tree for the doping athletes. We can see that 190 out of these 200 doping athletes test positive, a proportion of 0.95. This is given by the sensitivity of the test.

For the denominator of the LR, "assuming that the athlete is not doping" means that we only look at the branches of the tree for non-doping athletes. We can see that 490 out of the 9,800 non-doping athletes tested positive, a proportion of 0.05. This is given by 1 minus the specificity of the test.

The likelihood ratio in this example is equal to $\frac{\text{sensitivity}}{1-\text{specificity}}=\frac{0.95}{0.05}=19$. The meaning of this can be expressed in a number of equivalent ways:

- a positive test result increases the prior odds that the athlete is doping by a factor of 19,
- a positive test result is 19 times more likely to be observed from doping athletes than from non-doping athletes,
- a positive test result provides 19 times more support for the proposition that the athlete is doping compared to not doping. 

All of these expressions comment on the relative probability of observing a positive test result under the assumptions of the athlete doping and not doping. They do **not** state a value for the probabilities of the athlete doping or not, and they do **not** state that the athlete is 19 times more likely to be doping than not (this is an example of the prosecutor's fallacy from Section \@ref(prosecutor-fallacy)). 

Now let's look at the posterior odds: what are the odds of the athlete doping versus not doping given a positive result of the test?

From Figure \@ref(fig:freq-tree-lr-doping) we can extract the odds of a randomly selected athlete doping, versus not, prior to being tested. This is given by $\frac{200}{9800}$, which can be simplified to $\frac{1}{49}$ and also expressed as odds of $1:49$, or 49 to 1 against doping. These prior odds represent the base rate of 2% after converting from a probability to odds.

Using Bayes' rule and the LR, the posterior odds are
\begin{align}
  \text{posterior odds} &= \text{LR} \times \text{prior odds}, \\
  \text{posterior odds} &= 19 \times \frac{1}{49}, \\
  \text{posterior odds} &= \frac{19}{49},
\end{align}
corresponding to posterior odds of $19:49$, or 49 to 19 against doping given a positive test result. Converting this back to probability gives $\frac{19}{19+49}=0.2794118$, about 28%. This is the same as the answer in Section \@ref(exm-doping).

Notice that the likelihood ratio for a positive test gave support in favour of $H_p$ (the athlete doping) but the posterior odds were still in favour of $H_d$ (the athlete not doping). This was because the prior odds were more heavily in favour of $H_d$ than the factor by which the LR supported $H_p$. Committing the prosecutor's fallacy here would not only give an inaccurate answer for the posterior odds of doping, but it would mistake the odds as being in favour of $H_p$ when they are actually in favour of $H_d$. This is an important point; the likelihood ratio alone does not tell us anything about the posterior odds in favour of a proposition, it only tells us the relative size of the posterior odds compared to the prior odds. For the fact finder, this confirms that LRs should be used to **update** beliefs about competing propositions (as well as using other evidence), and not **as** beliefs about competing propositions.

```{block eval=FALSE, include=isDynamicOutput()}
### Interactive Doping LR Example

The example above shows that even in cases when the LR supports one proposition over the other, the posterior odds can still be heavily against the favoured proposition being true. The result depends on the base rate and the characteristics of the test that is being applied. In the doping example, the base rate of doping athletes is low and so even a positive test result from a test with 95% sensitivity and specificity still results in posterior odds which favour the athlete not doping. 

Below is an interactive tool to further explore the doping example. Change the sensitivity and specificity of the test and read the descriptions and calculations for the prior odds, LR, and posterior odds to put their values into context. See if you can find a combination of sensitivity and specificity which results in posterior odds that favour a randomly selected athlete to be doping.

```
```{r echo=FALSE}
if(isDynamicOutput()){
  knitr::include_app(getInteractiveLink("tabDopingTest_likelihoodRatio","DopingTest",TRUE), height = '1650px')
}
```


## Combining evidence

LRs can also be used to consider the combined value of multiple pieces of evidence together. This is useful when multiple pieces of evidence relate to the same pair of competing propositions. It is also useful for moving from the source level of proposition up to the activity level.

It is easier to see the connection between multiple pieces of evidence and competing propositions by using a graphical model. A graphical model is a visual representation of the evidence and case circumstances. Seeing the logical dependencies between the different pieces of evidence and the propositions allows us to clearly see the statistical dependencies that should be accounted for in the probability model of the evidence. This means that the probability and LR calculations can be made in the confidence that they accurately affect the case circumstances. This approach is becoming more popular amongst some scientific groups as more methodology is being developed. For example,  @koeijer2020 present an approach that is useful for an interdisciplinary forensic examination at the Netherlands Forensic Institute.

An example of a simple graphical model with multiple pieces of evidence is shown below:
```{r serial-evidence-chain-diagram, echo=FALSE, fig.cap="An example of a serial evidence scheme in a graphical model. Nodes on the graph depend upon previous nodes and this dependence should be accounted for in probability and LR calculations.", out.width = '100%', fig.align = 'center'}
# create simple graph of serial evidence chain (e.g. fig. 3 from de Koeijer 2020)
DiagrammeR::grViz("
graph {
  # a graph statement
  graph [rankdir = LR, fontsize = 10]
  
  # node font statement
  node [fontname = 'Sans-serif']
  
  # node statement for suspect
  node [shape = oval]
  A [label = 'Suspect']

  # node statement for items
  node [shape = rectangle]
  B [label  = 'Item 1']
  C [label = 'Item 2']
  
  # node statement for activity
  node [shape = diamond]
  D [label = 'Activity']
  
  # edge statements
  edge [fontname = 'Sans-serif']
  A--B [label = 'E@_{1}']
  B--C [label = 'E@_{2}']
  C--D [label = 'E@_{3}']
}
")
```

The graphical model in Figure \@ref(fig:serial-evidence-chain-diagram) shows a generic example of linking together the suspect, the forensic evidence, the physical items to which the evidence relates, and a proposition of interest to the case - in this case an activity. Each type of object on the graph is represented with a different shaped box in order to clearly show that they differ in nature. For example, physical items to which the forensic evidence relates are shown within a rectangular box, whilst the proposition of interest is shown within a diamond-shaped box. The edges indicate the forensic induction process between the objects. For example, $E_1$ above provides a link between the suspect and item 1 whilst $E_2$ provides a link between item 1 and item 2. etc. leading to a logical sequence that links the suspect to the activity. 

The graphical model in Figure \@ref(fig:serial-evidence-chain-diagram) also shows a specific type of evidence scheme. Objects along this type of scheme logically depend upon those which come before them in order to construct the sequence in the graph. For example, without $E_2$ there is no longer a chain of evidence between the suspect and the activity since there is no longer a link between item 1, which we can directly link to the suspect, and item 2. This type of evidence scheme is known as a serial evidence scheme [@koeijer2020] because of the serial nature of the evidence chain that it represents. This logical structure of the evidence must be accounted for in the probability model for this evidence chain and in the LR calculations. More specifically, this graph indicates that $E_1$ and $E_2$ cannot be considered as statistically independent conditional on the case circumstances and propositions because there is a clear dependence between $E_1$ and $E_2$ through item 1. The probability calculations to model these dependencies and output an LR can be complex and often use advanced probabilistic modelling methods such as Bayesian Networks, which are introduced in Book 3 of the RSS practitioner guides [@roberts2014]. Bayesian Networks provide a probabilistic graphical model for handling multiple evidence types and are a planned extension to this text.

Another type of evidence scheme is shown in the graphical model of Figure \@ref(fig:parallel-evidence-chain-diagram) below.
```{r parallel-evidence-chain-diagram, echo=FALSE, fig.cap="An example of a parallel evidence scheme in a graphical model. Item 1 and 2 from the chain of evidence only depend on each other through the activity and suspect and so could be considered as independent conditional on the suspect having perpetrated the activity, usually $H_p$.", out.width='100%', fig.align='center'}
# create simple graph of parallel evidence chain (e.g. fig. 4 de Koeijer 2020)
DiagrammeR::grViz("
graph {
   # a graph statement
  graph [rankdir = LR, fontsize = 10]
  
  # node font statement
  node [fontname = 'Sans-serif']
  
  # node statement for suspect
  node [shape = oval]
  A [label = 'Suspect']

  # node statement for items
  node [shape = rectangle]
  B [label  = 'Item 1']
  C [label = 'Item 2']
  
  # node statement for activity
  node [shape = diamond]
  D [label = 'Activity']
  
  # edge statements
  edge [fontname = 'Sans-serif']
  A--B [label = 'E@_{1}']
  B--D [label = 'E@_{2}']
  A--C [label = 'E@_{3}']
  C--D [label = 'E@_{4}']
}
")
```

The evidence scheme in Figure \@ref(fig:parallel-evidence-chain-diagram) represents a so-called parallel evidence scheme [@koeijer2020]. This is due to the two (or more) chains of evidence running in parallel to each other.

When the individual pieces of evidence are regarded as statistically independent (Section \@ref(double-coin-toss)), their individual LRs can be multiplied to obtain a combined LR. Labelling two pieces of evidence as $E_1$ and $E_2$, then the interpretation of the LR becomes: how much more likely is it that both $E_1$ **and** $E_2$ would have been observed if $H_p$ is true, rather than if $H_d$ is true. This approach is becoming more popular amongst some scientific groups as more methodology is being developed, e.g. @koeijer2020. It can only be done when the propositions underlying the probability calculations are the same, and specifically when they are at the same proposition level. For example, there might be both glass and DNA evidence which can address an activity level proposition of the suspect breaking and entering a residential home. When the proposition levels are different, then a more complex approach may be required.

Assuming statistical independence leads to convenient probability and LR calculations. However, when this assumption is incorrect then it leads to inaccurate probability and LR values and therefore to potentially misleading conclusions. For this reason, assumptions of statistical independence should be justified whenever they are made. Data sources, such as scientific experiments can be used to establish whether two events are (conditionally) statistically independent. Data sources that can help scientists to determine statistical independence between evidence types are limited in the scientific literature at this time. When the statistical independence assumption does not hold true, then more complex probabilistic methods need to be used. One such method for achieving this is Bayesian Networks, which are introduced in Book 3 of the RSS practitioner guides [@roberts2014]. Bayesian Networks provide a probabilistic graphical model for handling multiple evidence types and are a planned extension to this text. 

<!-- ## Example:  -->

<!-- ## Example: DNA loci -->
<!-- Combining evidence like this is the standard approach underlying DNA evidence evaluation. -->

## Robustness

The LR is based upon two underlying conditional probabilities. Each of these probabilities has its own degree of reliability, which depends upon factors listed in Section \@ref(reliable-probabilities). The degree to which these probabilities are reliable determines the overall robustness of an LR. The more reliable each of the probabilities is, the more robust the LR that is produced. It is worth noting that an LR which is not robust might still be the best available LR in the case circumstances, e.g. if there is limited data and expertise with which to inform the probability assignments. 

Any critical assessment of an LR is a critical assessment of its underlying probabilities. If an expert gives an LR as an order of magnitude, then the underlying probability assignments should be still consistent with this. For example, if the LR is between 100-1000, then the conditional probabilities which form this LR must be consistent with this value. Using this range of values for the LR, the greatest possible value for the probability of $E$ assuming $H_d$ would lie between 0.001-0.01. Otherwise, in order to make an LR or 100-1000, the probability of $E$ assuming $H_p$ would need to be greater than 1 which is impossible for a probability. You can explore these values as well as others using the LR calculator from Section \@ref(lr-calc). Notice how if you fix the probability of $E$ assuming $H_d$ to be 0.01 then it is impossible to get an LR of more than 100.

Assessing the robustness of an LR is aided by making a distinction between types of information that the expert uses when constructing conditional probabilities. In the Judicial Statistics Primer [@statsprimer2020] this information is divided into two categories:

-[S]: knowledge derived from robust systematic studies, ideally published, where the relevant features have been measured and studied statistically

-[E]: knowledge derived from personal experience, i.e. the expert's training and professional experience in the forensic specialism.

The robustness of an LR that has been informed by the [S] category can be assessed using the quoted studies and their data. The study design (observational, randomised controlled trial, etc.) and its limitations might be relevant, as well as its applicability to the case circumstances at hand. The robustness of studies and databases can be affected by factors such as the study sample size, the precision of collected measurements, any bias in sampling methodology, as well as others.

The robustness of an LR that has been informed by the [E] category can be assessed by focussing on the experience of the expert. This could involve the expert detailing their previous experience and its relevance to the present case. This evidence could include previous proficiency or calibration tests in which the expert has participated. The robustness of these tests may be assessed by considering their study design, such as whether the tests were conducted blind and without knowledge that an assessment was taking place, how relevant the study conditions were to regular casework, how many times the study tasks were repeated, as well as other factors. This category is also more likely to be where different experts disagree, since it contains more subjective knowledge than [S] does. Since this category is more subjective, there might be varying degrees of consensus within the expert's scientific field. Disagreements between experts may be reasonable and can be presented to the court for examination.

The robustness of a particular LR can also be empirically assessed by performing a sensitivity analysis. This involves making reasonable changes to the probabilities underlying the LR and observing how the LR varies with these changes. The changes in the probabilities are made to reflect uncertainty that might reasonably have been unaccounted for. This could include making minor adjustments to the data by which the probability was informed, using a range of reasonable values to inform the probability where previously a single value was used, or perhaps changing a key assumption that was made in the original probability assignment. A sensitivity analysis shows how sensitive the LR is to the method or data which was used to calculate the LR. An LR whose value does not meaningfully vary with reasonable changes to its underlying models and assumptions is more robust than one which does vary. Exactly what is meant by 'meaningful changes' will depend on the case circumstances and might be decided by the expert.

There are two other ways in which an expert may assign more robust LRs. The first is by assigning the LR to an interval of values rather than a single value. This might be done in situations in which the expert is unable to assign precise values for some sources of uncertainty, for example where relevant data is limited. The intervals shown in Table \@ref(tab:verbal-scale-table) are used for this purpose as well as for consistency of language. The second method is by assigning LRs conservatively in favour of the defence. This means assigning the LR to a minimum reasonable value, e.g. the smallest value from an interval which may have resulted from an interval assignment or sensitivity analysis. Doing so ensures that the LR is protected from overestimation that might unreasonably be biased against the accused.

## Communication

When the LR is chosen as the appropriate tool to convey the value of scientific evidence, it must still be understood by the fact finder in order to be effective. This means that the LR needs to be successfully communicated **to** fact-finders and legal counsel, as well as effectively communicated **by** expert witnesses, legal counsel, and the judge if it is the subject of any clarifications or guidance to jurors for a particular case. The expert determines the LR, but accurately communicating its meaning is a shared responsibility.

There are currently two prominent strategies for communicating LRs as an expert: numerically and verbally. In expert reports, the currently endorsed approach by the European Network for Forensic Science Institutes is to communicate LRs using their numerical value possibly converted into the verbal statements given in Table \@ref(tab:verbal-scale-table).

Suppose the following evidence and competing propositions:

- $E$: Blood-spatter pattern on the defendant's jeans. The blood is thought to originate from the complainant, which is a claim supported by DNA analysis, and this is not disputed by the defence. 

- $H_p$: the defendant kicked the complainant,
- $H_d$: the defendant did not kick the complainant. They claim to have been present but standing at least 5 metres away from the attack.

Suppose also that an expert has assigned an LR of 1000 in favour of $H_p$ compared to $H_d$. 

The most raw form of communicating this LR is in its numerical format. Such a statement could read like this:

> This pattern of blood spatter is 1000 times more likely to be seen if the defendant kicked the complainant, rather than if the defendant did not kick the complainant but was present during the attack standing at least 5 metres away.

Converting an LR of 1000 into a verbal statement using Table \@ref(tab:verbal-scale-table) could lead to a statement such as:

> This pattern of blood-staining provides strong support for the defendant having kicked the complainant, rather than the defendant not having kicked the complainant but having been present during the attack standing at least 5 metres away.

Which of these statements is selected is a decision made by the expert, and may be determined by the practice used by the expert's employer. In any case, a justification for the LR being 1000 should be made clear in the expert's report. If any data have been used then this should be detailed. Any expert knowledge and experience should also be detailed so that it is available for audit in court. For example, an LR of 1000 may have been assigned as the most conservative value in favour of the defence based upon a combination of findings in academic literature and internal experiments to replicate the case circumstances - details of which could be disclosed in the report. This would represent a combination of information from systematic studies and personal experience and so from the more objective [S] and more subjective [E] categories from the previous section. The robustness of this LR can be more clearly tested using this information.

Suppose now that the expert had not detailed an LR of 1000 and instead opted to communicate the probative value using only the corresponding verbal category of 'strong support'. This could mean one of two things:

1. An LR has been assigned to a single value within the range of the verbal category and this was not communicated,
2. The LR has been assigned to an interval covered by the verbal category and this was not communicated.

The reasons for the expert to choose either of the above options could be useful in exploring the robustness of this LR. Using an interval for the LR represents uncertainty that may or may not have been accounted for when assigning a single value. On the other hand, the single value may have been a conservative assignment. Any data or experience that the expert has used should still be made clear in their report.

<!-- Since the conditional probabilities which determine the LR are made explicit in this example (this is not common practice), then the LR can also be communicated visually using natural frequencies in Figure \@ref(fig:freq-tree-lr-kicking): -->

<!-- ```{r freq-tree-lr-kicking, echo=FALSE,fig.cap="The terms which contribute to the LR are shown in bold font. Since the populations we assumed are the same size, the LR is just 1000 divided by 1, or just 1000.", out.width = '80%', fig.align = 'center'} -->
<!-- e <- c(1, 2, 1, 3) -->
<!-- v <- c("10,000\nkickers", "10,000\nnon-kickers", "1000\nthis\npattern", "9000\nnot this\npattern", "1\nthis\npattern", "9999\nnot this\npattern") -->
<!-- freqTreeKick <- graph(edges=e, n=3, directed=FALSE) -->
<!-- freqTreeNonKick <- graph(edges=e, n=3, directed=FALSE) -->
<!-- V(freqTreeKick)$name <- v[c(1, 3, 4)] -->
<!-- V(freqTreeNonKick)$name <- v[c(2, 5, 6)] -->

<!-- colPal <- colorblind_pal()(8) -->
<!-- # the commented code below complicates the point -->
<!-- # V(freqTree)$color <- c(rep(colPal[1], 3),  -->
<!-- #                        colPal[4],  -->
<!-- #                        colPal[7], -->
<!-- #                        colPal[7], -->
<!-- #                        colPal[4]) -->
<!-- treeCol <- c(rep(colPal[1], 3)) -->
<!-- V(freqTreeKick)$color <- treeCol -->
<!-- V(freqTreeNonKick)$color <- treeCol -->
<!-- treeFont <- c(2, 2, 1) -->
<!-- V(freqTreeKick)$label.font <- treeFont -->
<!-- V(freqTreeNonKick)$label.font <- treeFont -->
<!-- par(mar = c(0, 0, 0, 0), mfrow=c(1, 2)) -->
<!-- plot(freqTreeKick, vertex.shape="none", vertex.label=V(freqTreeKick)$name, -->
<!--      vertex.label.color=V(freqTreeKick)$color, vertex.label.font=V(freqTreeKick)$label.font, -->
<!--      vertex.label.cex=1.2, edge.color="grey70",  edge.width=2, -->
<!--      layout=layout_as_tree(graph = freqTreeKick, root = 1), -->
<!--      vertex.size=50) -->
<!-- # legend("bottomright", legend=c("True", "False"), -->
<!-- #        col=colPal[c(4,7)], bty = "n", -->
<!-- #        pch=16) -->
<!-- plot(freqTreeNonKick, vertex.shape="none", vertex.label=V(freqTreeNonKick)$name, -->
<!--      vertex.label.color=V(freqTreeNonKick)$color, vertex.label.font=V(freqTreeNonKick)$label.font, -->
<!--      vertex.label.cex=1.2, edge.color="grey70",  edge.width=2, -->
<!--      layout=layout_as_tree(graph = freqTreeNonKick, root = 1), -->
<!--      vertex.size=50) -->
<!-- par(mar = defMar, mfrow=c(1, 1)) -->
<!-- ``` -->

<!-- An accompanying verbal statement could be: -->

<!-- > Out of 10,000 kickers in an equivalent situation to the defendant in the prosecution's version of events, this blood pattern would be found on 1000 of them. Out of 10,000 non-kickers in an equivalent situation to the defendant in the defence's version of events, this blood pattern would be found on 1 of them.  -->

<!-- Notice how the expert did not specify a prior probability of being the kicker or not, and so we had to split the trees into two groups, one for the kickers, and one for the non-kickers. -->

The effectiveness of communicating the LR is an active area of scientific research. Communication using the numerical and verbal scales presented in this section has shown mixed results in scientific studies employing experiments with mock jurors. Other methods of communication or evidence evaluation are likely to develop as we gather a better understanding of how LR communications are received and actioned upon by those within the courtroom.

<!-- ## More information -->

```{block eval=FALSE, include=isDynamicOutput()}
## Summary: likelihood ratio

Use the activity below to create a summary of the key points from this chapter.
```
``` {r likelihood-ratio-summary-questions, echo=FALSE}
if(isDynamicOutput()){
  knitr::include_url(paste0(QUESTIONS_HOST,"Likelihood-Ratio-Summary.html"), height=800)
}
```

## Research study

If you are taking part in our research study, please return to the survey now to answer the questions about this chapter.