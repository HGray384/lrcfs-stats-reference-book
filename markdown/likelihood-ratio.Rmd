# Likelihood ratio as value of evidence {#likelihood-ratio}

```{r package-load-likelihood-ratio, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
needPackages("igraph",
              "ggthemes",
              "kableExtra",
              "tidyverse")
# set some useful variables
source("./code/useful-variables.R")
```

This Chapter introduces the likelihood ratio

## Relative support for competing propositions

Suppose that we have two competing propositions, A and B, as in the previous chapter. We observe an event E, and wish to know whether E was more likely assuming A or B. If E is more likely assuming A was true, then we say that E provides more support for A than B, and vice versa if E is more likely assuming B to be true. The likelihood ratio (LR) quantifies this support.

The LR consists of the probability of observing E conditioned on A being true, divided by the probability of observing E conditioned on B being true. As a formula it is written as
$$\text{LR}=\frac{\text{probability of E assuming A is true}}{\text{probability of E assuming B is true}}.$$

The trick here is that by assuming each proposition to be true in turn, we can see how much more likely E was to occur in A's version of events compared to B's.

Since each term in the LR is a probability, their value must lie between 0 and 1. This means that the LR itself must be between 0 and $\infty$. Values of the LR which are greater than 1 indicate relative support for A compared to B, since it means that the probability of E assuming that A is true is greater than the probability of E assuming B is true. Values of the LR which are less than 1 indicate relative support for B compared to A, since it means that the probability of E assuming that B is true is greater than the probability of E assuming A is true. Values of the LR which are equal to 1 indicate that E provides equal support for A and B when compared to each other. This is shown in Table \@ref(tab:lr-meaning-table).

```{r lr-meaning-df, include=FALSE}
lrDf <- tibble("LR" = c("less than 1",
                        "equal to 1",
                        "greater than 1"),
                  "Meaning"=c("More support for B compared to A",
                              "Equal support for both when compared to each other",
                              "More support for A compared to B"))
```

```{r lr-meaning-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
lrDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Meaning of values of the LR for event E.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```

## Strength of support

The magnitude of the LR conveys the strength of the support which E provides for A or B when compared to each other. LRs of 1,000,000 and 10 both provide support for proposition A compared to B, but the LR of 1,000,000 provides much stronger support than the LR of 10 does. Similarly, LRs of 0.0000001 and 0.1 provide support for proposition B compared to A, but the LR of 0.0000001 provides much stronger support than the LR of 0.1 does. As the value of the LR gets further away from 1, the stronger the support is for proposition A or B (depending on whether the LR is greater or less than 1) when compared to its competitor.

One method to convey the numerical strength of an LR is to place it into a category of verbal expressions based on its numerical magnitude. Multiple suggested categorisations exist, such as the example given below in Table \@ref(tab:verbal-scale-table).

```{r verbal-scale-df, include=FALSE}
strength <- c("weak",
              "moderate",
              "moderately strong",
              "strong",
              "very strong",
              "extremely strong")
strengthSup <- paste(strength, "support")
props <- c("for A compared to B",
           "for B compared to A")
verbExpr <- c(paste(rev(strengthSup), props[2]),
              "equal support for A and B",
              paste(strengthSup, props[1]))
lrRange <- c("less than 0.000001",
             "at least 0.000001 but less than 0.0001",
             "at least 0.0001 but less than 0.001",
             "at least 0.001 but less than 0.01",
             "at least 0.01 but less than 0.1",
             "at least 0.1 but less than 1",
             "1",
             "at least 1 but less than 10",
             "at least 10 but less than 100",
             "at least 100 but less than 1000",
             "at least 1000 but less than 10,000",
             "at least 10,000 but less than 1,000,000",
             "at least 1,000,000")
vsDf <- dplyr::tibble("LR"=lrRange, "verbal expression"=verbExpr)
```


```{r verbal-scale-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
vsDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Verbal expressions to convey the strength of numerical LRs.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")


```

The LR is determined by the values of the conditional probabilities which underly it. Figure \@ref(fig:lr-function) shows how the value of the LR changes based on the values of these probabilities.

```{r lr-function, echo=FALSE, fig.cap="LR values as its underlying probabilities vary. Values are coloured by their support for either proposition: blue is support for A compared to B, and orange is support for B compared to A. White is equal support for both.", out.width = '80%', fig.align = 'center'}
gridVals <- 101
prob1 <- prob2 <- seq(0, 1, length.out = gridVals)[-1]
vars <- expand.grid(prob1, prob2)
lrDf <- tibble(prob1 = vars$Var1, prob2 = vars$Var2)%>%
  mutate(lr = prob1/prob2)

ggplot(lrDf, aes(x=prob2, y=prob1, fill=lr)) +
  geom_tile() +
  xlab("probability of E assuming B is true") +
  ylab("probability of E assuming A is true") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_gradientn(colors = c(colPal[7],
                                  colPal[2],
                                  "white",
                                  colPal[3],
                                  colPal[6]),
                       breaks = c(0.01, 0.1,
                                  1,10, 100),
                       labels=c(0.01, 0.1,
                                  1,10, 100),
                       name="LR", trans = "log"
                       # values = c(0, 0.4, 
                       #            0.5, 
                       #            0.6, 1)
  )
```

Figure \@ref(fig:lr-function) highlights some important properties of the LR:

- LR values close to 1 tell us nothing about the values of their conditional probabilities other than that they are roughly equal. So long as the probabilities are similar in size, extremely unlikely situations will be assigned the same LR as absolutely certain ones. This can be seen by the white strip across the diagonal of the figure.
- Most values for the probabilities lead to LRs which are no more than "moderate support" for either proposition in light of Table \@ref(tab:verbal-scale-table). This is the consequence of the probabilities being less than 1; if the probability of E conditioned on B is 0.25, then we know the LR can have a maximum value 4, since the probability of E conditioned on A is at most 1.
- Following on from the previous point, large or small LRs can only be obtained when one probability is small, i.e. when E is highly unlikely conditioned on one of the propositions. This can be seen by the block of darker colour for the smallest value of each of the probabilities. In order to see large or small LRs, we would really have to zoom into the areas of this plot where one probability is tiny, and this cannot be seen using the same scale of this current plot.

## Example: DNA match

Suppose a high quality full DNA profile is recovered from a blood stain at a crime scene. This is known as the **questioned profile**. The profile is analysed and is determined to contain only one person's DNA, making it a **single donor profile**. A suspect is detained and their DNA profile is taken. This is known as the **reference profile**. The reference profile is found to 'match' the questioned profile. Consider the following competing source-level propositions:

- the suspect is the source (of the questioned profile)
- someone other than the suspect is the source (of the questioned profile).

In this situation the DNA 'match' is the observation for which we would like probabilities conditional on the above propositions. The LR is given by

$$\text{LR}=\frac{\text{probability of a match assuming the suspect is the source}}{\text{probability of a match assuming someone other than the suspect is the source}}.$$
To obtain the LR, we need to obtain values for the above conditional probabilities. Let's consider the numerator first. 

The probability of obtaining a match assuming that the suspect is the source is usually set to 1; it is considered to be certain that a match would be obtained if the suspect was the source. This is reasonable although it is not strictly true. There is always the risk of a false positive or other laboratory or technical errors occuring, but it is assumed that the risks of these errors are negligible, especially for high-quality single donor full profiles. 

The denominator, the probability of obtaining a match assuming that someone other than the suspect is the source, is more nuanced. This is known as the **random match probability** (RMP). The RMP reflects how common the recovered profile is in the relevant population for the case. The more common the profile, the higher the RMP and the lower the LR when the formula is applied. The logic behind this is the following: the more common a characteristic is in a population, the worse that characteristic is at discriminating between propositions. Applied in this case, the previous logic states that more common DNA profiles are worse at discriminating between source-level propositions and so they decrease LRs accordingly. Figure \@ref(fig:freq-tree-rmp) visualises an RMP of 1 in 40 million for a population of 40 million and 1 people, which also includes the guilty individual.

```{r freq-tree-rmp, echo=FALSE,fig.cap="Out of 40 million innocent people, 1 DNA profile matches. The RMP is 1 in 40 million. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("40,000,001 \npeople", "1\nguilty", "40,000,000\ninnocent", "1\nmatch", "0\nno match", "1\nmatch", "39,999,999\nno match")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(1, 1, 2, 1, 1, 2, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

The RMP is calculated using known frequencies of profiles from specific ethnic groups of people, since ethnic group is a large factor in determining genetic variation. The genetics behind DNA evidence is highly discriminating between individuals, so the RMP is usually very small (it can be in the order of 1 in 1 billion). This results in LRs in the order of millions or billions for competing source level propositions for single donor full profiles. 

In the below interactive example, you can change the value for the RMP and see what affect that has on the LR.

[Interactive example with RMP; display graph of LR as a function of the RMP, the user can highlight points on the graph or input values of the RMP to see what the corresponding LR is.]

In the above discussion, we simplified things by ignoring an important issue of relatedness. For example, the degree of relatedness in a population affects the RMP. This is known as the **population sub-structure** or **co-ancestry**. It is a numerical factor which adjusts the RMP to take account of the co-ancestry present in the relevant population, which again often differs by ethnic background.

We also did not consider direct blood relatives. Our second proposition considered *anyone* other than the suspect. But if we considered close genetic relationships, such as siblings and parents of the suspect, then the RMP would not reflect the probability of matching them. The genetic similarity between these individuals would mean the probability of a match is much higher than the RMP. For this reason, and when there is no reason to suspect close relatives of the suspect, the proposition advocating non-guilt is sometimes changed to

- someone unrelated to the suspect is the source (of the questioned profile),

and the RMP is used to calculate the LR as usual.

There are also other corrective factors that are applied to calculate the conditional probabilities which feed into the LR in the context of DNA evidence, but they are not not mentioned here.

## Updating odds

The LR has a very clear interpretation from Bayes' theorem (Chapter ...)
$$\text{posterior odds} = \text{LR} \times \text{prior odds}.$$
It is the numerical factor by which we multiply odds before particular information (prior odds) in order to obtain odds conditioned on that information (posterior odds); the LR tells us *how much* to update beliefs in light of information. 

We saw in Table \@ref(tab:lr-meaning-table) that values of the LR equal to 1 meant that the event provided equal support for both propositions when compared to each other. Logically, if an event provides equal support for two competing propositions, then it should not update any beliefs we had prior to observing it. If we look at Bayes' theorem above and plug in an LR of 1, then this is exactly what happens. The prior odds between the propositions is equal to the posterior odds between them; observing the event did not change the odds. 

We also saw that when the LR is greater than 1, then the event E provides more support for proposition A than proposition B. The intuition behind that statement is that, after observing E, we should update our prior belief to be more in favour of proposition A (than B) than it was. This is confirmed with Bayes' theorem above; when the LR is greater than 1, then the prior odds in favour of A *increases* to become the posterior odds. Observing the event E was found to be more probable assuming A than B, and so the odds were increased in favour of A by a specific factor, which is given by the LR.

[example in which LR>1 and can be changed with fixed prior odds]

We lastly discuss when the LR is less than 1. In this situation, the event provides more support for proposition B than proposition A. This tells us that we should update our prior belief to be more in favour of proposition B (than A) that it was. Looking at Bayes' theorem, this is exactly what happens; an LR less than 1 means that the prior odds in favour of A are *reduced* to become the posterior odds. Reducing the odds in favour of A means *increasing* the odds in favour of B and so our intuition is confirmed. Observing the event E was found to be more probable assuming B than A, and so the odds were reduced in favour of A (increased in favour of B) by a specific factor, which is given by the LR.

[example with changable LR<1 with fixed prior odds]

## Example: doping (revisited)

Recall the example of detecting doping athletes from Chapter \@ref(exm-doping).

The test returns positive for 95 out of every 100 doping athletes. The test returns negative for 95 out of every 100 non-doping athletes. It was speculated that 2 out of every 100 athletes are doping. We used these numbers to work out the probability of an athlete doping given that they tested positive as around 28%.

A related question is this: How much more likely are we to see a positive test result when the athlete is doping compared to when the athlete is not doping? This question is important as it tells us how informative a positive test result is directly in relation to the information we're interested in: doping versus non-doping. And this is precisely the type of question that an LR can be used to address. 

In order to think about an LR, we need to break this question down into its implied competing propositions and the available evidence. 

Competing propositions:

- the athlete is doping
- the athlete is not doping

Event: 

- a positive test result

The likelihood ratio is given by
$$\text{LR} = \frac{\text{probability of a positive test result assuming the athlete is doping}}{\text{probability of a positive test result assuming the athlete is not doping}}.$$

Note how this is the ratio of the probabilities for the evidence having conditioned upon the competing propositions. The conditional probabilities which contribute to this LR can be extracted from Figure \@ref(fig:freq-tree-lr-doping), in which we assume a population of 10,000 athletes. 
```{r freq-tree-lr-doping, echo=FALSE,fig.cap="The terms which contribute to the LR are shown in bold font. ", out.width = '80%', fig.align = 'center'}
e <- c(1, 2, 1, 3, 2, 4, 2, 5, 3, 6, 3, 7)
v <- c("10,000 \nathletes", "200\ndoping", "9,800\nnot doping", "190\npositive", "10\nnegative", "490\npositive", "9,310\nnegative")
freqTree <- graph(edges=e, n=7, directed=FALSE)
V(freqTree)$name <- v

colPal <- colorblind_pal()(8)
# the commented code below complicates the point
# V(freqTree)$color <- c(rep(colPal[1], 3), 
#                        colPal[4], 
#                        colPal[7],
#                        colPal[7],
#                        colPal[4])
V(freqTree)$color <- c(rep(colPal[1], 7))
V(freqTree)$label.font <- c(1, 2, 2, 2, 1, 2, 1)
par(mar = c(0, 0, 0, 0))
plot(freqTree, vertex.shape="none", vertex.label=V(freqTree)$name,
     vertex.label.color=V(freqTree)$color, vertex.label.font=V(freqTree)$label.font,
     vertex.label.cex=1.2, edge.color="grey70",  edge.width=2,
     layout=layout_as_tree(graph = freqTree, root = 1),
     vertex.size=50)
# legend("bottomright", legend=c("True", "False"),
#        col=colPal[c(4,7)], bty = "n",
#        pch=16)
par(mar = defMar)
```

For the numerator of the LR, "assuming that the athlete is doping"  means that we are only looking at the branches of the tree for the doping athletes. We can see that 190 out of these 200 doping athletes test positive, a probability of 0.95 (this is also the sensitivity in the original example).

For the denominator of the LR, "assuming that the athlete is not doping" means that we only look at the branches of the tree for non-doping athletes. We can see that 490 out of the 9,800 non-doping athletes tested positive, a probability of 0.05 (this is 1 minus the specificity from the original example).

The likelihood ratio is then $\frac{0.95}{0.05}=19$. The meaning of this can be expressed in a number of equivalent ways:

- a positive test updates our prior odds that the athlete is doping, increasing them by a factor of 19,
- a positive test is 19 times more likely to be observed from doping athletes than a positive result from non-doping athletes,
- a positive test result provides 19 times more support for the proposition that the athlete is doping compared to not doping. 

All of these expressions comment on the relative probability of observing a positive test result under the assumptions of the athlete doping and not doping. They do **not** state a value for the probabilities of the athlete doping or not, and they do **not** state that the athlete is 19 times more likley to be doping than not. To believe the latter would be to illegitimately transpose the conditional and commit the prosecutor's fallacy [insert section reference]. 

To finish this example, let's take a look at the odds formulation of the question from Section \@ref(exm-doping): what are the odds of the athlete doping versus not doping given a positive result of the test?

From Figure \@ref(fig:freq-tree-lr-doping) we can extract the odds of a randomly selected athlete doping, versus not, prior to being tested. This is given by $\frac{200}{9800}$, which can be simplified to $\frac{1}{49}$ and also expressed as odds of $1:49$, or 49 to 1 against doping.

Now, using Bayes' theorem and the LR, we find the posterior odds to be
\begin{align}
  \text{posterior odds} &= \text{LR} \times \text{prior odds}, \\
  \text{posterior odds} &= 19 \times \frac{1}{49}, \\
  \text{posterior odds} &= \frac{19}{49},
\end{align}
corresponding to posterior odds of $19:49$, or 49 to 19 against doping given a positive test result. Converting this back to probability gives $\frac{19}{19+49}=0.2794118$, about 28%. This is the same as the answer that we worked out in Section \@ref(exm-doping).

Even though the likelihood ratio for a positive test gave support in favour of the athlete doping, the posterior probability for the proposition that the athlete is doping is still rather small. This is an important point; the likelihood ratio alone does not tell us anything about the absolute value of the posterior probability for a proposition, it only tells us the relative value compared to the prior probability. 

## Probative value of evidence

So far we have considered a generic event E and propositions A and B. When we view this framework from the perspective of a forensic evaluation, the event E can be seen as a piece of evidence and the propositions A and B are the propositions put forward by the prosecution and defence, hereon referred to as $H_p$ and $H_d$. These can be thought of as the prosecution and defence's claimed version of events, the truth of which we are uncertain.

Within this specific situation, the LR tells us the relative support that a piece of evidence provides for $H_p$ or $H_d$. In this sense, the LR conveys the **probative value** of a piece of evidence. It tells us how much more likely a piece of evidence is under the prosecution's version of events when compared to the defence's version of events.

This interpretation of the LR is one of the reasons why it is advocated as a tool to quantify expert testimony. It is the role of the expert witness to present the probative value of scientific evidence within their domain of expertise to the court. The LR provides a logical means to achieve this. The fact finder can then use the probative value of evidence given by the LR to reason about the truth of $H_p$ or $H_d$. This process can clearly be seen using Bayes' theorem again, expanded on below.

Using E as a specific piece of evidence and the specific propositions $H_p$ and $H_d$, the odds form of Bayes' theorem becomes
$$\frac{\text{probability of } H_p \text{ having accounted for the evidence}}{\text{probability of } H_d \text{ having accounted for the evidence}}=\text{LR}\times\frac{\text{prior probability of } H_p}{\text{prior probability of } H_d}.$$
It can be seen that the LR updates beliefs about $H_p$ and $H_d$. Notice how beliefs about $H_p$ and $H_d$ are for the fact finder to determine, and the LR is provided by the expert witness. This is the role that the LR plays with the fact finder: the expert witness provides the LR, and in doing so provides the quantitative factor by which the fact finder should update their odds of each counsel's version of events.

## Combining evidence

LRs can also be used to consider the value of multiple pieces of evidence together. 

When the individual pieces of evidence are regarded as statistically independent (Chapter ...), probability theory allows us to multiply their individual LRs to obtain a combined LR.

## Example: DNA loci


## Robustness

- [S] versus [E] in the primer
- changes in assumptions/values


[interactive example changing probability inputs to the LR]

## Communication

[verbal scale]

## More information

## Exercises