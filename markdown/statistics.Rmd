# Statistics to infer uncertainty {#statistics}

```{r package-load-statistics, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
needPackages("igraph",
             "ggthemes",
             "kableExtra",
             "tidyverse",
             "plotly",
             "grid",
             "plyr")
# set some useful variables
source("./code/useful-variables.R")
```

In the last Chapter we spoke about probability as a means of quantifying our uncertainty. Probability was introduced as personal in the sense that it depends upon the information and beliefs of an individual. However, these beliefs are not completely arbitrary as they should always be a best assessment of the available information. One way to improve this assessment and increase and share available information is to gather data from empirical observations. These data can then be used to inform our beliefs and create more reliable probabilities. 

The branch of mathematics that is concerned with describing and learning from empirical observations is known as statistics. In this Chapter we will look at statistics, how it relates to probability, what are some problems that statistics can be useful for, and how this is applied in forensic scientific evidence. 

## Learning from data

When we were thinking about probability, we were quantifying our theoretical uncertainties. This involved us thinking about our uncertainties to map out what we believe could occur in theory, sometimes creating a probabilistic model for this, and then using that theory to form expectations about would occur in practice. In the coin toss example, my probability of heads being assigned as 0.5 meant that I expect 5000 heads out of 10,000 tosses of that coin. But, what I expect to occur is rarely what we observe in reality.

Statistics is all about doing this process in reverse. We use observed events in the real world in order to try to learn about what kind of probabilistic models might have lead to them. For example, suppose we tossed a new coin 10 times and observed 7 heads. What should we infer from these tosses about the probability of heads for this new coin? One reasonable answer would be a probability of heads of 0.7 because that is the proportion of heads that we actually observed. Another reasonable answer is still 0.5 because, although this means that we would expect 5 heads from 10 tosses, we are not guaranteed to always observe 5 heads for every 10 tosses. We might ask how rare it would be to observe a sequence of 7 out of 10 heads when the probability of heads is 0.5 compared to when the probability of heads is 0.7. How we might then choose between these inferences about the probability of heads is a question that statistical theory can help with.

Probability and statistics are complementary. Probability is useful because it can quantitively describe our theoretical uncertainty. Statistics is useful because it can quantitively describe the uncertainty that is observed in practice. This empirical uncertainty can then be used to compare to and refine our theoretical uncertainty. Where possible and practical, this process can then be repeated many times to reduce epistemic uncertainty and characterise aleatory uncertainty.

This results in probabilistic models which are continually improved by gathering **data** about them from empirical observations. In the previous short example, when we tossed the new coin 10 times we generated data about the coin toss. The data was the ten coin tosses and their results. We can summarise data or perform mathematical operations on it to create quantities known as **statistics**. For example, by summarising that 7 out of the 10 tosses (70%) resulted in heads, we have a statistic which describes the proportion of heads in that coin toss data.

We then used this data, and the statistic computed from it, to think about the probability of heads occurring from the toss of the new coin. We could toss the coin many more times to gather more data, compute more statistics, and reassess our probabilities. This is called learning from data. Using statistical theory makes the magnitude of learning match the amount of information in the data (subject to our beliefs); without using statistical theory this would not be guaranteed.

Using probability and statistics together in this way has many similarities with the scientific method; generating hypotheses and then performing experiments to gather evidence to test those hypotheses against. This methodology is why it is sometimes known as **statistical science**. In most of the processes in the real world we do not know the underlying models which cause them and so we use statistical science to learn about them. This can be done to achieve many goals, and we restrict ourselves to the following:

- describing empirical observations,
- inferring general conclusions from empirical observations,
- evaluating empirical observations.

We get to these topics in the next few sections but first we need to introduce a couple of important concepts.

## Populations and samples

Two key concepts in statistics are populations and samples. A **population** is every possible member of a group of interest. A **sample** is a subset of members taken from the population.

## Describing observations

Using statistics to describe observations is probably their most familiar application. This is because they concisely and precisly report and summarise data, and this plays a central role in our ever growing data-centric society. These type of statistics are referred to as **descriptive statistics**. Some common examples are the average height of males or females in the UK, median household income, most popular baby's name for a girl etc.



## Example: GSR publications

In 2020, a systematic analysis of all academic publications related to gunshot residue (GSR) particles was published by @sobreira2020. A small subset of the data, containing information about the number of publications per year, can be found below in Table \@ref(tab:gsr-publication-table).

```{r gsr-publication-df, include=FALSE}
gsrDf <- as.tibble(read.table("./dat/gsr-publications.txt", header = TRUE, sep = "&")) %>%
  select(Year, Publications) %>%
  filter(Year > 2009)
```

```{r gsr-publication-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
gsrDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Academic publications related to gunshot residue analysis from the years 2010-2018.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```
## Inferring from observations

## Data sources

## Example: some real events

The below Figure shows some probabilities for more tangible events than theoretical coin tosses.

```{r, prop-ruler, echo=FALSE, fig.align = 'center'}
ruler.func<-function(gg){
seq.list<-list()
for(i in 1:length(gg)){  
  ystart<-seq(0.1,gg[i],0.1)
  yend<-ystart
  xstart<-rep(i-0.25,length(ystart))
  xend<-xstart+0.1
  nam.val<-c(LETTERS[i],rep(NA,length(ystart)-1))
  numb.val<-c(gg[i],rep(NA,length(ystart)-1))
  seq.list[[i]]<-data.frame(nam.val,numb.val,xstart,xend,ystart,yend)
}
df<-as.data.frame(do.call(rbind, seq.list))
p <- ggplot(df, aes(nam.val))
p <- p + geom_bar(aes(y=numb.val,fill=nam.val),stat="identity",width=0.5,color="black",lwd=1.1)+
    scale_x_discrete(limits=LETTERS[1:length(gg)])+
    geom_segment(aes(x=xstart,y=ystart,xend=xend,yend=yend))+
    geom_hline(yintercept=c(0.25, 0.5, 0.75),color="white",lwd=1.1)+
    ggtitle("Probability of real events")+
    ylim(c(0,max(gg)+0.5))+
  annotate("text",x=seq(1,length(gg),1),y=gg+0.1,label=gg,fontface="bold",size=rel(6))+
  theme_bw()+
  theme(axis.title=element_blank(),
        axis.text.y=element_blank(),
        axis.text.x=element_text(face="bold",size=rel(1.5)),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        panel.grid=element_blank(),
        legend.position = "bottom",
        legend.margin = margin()) +
  scale_fill_discrete(name="Event",
                      labels=c("an American male dying of cancer in their lifetime",
                               "any 2 people having the same birthday in a room of 23 random people",
                               "the Scottish city of Dundee being overcast on 26th January",
                               "a female born in 2020 living to age 70 or longer"))+
  guides(fill=guide_legend(nrow=4,byrow=TRUE))+
  coord_flip()
print(p)
}
suppressWarnings(ruler.func(c(0.21,0.51,0.68, 0.9)))
```

Whose probabilities are these?

## Relevant populations

## Statistics in forensic evidence

## More information

Cancer: https://www.cancer.org/cancer/cancer-basics/lifetime-probability-of-developing-or-dying-from-cancer.html

Life expectancy: https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthandlifeexpectancies/articles/lifeexpectancycalculator/2019-06-07

Weather: https://weatherspark.com/m/40087/6/Average-Weather-in-June-in-Dundee-United-Kingdom

Birthday: https://en.wikipedia.org/wiki/Birthday_problem

## Exercises

