# Statistics to infer uncertainty {#statistics}

```{r package-load-statistics, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
needPackages("igraph",
             "ggthemes",
             "kableExtra",
             "tidyverse",
             "plotly",
             "grid",
             "plyr")
# set some useful variables
source("./code/useful-variables.R")
```

In the last Chapter we spoke about probability as a means of quantifying our uncertainty. Probability was introduced as personal in the sense that it depends upon the information and beliefs of an individual. However, these beliefs are not completely arbitrary as they should always be a best assessment of the available information. One way to improve this assessment and increase and share available information is to gather data from empirical observations. These data can then be used to inform our beliefs and create more reliable probabilities. 

The branch of mathematics that is concerned with describing and learning from empirical observations is known as statistics. In this Chapter we will look at statistics, how it relates to probability, what are some problems that statistics can be useful for, and how this is applied in forensic scientific evidence. 

## Learning from data

When we were thinking about probability, we were quantifying our theoretical uncertainties. This involved us thinking about our uncertainties to map out what we believe could occur in theory, sometimes creating a probabilistic model for this, and then using that theory to form expectations about would occur in practice. In the coin toss example, my probability of heads being assigned as 0.5 meant that I expect 5000 heads out of 10,000 tosses of that coin. But, what I expect to occur is rarely what we observe in reality.

Statistics is all about doing this process in reverse. We use observed events in the real world in order to try to learn about what kind of probabilistic models might have lead to them. For example, suppose we tossed a new coin 10 times and observed 7 heads. What should we infer from these tosses about the probability of heads for this new coin? One reasonable answer would be a probability of heads of 0.7 because that is the proportion of heads that we actually observed. Another reasonable answer is still 0.5 because, although this means that we would expect 5 heads from 10 tosses, we are not guaranteed to always observe 5 heads for every 10 tosses. We might ask how rare it would be to observe a sequence of 7 out of 10 heads when the probability of heads is 0.5 compared to when the probability of heads is 0.7. How we might then choose between these inferences about the probability of heads is a question that statistical theory can help with.

Probability and statistics are complementary. Probability is useful because it can quantitively describe our theoretical uncertainty. Statistics is useful because it can quantitively describe the uncertainty that is observed in practice. This empirical uncertainty can then be used to compare to and refine our theoretical uncertainty. Where possible and practical, this process can then be repeated many times to reduce epistemic uncertainty and characterise aleatory uncertainty.

This results in probabilistic models which are continually improved by gathering **data** about them from empirical observations. In the previous short example, when we tossed the new coin 10 times we generated data about the coin toss. The data was the ten coin tosses and their results. We can summarise data or perform mathematical operations on it to create quantities known as **statistics**. For example, by summarising that 7 out of the 10 tosses (70%) resulted in heads, we have a statistic which describes the proportion of heads in that coin toss data.

We then used this data, and the statistic computed from it, to think about the probability of heads occurring from the toss of the new coin. We could toss the coin many more times to gather more data, compute more statistics, and reassess our probabilities. This is called learning from data. Using statistical theory makes the magnitude of learning match the amount of information in the data (subject to our beliefs); without using statistical theory this would not be guaranteed.

Using probability and statistics together in this way has many similarities with the scientific method; generating hypotheses and then performing experiments to gather evidence to test those hypotheses against. This methodology is why it is sometimes known as **statistical science**. In most of the processes in the real world we do not know the underlying models which cause them and so we use statistical science to learn about them. This can be done to achieve many goals, and we restrict ourselves to the following:

- describing empirical observations,
- inferring general conclusions from empirical observations,
- evaluating empirical observations.

We get to these topics in the next few sections but first we need to introduce a couple of important concepts.

## Populations and samples

Two key concepts in statistics are populations and samples. A **population** is every possible event/characteristic/individual in a certain group of interest. Some examples of populations are

Population 1: all of the people living the UK,

Population 2: all burglaries in the Greater London area of England,

Population 3: all the footwear of people living in a small local region.

Having information about these populations can have important practical impact. For example, knowing the locations of all burglaries in Greater London might reveal patterns such as hotspots. The Metropolitan Police could use this information to put preventative measures in place. Collecting data about the entire population is known as a **census**. The UK census is one such example, gathering data on people and households within the UK, occurring once every 10 years.

Censuses are often very difficult or impossible to fully complete. This can be for logistical reasons such as it being too expensive and resource intensive to conduct. It can also be because the population is unable to be fully surveyed. Population 3 above would be highly relevant for criminal investigations within the specific local region, e.g. for comparing footwear marks found at local crime scenes. However, it is practically impossible to gather and record all of this information. One reason for this would be legal restrictions preventing the systematic collection of this data.

For these reasons, it is common to take a **sample** from the population instead. A sample is a subset of members out of a population. Since samples don't aim to gather information from the entire population, they are much simpler and cheaper to conduct. As well as saving resources, the aim of taking a sample is to be sufficiently representative of the population. Below are some examples of samples:

Sample 1: a survey of people within the UK,

Sample 2: reported burglaries in a Met. Police database,

Sample 3: voluntarily submitted footwear marks from a local area.

Sampling has many different types as it is such an important aspect of statistics. In this text, we will only mention random samples and convenience samples. A **random** sample is a sample drawn from a population in such a way that every member in the population has an equal probability of being sampled. In sample 1 above, the survey can be conducted by randomly contacting potential respondents from a representative database of people.

A **convenience** sample is a sample drawn from a population in which only the most available members of the population are sampled. This means that convenience samples might not be representative of the population as a whole. For sample 2 above, it might be reasonable to say that these reported burglaries are fairly representative of all burglaries in Greater London i.e. population 2. This assumes that almost all burglaries are reported and logged and that there are no systematic differences in the burglaries which are not being reported (e.g. in their location).

Sample 3 is also a convenience sample, this time of submitted footwear. The degree to which the submitted footwear marks are representative of all footwear in a local area (e.g. population 3) depends upon the representativeness of the volunteers who submit them. For example, if a certain age group of people, say teenagers, is over-represented in the volunteers then the sample will be **biased** in favour of them. This means that when trying to count a simple measure such as the most common shoe size in the local area, the data will be biased in favour of shoe sizes which are probably smaller than the true most common size. This is because the teenagers will still be growing. If desired, statistical methods can be applied to take this bias into account, e.g. by weighing the teenager group's submissions proportionally less compared to other age groups. This is one way polling companies attempt to make a potentially biased sample more representative when trying to determine election outcomes.

Sampling adds another layer of uncertainty to consider, which lends itself to working with probability. In any sampling method, there is uncertainty about how representative the sample is of the population from which it is drawn, and also a natural randomness as to which members are sampled. This represents both epistemic and aleatory uncertainty. The epistemic uncertainty arises because we do not know some characteristics of the population (which is why we are sampling from it). Learning more about the population reduces this uncertainty, and so the sampling procedure itself can reduce this.

The aleatory uncertainty is because there will always be randomness when sampling. Randomness is clearly present in a random sampling procedure, but it is also present within convenience samples. This is because there is a randomness as to which convenient members are sampled. **Sample size** is an important factor for characterising the aleatory uncertainty associated with sampling. The more we perform a sampling procedure in a population, the more we learn about its inherent variability. But sample size isn't everything: a small but carefully constructed random sample can be more representative of the population than a large but careless convenience sample.

Now that we have seen what populations and samples are, we can look at the first use of statistics from our list: describing observations.

## Describing observations

Using statistics to describe observations is probably their most familiar application. This is because they concisely and precisly report and summarise data, and this plays a central role in our ever growing data-centric society. These type of statistics are referred to as **descriptive statistics**. Some common examples are the average height of males or females in the UK, median household income, most popular baby's name for a girl etc. Descriptive statistics give us information about a particular set of data. Two common categories these fall into are **central tendency** and **dispersion**.

Measures of central tendancy aim to tell us a number around which the data tend to cluster. There are three common measures used for this: the **mean**, **median**, and the **mode**. The **mean** describes the average value of the data. The **median** describes the value in the middle of the data after we put all of the numbers in order. The **mode** describes the most common value on the data.

Dispersion aims to tell us how closely the data cluster around the measures of central tendency. The most common of these known as the **standard deviation**. The **standard deviation** tells us on average how close the data are to the mean. Larger values indicate that the data is not well represented by the mean since they are spread widely around it. Smaller values indicate that the mean is a good representation because the data are tighly packed around it.

When naming specific descriptive statistics, it is good practice to mention whether that statistic is describing a sample of data or a population. This makes it clearer and more interpretable to the consumer of the statistic because they can think about it in context, e.g. "could the statistic be biased by convenience sampling?" When a statistic describes a population, it is referred to using words such as *population*, *true*, *parameter*, e.g. the population mean. When a statistic describes a sample it is usually referred to as a **sample statistic**, e.g. the sample mean. When this distinction is not made, there is the risk of confusing a sample for a population, which ignores uncertainty and leaves the potential to make an unwarrented and inaccurate generalisation. If in doubt, assume everything is a sample. Even for census data this can be a safe assumption since some forms are bound to not be completed or go missing after completion (even if this is a really small number).

The following example looks at descriptive statistics in action.

## Example: GSR publications

In 2020, a systematic analysis of all academic publications related to gunshot residue (GSR) particles was published by @sobreira2020. Among other things, the study looked at trends over time in the number of publications which are related to GSR. A small subset of the data, containing information about the number of publications per year between 2010-2018, can be found below in Table \@ref(tab:gsr-publication-table).

```{r gsr-publication-df, include=FALSE}
gsrDf <- as.tibble(read.table("./dat/gsr-publications.txt", header = TRUE, sep = "&")) %>%
  select(Year, Publications) %>%
  filter(Year > 2009)
```

```{r gsr-publication-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
gsrDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Academic publications related to gunshot residue analysis from the years 2010-2018.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```

Using this data, we can calculate measures of central tendency and dispersion. Let's assume this is a sample of the total population of GSR-related publications during the same timeframe. This is safe because some publications (albeit very few) may have been missed during the literature search.

To calculate the **sample mean**, we calculate the total number of publications and divide this by the number years we used. Using the GSR publication data, this gives:
$$\text{sample mean}=\frac{20+27+45+38+37+31+41+47+45}{9}=\frac{331}{9}\approx36.8,$$
which means that the sample mean number of GSR-related publications per year during the period 2010-2018 is roughly 36.8. In other words, if we wanted to report a single number for the rate at which GSR-related publications were published each year between 2010-2018 in this sample, then that rate would be 36.8. You can confirm this by multiplying 36.8 by the number of years (9) and compare that to the total number of publications.

Notice how 36.8 isn't itself a valid number of publications for any given year - the average doesn't exist as its own data point, it is a statistic which describes the data points. This highlights that there isn't such a thing as an "average year" for instance, which is sometimes how the average is reported. 



## Inferring from observations

## Data sources

## Example: some real events

The below Figure shows some probabilities for more tangible events than theoretical coin tosses.

```{r, prop-ruler, echo=FALSE, fig.align = 'center'}
ruler.func<-function(gg){
seq.list<-list()
for(i in 1:length(gg)){  
  ystart<-seq(0.1,gg[i],0.1)
  yend<-ystart
  xstart<-rep(i-0.25,length(ystart))
  xend<-xstart+0.1
  nam.val<-c(LETTERS[i],rep(NA,length(ystart)-1))
  numb.val<-c(gg[i],rep(NA,length(ystart)-1))
  seq.list[[i]]<-data.frame(nam.val,numb.val,xstart,xend,ystart,yend)
}
df<-as.data.frame(do.call(rbind, seq.list))
p <- ggplot(df, aes(nam.val))
p <- p + geom_bar(aes(y=numb.val,fill=nam.val),stat="identity",width=0.5,color="black",lwd=1.1)+
    scale_x_discrete(limits=LETTERS[1:length(gg)])+
    geom_segment(aes(x=xstart,y=ystart,xend=xend,yend=yend))+
    geom_hline(yintercept=c(0.25, 0.5, 0.75),color="white",lwd=1.1)+
    ggtitle("Probability of real events")+
    ylim(c(0,max(gg)+0.5))+
  annotate("text",x=seq(1,length(gg),1),y=gg+0.1,label=gg,fontface="bold",size=rel(6))+
  theme_bw()+
  theme(axis.title=element_blank(),
        axis.text.y=element_blank(),
        axis.text.x=element_text(face="bold",size=rel(1.5)),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        panel.grid=element_blank(),
        legend.position = "bottom",
        legend.margin = margin()) +
  scale_fill_discrete(name="Event",
                      labels=c("an American male dying of cancer in their lifetime",
                               "any 2 people having the same birthday in a room of 23 random people",
                               "the Scottish city of Dundee being overcast on 26th January",
                               "a female born in 2020 living to age 70 or longer"))+
  guides(fill=guide_legend(nrow=4,byrow=TRUE))+
  coord_flip()
print(p)
}
suppressWarnings(ruler.func(c(0.21,0.51,0.68, 0.9)))
```

Whose probabilities are these?

## Relevant populations

## Statistics in forensic evidence

## More information

Cancer: https://www.cancer.org/cancer/cancer-basics/lifetime-probability-of-developing-or-dying-from-cancer.html

Life expectancy: https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthandlifeexpectancies/articles/lifeexpectancycalculator/2019-06-07

Weather: https://weatherspark.com/m/40087/6/Average-Weather-in-June-in-Dundee-United-Kingdom

Birthday: https://en.wikipedia.org/wiki/Birthday_problem

## Exercises

