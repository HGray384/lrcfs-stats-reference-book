# Statistics to infer uncertainty {#statistics}

```{r package-load-statistics, include=FALSE}
source("./code/helper-functions.R")
# check and load the libraries
needPackages("igraph",
             "ggthemes",
             "kableExtra",
             "tidyverse",
             "plotly",
             "grid",
             "plyr")
# set some useful variables
source("./code/useful-variables.R")
```

In the last Chapter we spoke about probability as a means of quantifying our uncertainty. Probability was introduced as personal in the sense that it depends upon the information and beliefs of an individual. However, these beliefs are not completely arbitrary as they should always be a best assessment of the available information. One way to improve this assessment and increase and share available information is to gather data from empirical observations. These data can then be used to inform our beliefs and create more reliable probabilities. 

The branch of mathematics that is concerned with describing and learning from empirical observations is known as statistics. In this Chapter we will look at statistics, how it relates to probability, what are some problems that statistics can be useful for, and how this is applied in forensic scientific evidence. 

## Learning from data

When we were thinking about probability, we were quantifying our theoretical uncertainties. This involved us thinking about our uncertainties to map out what we believe could occur in theory, sometimes creating a probabilistic model for this, and then using that theory to form expectations about would occur in practice. In the coin toss example, my probability of heads being assigned as 0.5 meant that I expect 5000 heads out of 10,000 tosses of that coin. But, what I expect to occur is rarely what we observe in reality.

Statistics is all about doing this process in reverse. We use observed events in the real world in order to try to learn about what kind of probabilistic models might have lead to them. For example, suppose we tossed a new coin 10 times and observed 7 heads. What should we infer from these tosses about the probability of heads for this new coin? One reasonable answer would be a probability of heads of 0.7 because that is the proportion of heads that we actually observed. Another reasonable answer is still 0.5 because, although this means that we would expect 5 heads from 10 tosses, we are not guaranteed to always observe 5 heads for every 10 tosses. We might ask how rare it would be to observe a sequence of 7 out of 10 heads when the probability of heads is 0.5 compared to when the probability of heads is 0.7. How we might then choose between these inferences about the probability of heads is a question that statistical theory can help with.

Probability and statistics are complementary. Probability is useful because it can quantitively describe our theoretical uncertainty. Statistics is useful because it can quantitively describe the uncertainty that is observed in practice. This empirical uncertainty can then be used to compare to and refine our theoretical uncertainty. Where possible and practical, this process can then be repeated many times to reduce epistemic uncertainty and characterise aleatory uncertainty.

This results in probabilistic models which are continually improved by gathering **data** about them from empirical observations. In the previous short example, when we tossed the new coin 10 times we generated data about the coin toss. The data was the ten coin tosses and their results. We can summarise data or perform mathematical operations on it to create quantities known as **statistics**. For example, by summarising that 7 out of the 10 tosses (70%) resulted in heads, we have a statistic which describes the proportion of heads in that coin toss data.

We then used this data, and the statistic computed from it, to think about the probability of heads occurring from the toss of the new coin. We could toss the coin many more times to gather more data, compute more statistics, and reassess our probabilities. This is called learning from data. Using statistical theory makes the magnitude of learning match the amount of information in the data (subject to our beliefs); without using statistical theory this would not be guaranteed.

Using probability and statistics together in this way has many similarities with the scientific method; generating hypotheses and then performing experiments to gather evidence to test those hypotheses against. This methodology is why it is sometimes known as **statistical science**. In most of the processes in the real world we do not know the underlying models which cause them and so we use statistical science to learn about them. This can be done to achieve many goals, and we restrict ourselves to the following:

- describing empirical observations,
- inferring general conclusions from empirical observations,
- evaluating empirical observations.

We get to these topics in the next few sections but first we need to introduce a couple of important concepts.

## Populations and samples

Two key concepts in statistics are populations and samples. A **population** is every possible event/characteristic/individual in a certain group of interest. Some examples of populations are

Population 1: all of the people living the UK,

Population 2: all burglaries in the Greater London area of England,

Population 3: all the footwear of people living in a small local region.

Having information about these populations can have important practical impact. For example, knowing the locations of all burglaries in Greater London might reveal patterns such as hotspots. The Metropolitan Police could use this information to put preventative measures in place. 

Collecting data about the entire population is known as a **census**. The UK census is one such example, gathering data on people and households within the UK, occurring once every 10 years. Censuses are often very difficult or impossible to fully complete. This can be for logistical reasons such as it being too expensive and resource intensive to conduct. It can also be because the population is unable to be fully surveyed. Population 3 above would be highly relevant for criminal investigations within the specific local region, e.g. for comparing footwear marks found at local crime scenes. However, it is practically impossible to gather and record all of this information. One reason for this would be legal restrictions preventing the systematic collection of this data.

For these reasons, it is common to take a **sample** from the population instead. A sample is a subset of members out of a population. Since samples don't aim to gather information from the entire population, they are much simpler and cheaper to conduct. As well as saving resources, the aim of taking a sample is to be sufficiently representative of the population. Below are some examples of samples:

Sample 1: a survey of people within the UK,

Sample 2: reported burglaries in a Met. Police database,

Sample 3: voluntarily submitted footwear marks from a local area.

Sampling has many different types as it is such an important aspect of statistics. In this text, we will only mention random samples and convenience samples. A **random** sample is a sample drawn from a population in such a way that every member in the population has an equal probability of being sampled. In sample 1 above, the survey can be conducted by randomly contacting potential respondents from a representative database of people.

A **convenience** sample is a sample drawn from a population in which only the most available members of the population are sampled. This means that convenience samples might not be representative of the population as a whole. For sample 2 above, it might be reasonable to say that these reported burglaries are fairly representative of all burglaries in Greater London i.e. population 2. This assumes that almost all burglaries are reported and logged and that there are no systematic differences in the burglaries which are not being reported (e.g. in their location).

Sample 3 is also a convenience sample, this time of submitted footwear. The degree to which the submitted footwear marks are representative of all footwear in a local area (e.g. population 3) depends upon the representativeness of the volunteers who submit them. For example, if a certain age group of people, say teenagers, is over-represented in the volunteers then the sample will be **biased** in favour of them. This means that when trying to count a simple measure such as the most common shoe size in the local area, the data will be biased in favour of shoe sizes which are probably smaller than the true most common size. This is because the teenagers will still be growing. If desired, statistical methods can be applied to take this bias into account, e.g. by weighing the teenager group's submissions proportionally less compared to other age groups. This is one way polling companies attempt to make a potentially biased sample more representative when trying to determine election outcomes.

Sampling adds another layer of uncertainty to consider, which lends itself to working with probability. In any sampling method, there is uncertainty about how representative the sample is of the population from which it is drawn, and also a natural randomness as to which members are sampled. This represents both epistemic and aleatory uncertainty. The epistemic uncertainty arises because we do not know some characteristics of the population (which is why we are sampling from it). Learning more about the population reduces this uncertainty, and so the sampling procedure itself can reduce this.

The aleatory uncertainty is because there will always be randomness when sampling. Randomness is clearly present in a random sampling procedure, but it is also present within convenience samples. This is because there is a randomness as to which convenient members are sampled. **Sample size** is an important factor for characterising the aleatory uncertainty associated with sampling. The more we perform a sampling procedure in a population, the more we learn about its inherent variability. But sample size isn't everything: a small but carefully constructed random sample can be more representative of the population than a large but careless convenience sample.

Now that we have seen what populations and samples are, we can look at the first use of statistics from our list: describing observations.

## Describing observations

Using statistics to describe observations is probably their most familiar application. This is because they concisely and precisly report and summarise data, and this plays a central role in our ever growing data-centric society. These type of statistics are referred to as **descriptive statistics**. Some common examples are the average height of males or females in the UK, median household income, most popular baby's name for a girl etc. Descriptive statistics give us information about a particular set of data. Two common categories these fall into are **central tendency** and **dispersion**.

Measures of central tendancy aim to tell us a number around which the data tend to cluster. There are three common measures used for this: the **mean**, **median**, and the **mode**. The **mean** describes the average value of the data. The **median** describes the value in the middle of the data after we put all of the numbers in order. The **mode** describes the most common value on the data.

Dispersion aims to tell us how closely the data cluster around the measures of central tendency. The most common of these known as the **standard deviation**. The **standard deviation** tells us on average how close the data are to the mean. Larger values indicate that the data is not well represented by the mean since they are spread widely around it. Smaller values indicate that the mean is a good representation because the data are tighly packed around it.

When naming specific descriptive statistics, it is good practice to mention whether that statistic is describing a sample of data or a population. This makes it clearer and more interpretable to the consumer of the statistic because they can think about it in context, e.g. "could the statistic be biased by convenience sampling?" When a statistic describes a population, it is referred to using words such as *population*, *true*, *parameter*, e.g. the population mean. When a statistic describes a sample it is usually referred to as a **sample statistic** or using the word *empirical*, e.g. the sample mean. When this distinction is not made, there is the risk of confusing a sample for a population, which ignores uncertainty and leaves the potential to make an unwarrented and inaccurate generalisation. If in doubt, assume everything is a sample. Even for census data this can be a safe assumption since some forms are bound to not be completed or go missing after completion (even if this is a really small number).

The following example looks at descriptive statistics in action.

## Example: GSR publications

In 2020, a systematic analysis of all academic publications related to gunshot residue (GSR) particles was published by @sobreira2020. Among other things, the study looked at trends over time in the number of publications which are related to GSR. A small subset of the data, containing information about the number of publications per year between 2010-2018, can be found below in Table \@ref(tab:gsr-publication-table).

```{r gsr-publication-df, include=FALSE}
gsrDf <- as.tibble(read.table("./dat/gsr-publications.txt", header = TRUE, sep = "&")) %>%
  select(Year, Publications) %>%
  filter(Year > 2009)
```

```{r gsr-publication-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
gsrDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Academic publications related to gunshot residue analysis from the years 2010-2018.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```

Using this data, we can calculate some descriptive statistics. Let's assume this is a sample of the total population of GSR-related publications during the same timeframe. This is a safe assumption because some publications (albeit very few) may have been missed during the literature search.

To calculate the **sample mean**, we calculate the total number of publications and divide this by the number years for which we counted publications. Using the GSR publication data, this gives:
$$\text{sample mean}=\frac{20+27+45+38+37+31+41+47+45}{9}=\frac{331}{9}\approx36.8,$$
which means that the sample mean number of GSR-related publications per year during the period 2010-2018 is roughly 36.8. In other words, if we wanted to report a single number for the rate at which GSR-related publications were published each year between 2010-2018 in this sample, then that rate would be 36.8. You can confirm this by multiplying 36.8 by the number of years (9) and compare that to the total number of publications. 

Notice how 36.8 isn't itself a valid number of publications for any given year - the average doesn't exist as its own data point, it is simply a statistic which describes the data points. This highlights that there isn't such a thing as an "average year" for instance, which is sometimes how the average is reported.

How much spread around this sample mean is there in this data? We can answer that by calculating the **sample standard deviation**. The idea is to see a single number descriptive statistic for how different the number of publications are each year compared to the sample mean. The closer the number is to 0, the tighter the data are grouped around the sample mean. The further away from 0, the more spread there is.

To show some intuition for the sample standard deviation and how it is computed, we can plot the number of publications each year and highlight the difference between these and the sample mean.

```{r gsr-mean-plot, echo=FALSE, fig.cap="The number of GSR-related publications between 2010-2018. The black dashed line represents the mean number of publications per year in this sample. The red dashed lines show the difference between the sample mean and the observed number of publications.", out.width = '80%', fig.align = 'center'}
ggplot(gsrDf, aes(x=Year, y=Publications)) + geom_point() +
  geom_hline(yintercept=36.8, linetype="dashed") +
  geom_segment(aes(x = 2010, y = 20, xend = 2010, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2011, y = 27, xend = 2011, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2012, y = 45, xend = 2012, yend = 36.8),linetype="dashed", color='red') +
geom_segment(aes(x = 2013, y = 38, xend = 2013, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2014, y = 37, xend = 2014, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2015, y = 31, xend = 2015, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2016, y = 41, xend = 2016, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2017, y = 47, xend = 2017, yend = 36.8),linetype="dashed", color='red') +
  geom_segment(aes(x = 2018, y = 45, xend = 2018, yend = 36.8),linetype="dashed", color='red') 
```

The standard deviation reduces the differences in Figure \@ref(fig:gsr-mean-plot) into a single number summary. The first couple of steps involve calculating the differences, squaring them (this mean multiplying them by themselves), and then adding all of these squared differences together. The squaring operation means that each individual difference becomes a positive number and so we end up adding together a collection of positive numbers. If any of the numbers were negative, then adding them together with positive numbers would decrease the total sum of differences. Squaring the differences also means that bigger differences become much bigger. This means that big differences contribute much more to the total sum of differences. We can see this by calculating the differences and their squares in Table \@ref(tab:gsr-squared-diffs-table) below.

```{r gsr-squared-diffs-table, echo=FALSE}
diffsDf <- gsrDf %>%
  mutate(sampleMean = 36.8) %>%
  mutate(Differences = (Publications-sampleMean)) %>%
  mutate(squaredDifferences = Differences^2)
options(kableExtra.html.bsTable = T)
diffsDf %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Number of GSR-related publications per year. The differences represent the number of publications that year minus the sample mean number of publications 36.8. These differences are then squared in the squaredDifferences column.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")


```

If you look at the year 2013 as an example, you can see that its difference from the sample mean is only 1.2. Multiplying this by itself gives $1.2\times 1.2=1.44$, and so the squared difference is only a small increase from the original difference. Compare that to the year 2010. For that year, there is a difference of -16.8 compared to the sample mean, but when this is squared we get $(-16.8)\times(-16.8)=282.24$. The years 2013 and 2010 are not too far away in terms of their difference to the mean (1.2 compared to -16.8), but they are very far away on the squared difference scale (1.44 compared to 282.24). Notice how squaring decreases differences which are less than 1 (this happens in 2014). 

Now that we have the squared differences we add them all up to get a total of 669.56. The next step is to divide this total by a number so that it approximately represents the squared differences per year. This is where the calculation of the standard deviation can be different for a population versus a sample. 

For a population, we would divide the sum of squared differences by the number of years which contributed to the sum, 9 (just as is done for both the population and sample mean). This is the more intuitive quantity because it exactly represents the squared differences from the sample mean per year.

However, for a sample, there are two ways we can do it. We can either divide the sum of squared differences by the same number as for the population standard deviation, 9, or we can divide by this number minus 1, which is 8. The former is known as the biased sample variance, and the latter as the unbiased sample variance. We focus on the unbiased sample variance in this book.

The reason that the unbiased sample variance divides by this less-intuitive number involves more complex statistical theory which is not covered here. One intuition behind it is that the number we divide by is like a currency. Each data point gives an extra unit of currency to spend. We pay a price of this currency in the computation of a new statistic that uses the original data as well other sample statistics based upon that data. The price paid is equal to the number of extra statistics we use in the computation of the new one. 

To compute the mean (population or sample), we added the data for the number of publications per year together as the first step. Since this involved no statistics, we paid no price and so could divide by the number of data points. To compute the population standard deviation, we would add the squared differences from the population mean together. Since this involved the population mean, and not a sample statistic, no price is paid and so we could just divide by the number of data points. For the sample standard deviation however, the squared differences are computed based upon the sample mean. Since this is a sample statistic, we pay a price equal to one. We therefore divide the sum of squared differences by the number of data points minus 1. 

Back to the GSR dataset, when we divide the sum of squared differences by $9-1=8$ we get $83.695$. The squared differences and the result after dividing their sum by 8 is shown in Figure \@ref(fig:gsr-squared-diffs-plot) below.

```{r gsr-squared-diffs-plot, echo=FALSE, fig.cap="The squared differences between the actual number of publications each year and the sample mean. The dashed line shows the sum of the squared differences 669.56 divided by 8. ", out.width = '80%', fig.align = 'center'}
ggplot(diffsDf, aes(x=Year, y=squaredDifferences)) + geom_point() +
  geom_hline(yintercept=sum(diffsDf$squaredDifferences)/8, linetype="dashed") +
  ylab("Squared differences")
```

At this point, the number $83.695$ represents the typical squared difference between the data points and the sample mean. It is more interpretable to convert that from the squared scale of differences back to the original scale. This is the final step of computing the unbiased sample standard deviation. The mathematical operation which does this conversion, the 'opposite' of squaring, is called the square root. The square root takes a number and finds which other number is squared to make it. The square root of our quantity of interest is $\sqrt{83.695}\approx9.15$. This is the unbiased sample standard deviation.

To recap, the steps we took to compute the unbiased sample standard deviation were:

1. compute the sample mean,
2. compute the differences in the number of publications each year compared to the sample mean,
3. square the differences,
4. add them together,
5. divide by the number of years which contributed to the sum minus 1,
6. compute the square root.

A sample standard deviation of 9.15 means that there is a typical spread of 9.15 around the sample mean of 36.8 in this dataset. 

The **sample median** is best seen by rearranging Table \@ref(tab:gsr-publication-table) so that the number of publications are in order from lowest to highest, rather than in chronological order by year. This is shown in Table \@ref(tab:gsr-publication-sorted-table).

```{r gsr-publication-sorted-table, echo=FALSE}
options(kableExtra.html.bsTable = T)
gsrDf %>% 
  arrange(Publications) %>%
  knitr::kable(booktabs = TRUE, escape = F, align = "c",
             caption = 'Academic publications related to gunshot residue analysis from the years 2010-2018 in ascending order.') %>%
  kable_styling(c("striped", "condensed"), 
                latex_options = "striped")
```
Since we have 9 values, the middle value is the one which has 4 values lower than it and 4 values higher than it. This is seen by inspecting Table \@ref(tab:gsr-publication-sorted-table). The median number of GSR-related publications in this sample during the period 2010-2018 is 38. This is quite similar to the sample mean of 36.8, and so these two measures of central tendency are in general agreement with each other. 

The median isn't always this easy to calculate. When there is an even number data points then we have to decide upon a consistent method to select the median value. For example, if we excluded 2010 from our sample of GSR-related publications, then we have 8 years of publication data. There are then a range of values between 38 and 41 which leave 4 observed values above and below them. However, the technicalities of these situations are beyond the scope of this content.

The final measure of central tendency to calculate is the **sample mode**. This is the most common number of GSR-related publications that appears in our sample. We can see from Table \@ref(tab:gsr-publication-sorted-table) that one value appears twice, and all others appear once. The sample mode is therefore the value which appears twice, which is 45.

All of the above statistics should be subject to a fair amount of criticism for this dataset. The least reliable here is the mode. With just two occurences, including another single year with 20 publications into the dataset would change it from 45 down to jointly 45 and 20. All of these descriptive statistics miss the main message of this dataset, which can be seen from Figure \@ref(fig:gsr-mean-plot): there appears to be an upwards trend in the number of GSR-related publications during this time period. There are descriptive statistics for this too, but the main message is that descriptive statistics alone, whilst very useful, might not tell the whole story.

Depending on the question of interest, it could be meaningful to compare descriptive statistics from this sample to those of another sample of publications, e.g. the same GSR publications from the 9-year period 2001-2009. Such comparisons often go beyond just describing the data, and begin to start to infer conclusions from them. This brings us on to the other type of statistics: inferential statistics.

## Inferring from observations

<!-- Inferring from observationorm -->
<!-- commenting out these lines as they are bugged!! -->
<!-- Inferring from observations means drawing more general conclusions about populations based upon information gathered from samples. It is known as statistical inference, and is done using inferential statistics. -->

Inferring from observations means drawing more general conclusions about populations based upon information gathered from samples. This is known as **statistical inference**, and it is done using **inferential statistics**. Inferential statistics go beyond describing information in a sample, and that is what makes them different from descriptive statistics.

A popular application of statistical inference is **estimation**. Estimation aims to learn about some property of a population of interest. A common type of estimation is **parameter estimation**, in which certain parameters of the population are estimated using probability models and statistics from samples. We saw an example of this at the beginning of this chapter. The example stated that a coin was flipped 10 times and 7 of those resulted in heads. The sample proportion of heads, a descriptive statistic, is 0.7. If we use this sample statistic to make an inference about the population (e.g. the population proportion of heads, or the probability of heads is 0.7) then we say it is an **estimate** and in this use it is an inferential statistic. This estimate is a single number summary, and we call such estimates **point estimates**. In the example above, we used the sample proportion of heads as a point estimate for the population proportion of heads.

As well as estimation using point estimates, we can perform estimation using **interval estimates**. Rather than using a single number summary, interval estimates give a range of values as their estimate. For example, given that the sample proportion of heads from 10 coins flips was 0.7, we might take an educated guess that the population proportion of heads is  between 0.4 and 1. We could make this range more narrow, say 0.6 to 0.8, but as we make the interval more narrow, we also increase our uncertainty. Logically, we have to be less certain that the population proportion of heads lies between 0.6 and 0.8, compared with 0.4 and 1, since the former interval is fully contained within the latter interval. A common interval estimate is a confidence interval, where the size of the interval depends upon a level of confidence we wish to have (amongst other factors).

Estimates are carefully selected so that they have desirable statistical properties and are supported by statistical theory as being "optimal" in some sense. For example, the sample proportion of heads can be thought of as the point estimate which maximises the probability of observing 7 out of 10 coins as heads (with some reasonable assumptions). In addition, a correctly constructed 95% confidence interval has the property that if enough samples are gathered and intervals calculated for each sample, then 95 out of every 100 of them will contain the population parameter. Individuals may disagree about what are "reasonable" assumptions or which criterion should be maximised and so there can be reasonable discrepencies between estimates --- there may be no "correct" answer. Transparency about these matters is key to intepreting and evaluating inferences.

From this basis of estimation we can branch out into other forms of inference such as statistical hypothesis testing, making predictions about the future, and, most relevant to forensic science, evaluating scientific evidence. What is important before making any statistical inference is the data upon which the inference will be based. The quality of the inference can only be as good as the quality of the data. Assessing the quality of data is the topic of the next section.

## Data sources

A data source is something which contains data. This broad definition covers anything from extensive national criminal databases, which contain information about convicted criminals, to handwritten personal notes about the outcome of coin tosses. Examples between these extremes include academic journal articles detailing experiments related to forensic problems and their results, and proprietary in-house databases with images of footwear marks from crime scenes managed by commerical forensic service providers. Clearly there is a range in quality for these sources of data. A range in quality for the data means that the resulting statistical inferences will necessarily vary in quality. That is why it is important to be able to recognise the characteristics which contribute to a high quality data source when presented with any statistical inference.

Statistics can be computed on any source of data. The source of data need not be personally collected. As we saw in the GSR example, the source of data can be second-hand from someone else's data collection. This grants statistics a wide range of applications, and means that so long as the data is accessible, then it can be analysed. When data is collected and analysed by the same individual or research team, then the experimental protocal and quality of the data is known by them. However, when data is analysed second-hand, then the experimental design is unknown and so is the quality of data. This makes transparancy and trust key. Transparancy can be achieved by detailing the conditions of the experiment or sampling methodology which are meaningful to the quality of the data and resulting data analysis. 

**quality**: there a few main components to think about when it comes to assessing the quality of a data source. 

1. variability. variability refers to how much the data vary. this can be because the target population varies or because the data capture protocol influenced variability. Variability can be better understood through repeated sampling. 

2. size. size refers to the size of the data source. 

3. validity. validity refers to the integrity of the data, its collection strategy, its measurement strategy, any verification, replication/reproduction, and accreditation. 

4. representativeness. this relates to how representative the collected data is of its target population or overall purpose. 

all of these components are related to each other. for example, a data source which measures something with very low variability will require a smaller number fo measurements to capture this variability, and therefore a smaller overall size. 

In addition a big data source which has no validity is of little use, as is a data source which does not sufficiently represent its target population.

As well as assessing the quality of a data source, another factor to consider is its relevance to the circumstances in which it is being used. This will vary on a case-by-case basis, depending on the question that is being addressed. After the question has been fixed, then the **relevant population** can be identified and hopefully there are data sources which have sampled from it, and if not then an experiment can be designed to sample from it. 

For example, if we wanted to investigate the claim that life expectancy in the UK is increasing, then the relevant population is the ages of the people of the UK measured over a period of time. A relevant data source would be one which contains this information. Official national statistics based on UK census data would be a relevant and high quality data source. Anecdote from an individual in the UK about many generations of their personal family history is also relevant, but lower in quality than official statistics due to its smaller and biased sample. 

Data sources can be highly relevant but low quality, and have low relevance but high quality. Both of these properties need to be assessed in order to evaluate the overall value of a data source to a question at hand.

In order to fully assess the quality and relevance of a data source, it needs to be accessible to those assessing it. Maximum accessibility is achieved when data are published as open access, which means that anyone  can theoretically gain access. This isn't widely practiced in reality. What often occurs is that access is given to a restricted group of individuals. These individuals might be accredited with a governing body to assure their quality and integrity, or members of a private group (e.g. forensic service providers) who own the data. This involves a level of trust in the accreditation process achieving its aims.

Due to the varied nature of crimes, sometimes the only available data source in a forensic context might be the experience of an examiner. This presents challenges to accessibility, assessibility and quality, but it is the best information that might be available for a particular case. In a criminal case, comments on these proporties will be made in the expert witness's report and can therefore be explored pre-trial in meetinhgs between experts and in cross-examination.

## Example: Ogden Tables

Civil cases involving personal injury and fatal accidents will likely have to calculate the financial damages experienced on behalf of the claimant. This means calculating future costs and losses, which would not have been incurred had it not been for the injury in question. Such losses include lost income and pension benefits when employment has been affected, cost of equipment now needed to manage the effects of the injury, etc. Factors which need to be considered in order to value the losses include age-to-retirement of the claimant, life expectancy of the claimant, time for which specialist equipment might be required etc. These calculations are complicated. 

Actuarial tables containing values needed to calculate damages were permitted for use in The Civil Evidence Act 1995. The tables are informally known as the Ogden Tables, after the chair of the Working Party who introduced them. For civil disputes of personal injury, the Ogden Tables are the primary data source for calculating damages. The values in them have been calculated by UK government actuarial scientists using current mortality rates and are presented for a range of factors, such as claimant age. This means that the complicated parts of the damage calculations have already been done by specialists and so in many common claims, only a simple calculation by reading values from the tables is required. 

The Ogden tables are an example of a high quality data source. The values contained within its tables are based upon official statistics, which have been verified and agreed by accredited government statisticians. This makes the data underlying the tables accessible and highly validated. The calculations which lead to the values in the table are not available but have been performed and agreed by expert committees, this lends them validity too. The data source is representative since the target population are members of the UK and that is whose official data has been used in the underlying calculations for the values in the table. 

Official statistics will often be aggregates of many data sources. This means that although single number summaries (e.g. medians) of lots of data will be used. This gives some guarantee that the data source is of an appropriate size. Not much information is presented in the tables about variability, as the values are single number summaries. A level of trust in the process of calculating these values is needed for this. This trust is easier to achieve with this data source due to the recognised and accredited nature of the organisations involved in creating the tables.

The relevance of the Ogden tables depends upon the case at hand. The tables have been constructed to cover a range of common scenarios, e.g. claimant age and some common pension ages. When the claimant in question is covered by these common factors then the tables are a highly relevant (and endorsed) source of data. When the claimant in question is not sufficiently covered by these factors, then the tables cannot be used and are thus not a relevant source of data. This could occur because the claimant holds a more complex pension plan. In this situation the correct data source is a Fellow of the Institute and Faculty of Actuaries. 

<!-- The below Figure shows some probabilities for more tangible events than theoretical coin tosses. -->

<!-- ```{r, prop-ruler, echo=FALSE, fig.align = 'center'} -->
<!-- ruler.func<-function(gg){ -->
<!-- seq.list<-list() -->
<!-- for(i in 1:length(gg)){   -->
<!--   ystart<-seq(0.1,gg[i],0.1) -->
<!--   yend<-ystart -->
<!--   xstart<-rep(i-0.25,length(ystart)) -->
<!--   xend<-xstart+0.1 -->
<!--   nam.val<-c(LETTERS[i],rep(NA,length(ystart)-1)) -->
<!--   numb.val<-c(gg[i],rep(NA,length(ystart)-1)) -->
<!--   seq.list[[i]]<-data.frame(nam.val,numb.val,xstart,xend,ystart,yend) -->
<!-- } -->
<!-- df<-as.data.frame(do.call(rbind, seq.list)) -->
<!-- p <- ggplot(df, aes(nam.val)) -->
<!-- p <- p + geom_bar(aes(y=numb.val,fill=nam.val),stat="identity",width=0.5,color="black",lwd=1.1)+ -->
<!--     scale_x_discrete(limits=LETTERS[1:length(gg)])+ -->
<!--     geom_segment(aes(x=xstart,y=ystart,xend=xend,yend=yend))+ -->
<!--     geom_hline(yintercept=c(0.25, 0.5, 0.75),color="white",lwd=1.1)+ -->
<!--     ggtitle("Probability of real events")+ -->
<!--     ylim(c(0,max(gg)+0.5))+ -->
<!--   annotate("text",x=seq(1,length(gg),1),y=gg+0.1,label=gg,fontface="bold",size=rel(6))+ -->
<!--   theme_bw()+ -->
<!--   theme(axis.title=element_blank(), -->
<!--         axis.text.y=element_blank(), -->
<!--         axis.text.x=element_text(face="bold",size=rel(1.5)), -->
<!--         axis.ticks=element_blank(), -->
<!--         panel.border=element_blank(), -->
<!--         panel.grid=element_blank(), -->
<!--         legend.position = "bottom", -->
<!--         legend.margin = margin()) + -->
<!--   scale_fill_discrete(name="Event", -->
<!--                       labels=c("an American male dying of cancer in their lifetime", -->
<!--                                "any 2 people having the same birthday in a room of 23 random people", -->
<!--                                "the Scottish city of Dundee being overcast on 26th January", -->
<!--                                "a female born in 2020 living to age 70 or longer"))+ -->
<!--   guides(fill=guide_legend(nrow=4,byrow=TRUE))+ -->
<!--   coord_flip() -->
<!-- print(p) -->
<!-- } -->
<!-- suppressWarnings(ruler.func(c(0.21,0.51,0.68, 0.9))) -->
<!-- ``` -->

<!-- Whose probabilities are these? -->


## Applications to forensic science

[journal articles; descriptive statistics, inferential statistics, data source]

[case work; descriptive statistics, inferential statistics, data source]

Now we have the foundation of statistics and probability theory to begin applying it to case situations. However, when applying these new thought techniques to case situations, there are practical procedural elements which also need to be followed. In the analysis of forensic scientific evidence, this means merging probability and statistics with contextual information from both the prosecution and defence. In particular, it is required to take into account what the prosecution and defence's version of events are when considering evidence which has been recovered. These statements are formalised into what are known as **propositions**, and this is the topic of the next chapter.

## More information

<!-- Cancer: https://www.cancer.org/cancer/cancer-basics/lifetime-probability-of-developing-or-dying-from-cancer.html -->

<!-- Life expectancy: https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthandlifeexpectancies/articles/lifeexpectancycalculator/2019-06-07 -->

<!-- Weather: https://weatherspark.com/m/40087/6/Average-Weather-in-June-in-Dundee-United-Kingdom -->

<!-- Birthday: https://en.wikipedia.org/wiki/Birthday_problem -->

## Exercises

